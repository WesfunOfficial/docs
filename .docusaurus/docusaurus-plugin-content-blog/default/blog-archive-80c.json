{
  "blogPosts": [
    {
      "id": "2023/04/21/snakes-on-a-boat",
      "metadata": {
        "permalink": "/blog/2023/04/21/snakes-on-a-boat",
        "source": "@site/blog/snakes-on-a-boat.md",
        "title": "Snakes On A Boat",
        "description": "We had intended to get a blog post out a little bit quicker, but the last month has been extremely",
        "date": "2023-04-21T21:12:56.000Z",
        "formattedDate": "April 21, 2023",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.335,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Snakes On A Boat",
          "date": "2023-04-21T21:12:56.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/snakes-on-a-boat/mothertruckingsnakes.webp",
          "slug": "2023/04/21/snakes-on-a-boat"
        },
        "nextItem": {
          "title": "Infrastructure Launched!",
          "permalink": "/blog/2023/03/18/infrastructure-launched"
        }
      },
      "content": "We had intended to get a blog post out a little bit quicker, but the last month has been extremely\naction packed. However, it has paid off immensely. As our friends at Solus [recently announced](https://getsol.us/2023/04/18/a-new-voyage/)\nit is time to embark on a new voyage.\n\n![Snaaaakes](/img/blog/snakes-on-a-boat/mothertruckingsnakes.webp)\n\n<!--truncate-->\n\n### General gist of it\n\nThere is very little point in rehashing the post, so I'll cover the essentials\n\n - We assisted the Solus team in deploying a complete new infrastructure. Some of this work is ongoing\n - The package delivery pipeline is in place, meaning updates will soon reach users.\n - We're working towards a mutually beneficial future whereby Solus adopts our tooling.\n\n### Is Serpent OS over?\n\nOh no, no. The most practical angle to view this from is that Serpent OS is free to build and innovate to create the\nbasis of Solus 5. Outside of the distribution we're still keen to continue development on our tooling\nand strategies.\n\nIt is therefore critical that we continue development, and find the best approach for both projects, to make\nthe transition to Solus 5 as seamless as possible. To that end we will still need to produce ISOs and have an\nactive community of users and testers. During this transition period, despite being two separate projects, we're both\nheading to a common goal and interest.\n\n### Finances\n\nWe have an exciting journey ahead for all of us, and there are many moving parts involved. Until we're at the point\nof mutual merge, we will keep the entities and billing separate. Thus, funds to the Solus [OpenCollective](https://opencollective.com/getsolus)\nare intended for use within Solus, whereas we currently use [GitHub Sponsors](https://github.com/sponsors/ikeycode?o=sd&sc=t)\nfor our own project needs.\n\nCurrently Solus and Serpent OS share one server, which was essential for quick turnaround on infrastructure enabling.\nAt the end of this month Serpent OS will migrate from that server to a new, separate system, ensuring the projects\nare billed separately.\n\nLong story short, if you wish to sponsor Serpent OS **specifically**, please do so via [our GitHub sponsors account](https://github.com/sponsors/ikeycode?o=sd&sc=t) as our monthly running costs\nwill immediately rise at the end of this month. If you're supporting Solus development, please do visit them and sponsor them =)\n\n### Next steps?\n\nGlad you asked! Now that the dust is settling, we're focusing on Serpent OS requirements, and helping Solus where we can.\nOur immediate goals are to build a dogfooding system for a small collection of developers to run as an unsupported prealpha\nconfiguration, allowing us to flesh out the tooling and processes.\n\nThis will include a live booting GNOME ISO, and sufficient base packages to freely iterate on `moss`, `boulder`, etc as well\nas our own infrastructure. Once we've attained a basic quality and have an installer option, those ISOs will be made available\nto you guys!"
    },
    {
      "id": "2023/03/18/infrastructure-launched",
      "metadata": {
        "permalink": "/blog/2023/03/18/infrastructure-launched",
        "source": "@site/blog/infrastructure-launched.md",
        "title": "Infrastructure Launched!",
        "description": "After many months and much work, our infrastructure is finally online.",
        "date": "2023-03-18T23:20:55.000Z",
        "formattedDate": "March 18, 2023",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.315,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Infrastructure Launched!",
          "date": "2023-03-18T23:20:55.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/infrastructure-launched/Featured.webp",
          "slug": "2023/03/18/infrastructure-launched"
        },
        "prevItem": {
          "title": "Snakes On A Boat",
          "permalink": "/blog/2023/04/21/snakes-on-a-boat"
        },
        "nextItem": {
          "title": "Lift Off",
          "permalink": "/blog/2022/12/24/lift-off"
        }
      },
      "content": "After many months and much work, our infrastructure is [finally online](https://dash.serpentos.com).\nWe've had a few restarts, but it's now running fully online with 2 builders, ready to serve builds\naround the clock.\n\n![Infra launched](/img/blog/infrastructure-launched/Featured.webp)\n\n<!--truncate-->\n\nFirstly, I'd like to apologise for the delay since our last blog post. We made the decision to move\nthis website to static content, which took longer than expected. We're still using our own D codebase,\nbut prebuilding the site (and fake \"API\") so we can lower the load on our web server.\n\n### Introducing the Infra\n\nOur infrastructure is composed of 3 main components.\n\n#### Summit\n\nThis is the page you can see over at [dash.serpentos.com](https://dash.serpentos.com). It contains\nthe build scheduler. It monitors our git repositories, and as soon as it discovers any missing builds\nit creates build tasks for them. It uses a graph to ensure parallel builds happen as much as possible,\nand correctly orders (and blocks) builds based on build dependencies.\n\n#### Avalanche\n\nWe have 2 instances of Avalanche, our builder tool, running. This accepts configuration + build requests\nfrom Summit, reporting status and available builds.\n\n#### Vessel\n\nThis is an internal repository manager, which accepts completed packages from an Avalanche instance.\nPublished packages land at [dev.serpentos.com](https://dev.serpentos.com)\n\n### Next steps\n\nSuper early days with the infra but we now have builds flowing as part of our continuous delivery solution.\nKeeping this blog post short and sweet... we're about to package GNOME and start racing towards our first\n**real** ISOs!\n\n### Don't forget\n\nIf you like what the project is doing, don't forget to [sponsor](https://github.com/sponsors/ikeycode?o=sd&sc=t)!"
    },
    {
      "id": "2022/12/24/lift-off",
      "metadata": {
        "permalink": "/blog/2022/12/24/lift-off",
        "source": "@site/blog/lift-off.md",
        "title": "Lift Off",
        "description": "Enough of this \"2 years\" nonsense. We're finally ready for lift off. It is with immense pleasure we can",
        "date": "2022-12-24T01:32:04.000Z",
        "formattedDate": "December 24, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.635,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Lift Off",
          "date": "2022-12-24T01:32:04.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/lift-off/Featured.webp",
          "slug": "2022/12/24/lift-off"
        },
        "prevItem": {
          "title": "Infrastructure Launched!",
          "permalink": "/blog/2023/03/18/infrastructure-launched"
        },
        "nextItem": {
          "title": "Ready, Set, Go",
          "permalink": "/blog/2022/11/18/ready-set-go"
        }
      },
      "content": "Enough of this \"2 years\" nonsense. We're finally ready for lift off. It is with *immense pleasure* we can\nfinally announce that Serpent OS has transitioned from a promise to a deliverable. Bye bye, phantomware!\n\n![Lift off](/img/blog/lift-off/Featured.webp)\n\n<!--truncate-->\n\n### We exist\n\nAs mentioned, we spent 2 years working on tooling and process. That's .. well. Kinda dull, honestly. You're\nnot here for the tooling, you're here for the OS. To that end I made a decision to accelerate development of\nthe actual *Linux distro* - and shift development of tooling into a parallel effort.\n\n### Infrastructure .. intelligently deferred\n\nI deferred final enabling of the infrastructure until January to rectify the chicken/egg scenario whilst allowing\nus to grow a base of contributors and an actual distro to work with. We're in a good position with minimal blockers\nso no concern there.\n\n### A real software collection\n\nThis is our term for the classical \"package repository\". We're using a temporary collection right now to store all\nof the builds we produce. In keeping with the `Avalanche` requirements, this is the **volatile** software collection. Changes\na lot, hasn't got a release policy.\n\n### A community.\n\nIt goes without saying, really, that our project isn't remotely possible without a **community**. I want to take the time\nto personally thank everyone that stepped up to the plate lately and contributed to Serpent OS. Without the work of the\nteam, in which I include the contributors to our `venom` recipe repository, an ISO was never possible. Additionally contributions\nto tooling has helped us make significant strides.\n\nIt should be noted we've practically folded our old \"team\" concept and ensured we operate across the board as a singular community,\nwith some members having additional responsibilities. Our belief is all in the community have equal share and say. With that said,\nto the original \"team\", members both past and present, I thank for their (long) support and contributions to the project.\n\n### An ISO.\n\nWe actually went ahead and created our first ISO. OK that's a lie, this is probably the 20th revision by now. And let's be brutally\nhonest here:\n\n**It sucks.**\n\nWe expected no less. However, the time is definitely here for us to begin our public iteration, transitioning from suckness to a project\nworth using. In order to do that, we need to get ourselves to a point whereby we can dogfood our work and build a daily driver. Our focus\nright now is building out the core technology and packaging to achieve those aims.\n\nSo if you want to try our uninstallable, buggy ISO, chiefly created as a brief introduction to our package manager and toolchain, head to our\nnewly minted [Download](/download) page. Set your expectations low, ignore your dreams, and you will not be disappointed!\n\nAll jokes aside, it took a long time to get to point where we could even construct our first, KVM-focused, UEFI-only `snekvalidator.iso`. We now\nhave a baseline to improve on, a working contribution process, and a booting, self-hosting system.\n\nThe ISO is built using 2 layered collections, the `protosnek` collection containing our toolchain, and the new `volatile` collection. Much of the\npackaging work has been submitted by `venom` contributors and the core team. Note you can install `neofetch` which our very own Rune Morling (`ermo`)\npatched to support the Serpent OS logo.\n\nBoot it in Qemu (or certain Intel laptops) and play with moss now! Note, this ISO is not installable, and no upgrade path exists. It is simply\nthe beginnings of a public iteration process.\n\n### Next steps\n\nIn January we'll launch our infrastructure to scale out contributions as well as to permit the mass-rebuilds that need to happen. We have to\nenable our `-dbginfo` packages and stripping, which were disabled due to a parallelism issue. We need to introduce our boot management based around\n`systemd-boot`, provide more kernels, do hardware enabling, introduce `moss-triggers`, and much more. However, this is a **pivotal moment** for our\nproject as we've finally become a **real**, if not __sucky__, distro. The future is incredibly bright, and we intend to deliver on every one of our\npromises.\n\nAs always, if you want to support our development, please consider [sponsoring](/sponsors) the work, or engaging with the community on Matrix or indeed\nour [forums](https://forums.serpentos.com). \n\nYou can discuss this blog post, or leave feedback on the ISO, over at our [forums](https://forums.serpentos.com/d/40-lift-off)."
    },
    {
      "id": "2022/11/18/ready-set-go",
      "metadata": {
        "permalink": "/blog/2022/11/18/ready-set-go",
        "source": "@site/blog/ready-set-go.md",
        "title": "Ready, Set, Go",
        "description": "While the blog has been quiet, the git commits have been flowing! The core pieces of our infrastructure are nearing",
        "date": "2022-11-18T07:07:25.000Z",
        "formattedDate": "November 18, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 5.765,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Ready, Set, Go",
          "date": "2022-11-18T07:07:25.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/ready-set-go/Featured.webp",
          "slug": "2022/11/18/ready-set-go"
        },
        "prevItem": {
          "title": "Lift Off",
          "permalink": "/blog/2022/12/24/lift-off"
        },
        "nextItem": {
          "title": "The Big Update",
          "permalink": "/blog/2022/09/14/the-big-update"
        }
      },
      "content": "While the blog has been quiet, the git commits have been flowing! The core pieces of our infrastructure are nearing\nthe testing phase - a complete workflow for developers to submit and build packages and for them to become available in\nthe Serpent OS repo. At this point we can get many more contributors involved in packaging while circling back to make\nit more automated and efficient for them.\n\n![img](/img/blog/ready-set-go/Featured.webp)\n\n<!--truncate-->\n\nBy respecting the time of contributors, we can get more done in the same time, utilise and manage more contributions to\nreduce the workload on everyone. This also makes Serpent OS an attractive platform for new contributors and enables more\nusers to get involved by making packaging so easy.\n\n**TL:DR - With the infrastructure and tooling almost in place, the bringup of Serpent OS begins**\n\n\n# The Long Road Reaps Rewards\n\nWhat we have done is worked on getting all the tools and infrastructure in place before trying to release anything\ninstallable to users. Experience has shown that large refactors or complete rebuilds of the tools are difficult to\npursue and take years to eventuate (if at all). This means that efforts can now go towards making the distribution\nyou've been waiting for without having issues scaling or tools that need a rewrite due to hurting the experience of\ncontributors.\n\nGoing forward users and developers will see tangible benefits from the work put into Serpent OS without significant time\nspent on the hidden parts of the distribution (which are essential but time consuming). This is not to say that they are\nfeature complete, but that the focus can be on making visible improvements to people's Serpent experience.\n\nFrom a fundraising point of view, this has meant it has taken longer to get a usable ISO into your hands...but the\nbenefits will be seen for years to come.\n\n# `summit` - Managing the Distribution\n\nAt a high level, `summit` sits in the middle of everything, communicates with our other tools `avalanche` and `vessel`,\nand provides an overview of the project and what's happening within it via dashboards. When a build is generated,\n`summit` fetches the build information, determines the optimal build order, finds a builder to create the package and\nthen lets `vessel` know when it's ready to enter the main repository. This is all backed by authentication to ensure\nonly authorised users and builders are used to maintain security.\n\nOur build files are backed by git repositories on GitHub, where `summit` brings the important information much closer to\ndevelopers. Our git repositories are organised into logical groups representing the delegation of responsibilities. For\nexample, we can add many more developers to our `plasma` group and enable them to push builds directly where we may not\nwant them to modify the toolchain or kernel without a second review.\n\n`summit` also parses information from git to display in an easy to consume format. Here we see an example of the\nmetadata page of the `ccache` package.\n\n![Metadata](/img/blog/ready-set-go/Metadata.webp \"Metadata\")\n\n# `avalanche` - Builder as a Service\n\n`avalanche` runs as a service ready to accept builds from `summit`. It is low overhead and monitors the state of the\nmachine via a dashboard so if the builder were to run out of disk space or stall it will be easily seen by the\n`avalanche` owner. This makes it very easy for people to spin up a builder and will eventually be utilised for people\nwho lack powerful hardware to test builds on before submitting as official packages. Note that distribution packages\nwill only be built on the most trusted builders and test builds will be made to ensure the correctness of all\n`avalanche` builders is maintained.\n\n![Avalanche](/img/blog/ready-set-go/Avalanche.webp \"Avalanche\")\n\nThere are many use cases for `avalanche`, where we can partition system resources via `systemd-nspawn` instances to\nprovide multiple builders on one host. Many builds only utilise a few cores, so it becomes more efficient to run\nmultiple builders concurrently. As builds are isolated from the host, `avalanche` instances can easily be run on\nbare metal or in a container/VM if preferred.\n\nOnce a build is completed, `avalanche` is responsible for reporting completion to `summit`. `summit` then instructs\n`vessel` to fetch the build artefacts from the relevant `avalanche` instance.\n\nThis design decision allows `summit` to schedule a new build to the `avalanche` instance as soon as it has reported\ncompletion of the prior build, thus allowing for maximum use of both upload and download bandwidth for each `avalanche`\ninstance -- and as a direct consequence, higher overall build throughput across the entire set of builders.\n\n# `vessel` - Making Packages Available to Users\n\n`vessel` is a fairly simple (but very important) part of the build infrastructure. After fetching a package from a\nbuilder, `vessel` will then update its index with the newly minted package, thus making the package available in\nour official repository for installation.\n\nIn future `vessel` will be expanded to support versioned indexes (making each update a new release) and branches to\nenable testing new features complete with rollback that `moss` supports across all updates.\n\n# Enabling Faster Packages!\n\nFeature work has started on adding ISA levels to enable support for adding `x86-64-v3x` packages. This can provide extra\nperformance and reduced power use for supported CPUs (this denotes the `x86-64-v3` psABI with some extra e**x**tensions\nthat your processor will likely also include). As we aren't relying on the `glibc` hwcaps feature, we have the ability\nto adjust the featureset to the benefit of our users. Once we have a `x86-64-v4` builder available, we can also look at\nproviding an extra layer for packages that heavily utilise math. This will be the result of actual benchmarks to\nindicate which packages provide additional performance from their newer CPU features.\n\nWith the infrastructure already supporting multiple builders (and an optimised toolchain to minimise build time), adding\nmore architectures will not pose any capacity issues.\n\n# Finally, a note on fundraising\n\nRecently we switched away from OpenCollective for our fundraising to GitHub Sponsors, as the platform offers a zero-fees\nsponsorship. Being an independent project - every penny really does count! However, we did lose an element of transparency,\nso I'd like to provide some initial data:\n\n - October 2022: £3,003.70\n - November 2022: £540.58\n\nAt some point this website will incorporate some more up-to-date information on sponsorship information to facilitate\na more transparent approach. Recently I've taken some additional steps towards this, including opening a (Revolut) bank\naccount specifically for Serpent OS funds + expenses.\n\nI'd like to thank everyone supporting the project - the time for us to reward you has now come. Intermittent funding and\nthe cost of living crisis has had a huge impact on the timescale of the project to date, however we're not shying away from\nthe challenge. Rather, we've cracked on with implementing the proto-infrastructure and will be deploying it within *days*.\n\nLet's do this.\n\n--\n\nYou can discuss this post over on our [forums](https://forums.serpentos.com/d/30-ready-set-go)"
    },
    {
      "id": "2022/09/14/the-big-update",
      "metadata": {
        "permalink": "/blog/2022/09/14/the-big-update",
        "source": "@site/blog/the-big-update.md",
        "title": "The Big Update",
        "description": "Well - we've got some big news! The past few weeks have been an incredibly busy",
        "date": "2022-09-14T11:55:04.000Z",
        "formattedDate": "September 14, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.97,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "The Big Update",
          "date": "2022-09-14T11:55:04.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/the-big-update/nspawn.webp",
          "slug": "2022/09/14/the-big-update"
        },
        "prevItem": {
          "title": "Ready, Set, Go",
          "permalink": "/blog/2022/11/18/ready-set-go"
        },
        "nextItem": {
          "title": "The Shopping List",
          "permalink": "/blog/2022/08/11/the-shopping-list"
        }
      },
      "content": "Well - we've got some big news! The past few weeks have been an incredibly busy\ntime for us, and we've hit some major milestones.\n\n![nspawn](/img/blog/the-big-update/nspawn.webp)\n\n\n<!--truncate-->\n\n# Funding\n\nAfter much deliberation - we've decided to pull out of Open Collective. Among other reasons, the fees\nare simply too high and severely impact the funds available to us. In our early stages, the team consensus\nis that funds generated are used to compensate my time working on Serpent OS.\n\nAs such I'm now moving funding to my own [GitHub Sponsors](https://github.com/sponsors/ikeycode?o=sd&sc=t) page - please do migrate! It ensures your entire\ndonation makes it and keeps the lights on for longer =) Please remember I'm working **full time** on Serpent OS\nexclusively - I need your help to keep working.\n\n# Moved to GitHub\n\nWe've pretty much completed our transition to GitHub. We've now got the following organisations:\n\n - [snekpit](https://github.com/snekpit) - Packaging work\n - [serpent-os](https://github.com/serpent-os) - Main code development (moss/boulder/etc)\n\n# Forums\n\nDon't forget - our forums are **live** over at [forums.serpentos.com](https://forums.serpentos.com) - please feel\nfree to drop in and join in with the community =)\n\n# Rehash on the tooling\n\nOK so what exactly *are* `moss` and `boulder`? In short - they're the absolute core pieces of our distribution model.\n\n## \"What is moss?\"\n\nOn the surface, moss looks and feels roughly the same as just about any other traditional package manager out there.\nInternally, however, its far more modern and has a few tricks up its sleeve. For instance, every time you initiate\nan operation in moss, be it installation, removal, upgrade, etc, a new filesystem transaction is generated. In short,\nif something is wrong with the new transaction - you can just boot to an older transaction when things worked fine.\n\nNow, it's not implemented using bundles or filesystem specific functionality, internally its just intelligent use of\nhardlinks and deduplication policies, and we have our own container format with zstd based payload compression. Our\nstrongly typed, deduplicating binary format is what powers moss.\n\nBehind the scenes we also use some other cool technology, such as LMDB for super reliable and speedy database access.\nThe net result is a next generation package management solution that offers all the benefits of traditional package\nmanagers (i.e. granularity and composition) with new world features, like atomic updates, deduplication, and repository\nsnapshots.\n\n## boulder\n\nIt's one thing to manage and install packages, it's another entirely to **build them**. `boulder` builds conceptually\non prior art such as `pisi` and the `package.yml` format used in `ypkg`. It is designed with automation\nand ease of integration in mind, i.e. less time spent focusing on **packaging** and more time on actually\ngetting the thing building and installing correctly.\n\nBoulder supports \"macros\" as seen in the RPM and ypkg world, to support consistent builds and integration.\nAdditionally it automatically splits up packages into the appropriate subpackages, and automatically scans\nfor binary, pkgconfig, perl and other dependencies during source analysis and build time. The end result\nis some `.stone` binary packages and a build manifest, which we use to flesh out our **source package index**.\n\n# Major moss improvements\n\nWe've spent considerable time reworking `moss`, our package manager. It now features\na fresher (terminal) user interface, progress bars, and is rewritten to use the\n[moss-db](https://github.com/serpent-os/moss-db) module encapsulating LMDB.\n\nIt's also possible to manipulate the binary collections (software repositories)\nused by moss now. Note we're going to rename \"remote\" to \"collection\" for consistency.\n\nAt the time of writing:\n\n```bash\n$ mkdir destdir\n$ sudo moss remote add -D destdir protosnek https://dev.serpentos.com/protosnek/x86_64/stone.index\n$ sudo moss install -D destdir bash dash dbus dbus-broker systemd coreutils util-linux which moss nano\n$ sudo systemd-nspawn -b -D destdir\n```\n\nThis will be simplified once we introduce `virtual` packages (Coming Soon &trade;)\n\n# Local Collections\n\nBoulder can now be instructed to utilise a local collection of stone packages, simplifying the development of large stack items.\n\n```bash\nsudo boulder bi stone.yml -p local-x86_64\n```\n\nPackages should be moved to `/var/cache/boulder/collections/local-x86_64` and the index\ncan be updated by running:\n\n```bash\nsudo moss idx /var/cache/boulder/collections/local-x86_64\n```\n\n\n# Self Hosting\n\nSerpent OS is now officially self hosting. Using our own packages, we're able to\nconstruct a root filesystem, then within that rootfs container we can use our own\nbuild tooling (`boulder`) to construct **new builds** of our packages in a nested\ncontainer.\n\nThe `protosnek` collection has been updated to include the newest versions of moss\nand boulder.\n\n![self hosting](/img/blog/the-big-update/self-hosting.webp)\n\n\n# Booting\n\nAs a fun experiment, we wanted to see how far along things are. Using a throwaway\nkernel + initrd build, we were able to get Serpent OS booting using virtualisation (`qemu`)\n\n![booting](/img/blog/the-big-update/booting.webp)\n\n# ISO When?\n\nRight now everyone is working in the [snekpit](https://github.com/snekpit) organisation to\nget our base packaging in order. I'm looking to freeze `protosnek`, our bootstrap collection,\nat the latest of tomorrow evening.\n\nWe now support layered, priority based collections (repositories) and dependency solving across\ncollection boundaries, allowing us to build our new `main` collection with `protosnek` acting as\na bootstrap seed.\n\nThroughout this week, I'll focus on getting Avalanche, Summit and Vessel into shape for PoC so\nthat we can enjoy automated builds of packages landing in the yet-to-be-launched `volatile` collection.\n\nFrom there, we're going to iterate + improve packaging, fixing bugs and landing features as we\ndiscover the issues. Initially we'll look to integrate **triggers** in a stateless-friendly\nfashion (our packages can only ship `/usr` by design) - after that will come boot management.\n\nAn early target will be Qemu support via a stripped `linux-kvm` package to accelerate the bring up,\nand we encourage everyone to join in the testing. We're self hosting, we know how to boot, and\nnow we're able to bring the awesome.\n\nI cannot stress how important the support to the project is. Without it - I'm unable to work full\ntime on the project. Please consider supporting my development work via [GitHub Sponsors](https://github.com/sponsors/ikeycode?o=sd&sc=t).\n\nThank you!\n\nYou can discuss this blog post over on [our forums](https://forums.serpentos.com/d/20-the-big-update)"
    },
    {
      "id": "2022/08/11/the-shopping-list",
      "metadata": {
        "permalink": "/blog/2022/08/11/the-shopping-list",
        "source": "@site/blog/the-shopping-list.md",
        "title": "The Shopping List",
        "description": "Quick on the heels of our last blog post - its high time for us to formulate \"the plan\".",
        "date": "2022-08-10T23:46:07.000Z",
        "formattedDate": "August 10, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.29,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "The Shopping List",
          "date": "2022-08-10T23:46:07.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/the-shopping-list/Featured.webp",
          "slug": "2022/08/11/the-shopping-list"
        },
        "prevItem": {
          "title": "The Big Update",
          "permalink": "/blog/2022/09/14/the-big-update"
        },
        "nextItem": {
          "title": "Let's Do This",
          "permalink": "/blog/2022/08/08/lets-do-this"
        }
      },
      "content": "Quick on the heels of our last blog post - its high time for us to formulate \"the plan\".\nWe'll split this up into a few sections so you know what needs to happen to proceed. Tl;DR\nwe need build machines and money to pull this off. **Now** is the time.\n\n![img](/img/blog/the-shopping-list/Featured.webp)\n\n<!--truncate-->\n\n# Immediate action plan\n\nOne major item we needed testing was `moss-db`, the bucket+ORM layer we built on top of\n`LMDB`. Interestingly this website is powered using `moss-db`, so we kinda know it works\nnow =)\n\nHere's the general \"hit list\" going forward:\n\n - Rebase `moss` on `moss-db`\n - Get `avalanche` to builds things\n - Get `summit` to control our new `snekbook` organisation on GitHub\n - Do builds\n - Add missing things to build an ISO.\n\n\n# Next Steps\n\nWe have all the major pieces of the codebase in place, such as `moss-deps` for dependency\nmanagement and analysis, `moss-db` for database abstraction, `moss-fetcher` for parallel\ndownloads, etc.\n\n`avalanche` is the easiest piece to implement:\n\n - Use `vibe.d` to listen on control port\n - Accept build messages (REST) from `summit` (auth'd with JWT)\n - Clone the payloads ref\n - Build it\n - Report status along with public URI to download assets\n\nWhen `summit` is informed of a successful build, it instructs `vessel` (the repo manager)\nto go and incorporate those assets, and then we can finally update the job status on the\ndashboard.\n\nThe repository manager is another simple `vibe.d` application which uses auth'd REST APIs,\nalong with `moss-format` for reading/writing moss files, and `moss-db` for persistence.\nEverything else is simply putting files in the right places and spitting out an index file\nfor clients.\n\nSo with all that tooling coming to a point of early <abbr title=\"Proof of concept\">PoC</abbr> fruition, what do we need?\n\n# What We Need\n\nHere it is, the shopping list for success :)\n\n## Web servers\n\nOur main website is living on an AWS node right now, which isn't sustainable. We need somewhere else\nfor this to live, along with the `Summit` dashboard.\n\nThe plan is to use our kindly provided [Fosshost](https://fosshost.org) system for the repository, enabling\nintegration with the mirror network.\n\n## Build servers\n\nWe do have access to partial-uptime systems in our developer network, but we don't have any statically configured\n\"beefy servers\" to enable builds.\n\nAt minimum, we'll need modern always-up x86-64 (`v3` + `v2`) build machines. To enable AArch64 down the road we\nwill also need dedicated build machines and a test system such as the Pinebook.\n\nFor a quick bring up we realistically need **two** `x86_64` build systems. They don't need boatloads of disk space,\nenough for 25-30GB build trees and 32GB RAM minimum. This is likely going to be quite costly each month, so sponsor-\nprovided hardware will be the most sustainable option. Cloud VPS with janky static kernels won't cut it.\n\n## Income\n\nThis one sounds stupid, doesn't it? Guy trying to bring up a project asking for income before its out. I feel dumb\nasking too. I'm pouring my heart, soul and experience into building this as I **know** it's not only going to be\nsuccessful, but highly disruptive. Most of us in the community are just trying to settle on something innovative\nthat **works** without breaking the classic reasons to use Linux.\n\nI'm incredibly close to getting us there. We have all the modules, its now a case of tying it all together into\na cohesive *whole*. We've taken the long route to ensure we'll have the tooling to scale when we kick off, I now\nneed **your help** to make us scale **now**.\n\n# And when you have all that..?\n\nSpit out ISOs. With our tooling we'll be able to rapidly go from no-repos to fully fledged test ISOs with a\ndesktop environment. We'll flesh the tooling out as we go along for triggers and boot management to facilitate\ninstalls and then its a case of stabilising/fleshing everything out in line with our vision.\n\nWithout further ado..\n\n## I'm a community member, how do I help?\n\nSign up on our [forums](https://forums.serpentos.com) and get ready to enter into a test-feedback cycle.\nVisit our [OpenCollective](https://opencollective.com/serpent-os) now to beef out the project income for\nboth myself and the admin fees for everything. The community is the heart and soul of Serpent OS, we need\n**you** to build the foundations for a fledgling project that will provide a technological home for many years\nto come.\n\nIf OpenCollective doesn't work as an option, we're in the process of setting up Stripe, and there is the possibility\nof direct bank transfer (EUR). Just send me an [email :)](mailto:ikey@serpentos.com)\n\n## I represent an organisation and I want to ensure the success of Serpent OS\n\nShoot me an [email now](mailto:ikey@serpentos.com) from your company email address. We're willing to work\nout mutual advertising partnerships that don't compromise the project in any way. Long story short, your\norganisation will forever be entwined in the success story of Serpent OS, a highly disruptive project offering\nsomething genuinely **new** to the world of Arch Linux and Fedora users, among other groups.\n\nLet's do this guys. We're on the cusp.\n\n[Discuss on the forums](https://forums.serpentos.com/d/9-the-shopping-list-serpent-os)"
    },
    {
      "id": "2022/08/08/lets-do-this",
      "metadata": {
        "permalink": "/blog/2022/08/08/lets-do-this",
        "source": "@site/blog/lets-do-this.md",
        "title": "Let's Do This",
        "description": "It's high time we had a project relaunch. What better way to do that than with a brand new website?",
        "date": "2022-08-08T14:08:21.000Z",
        "formattedDate": "August 8, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.135,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Let's Do This",
          "date": "2022-08-08T14:08:21.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/lets-do-this/Featured.webp",
          "slug": "2022/08/08/lets-do-this"
        },
        "prevItem": {
          "title": "The Shopping List",
          "permalink": "/blog/2022/08/11/the-shopping-list"
        },
        "nextItem": {
          "title": "Infrastructure Update",
          "permalink": "/blog/2022/07/10/infrastructure-update"
        }
      },
      "content": "It's high time we had a project relaunch. What better way to do that than with a brand new website?\nAlright I'll admit it's been a while since our last blogpost, so let's jump right into it then.\n\n![Ohhh yeaah](/img/blog/lets-do-this/Featured.webp)\n\n<!--truncate-->\n\nWe're at a point now where we'd like to go *live*.\n\nI repeat.\n\n# Go Live\n\nSo, to enable that, we're bringing up all the services required to support such a move. We got the\nnew website, forums, fixing up email, and putting the last touches on the build infrastructure to get\nthis boat moving.\n\n# New website\n\nSince the project began we've been chopping away at a Hugo based website. Over time it has become an\nabsolute nightmare to maintain, and has fallen by the wayside. Plus, it's absolutely <del>hideous</del> ungorgeous.\n\nFirst thing to note, we've implemented the new website with a [D Lang](https://dlang.org) backend using the [vibe.d](https://vibed.org) framework.\nThe frontend is implemented using a mixture of static templates and JavaScript rendering from a REST backend. This is\nwhat powers the new paginated [blog listing](/blog).\n\n## Static. Kinda\n\nThe new website is **kinda** static but also not. For now our <abbr title=\"Proof Of Concept\">POC</abbr> simply loads all of\nthe old content from the Hugo markdown files, and bakes them ahead-of-time into an [LMDB](https://www.symas.com/lmdb) powered database\nThen at runtime, we simply load the contents from the database and merge into a template.\n\n```d\nwriteln(\"Also, we're using Chroma for syntax highlighting, baked ahead of time\");\n```\n\n## Faster. Much Faster.\n\nDespite the old website being **entirely** static, loading all those files was ironically slow. The main web server now uses\nnginx and proxies requests to the `website` application, which in turn has most content accessible via `mmap` thanks to LMDB.\nAdditionally we've got proper caching policies, an improved stylesheet thanks to [tabler](https://tabler.io), and dropped all\nthe huge PNGs in the site in favour for `.webp` - resulting in significantly faster page load times.\n\n\n# Forums\n\nWe're still in the process of it but we got ourselves some brand new shiny forums! Due to an email host\nissue we're manually activating accounts, though you can login with GitHub OAuth and sidestep the issue.\nPlease do join in and meet the community, we're very excited to have you =)\n\n![Forums](/img/blog/lets-do-this/forums.webp)\n\n# To GitHub! (Again)\n\nIt hasn't quite happened just yet, but we're in the process of moving back to GitHub. I know some may question\nthis in terms of the openness of the platform, etc. In order for our build workflow to actually.. work, we need\norganisation-wide access tokens - which is something that GitLab offers for SaaS and self-hosted enterprise licenses.\n\nGitHub, on the other hand, has a very well documented GitHub Apps API and will allow us to implement our dashboard\nin a way that permissions, team roles and repositories can be controlled and synced to automate our workflow.\n\n# Next steps\n\nApart from some bugs in this website that will need resolving, all focus is on the build infrastructure. Once deployed\nwe'll have a rolling pipeline for package builds and an open infrastructure to permit collaboration on our packaging\nand updates. From there it's a race towards our first goals of getting everyone on Serpent OS installs for testing!\n\n# Lastly...\n\nI know the world isn't making things easier right now for any of us, so from the bottom of my heart I want to thank\neveryone who has been supporting the project, it means the absolute world to me. We're now in a position of community\nbuilding and developer enabling, a position we've fought the last 2 years to get to. Thanks to you all, we're hitting the\nlaunch button and going hypersonic.\n\nYou can leave feedback on this blog post over [on the forums](https://forums.serpentos.com/d/7-lets-do-this)"
    },
    {
      "id": "2022/07/10/infrastructure-update",
      "metadata": {
        "permalink": "/blog/2022/07/10/infrastructure-update",
        "source": "@site/blog/infrastructure-update.md",
        "title": "Infrastructure Update",
        "description": "Since the last post, I've pivoted to full time work on Serpent OS, which is",
        "date": "2022-07-10T10:26:05.000Z",
        "formattedDate": "July 10, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.93,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Infrastructure Update",
          "date": "2022-07-10T10:26:05.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/infrastructure-update/Featured.webp",
          "slug": "2022/07/10/infrastructure-update"
        },
        "prevItem": {
          "title": "Let's Do This",
          "permalink": "/blog/2022/08/08/lets-do-this"
        },
        "nextItem": {
          "title": "Packaging Automation, Next Steps",
          "permalink": "/blog/2022/06/22/packaging-automation-next-steps"
        }
      },
      "content": "Since the last post, I've pivoted to full time work on Serpent OS, which is\nmade all the more possible thanks to everyone supporting us via [OpenCollective](https://opencollective.com/serpent-os) <3.\n\nWe've been working towards establishing an online infrastructure to support the automation of package **builds**, while\nrevisiting some core components.\n\n![img](/img/blog/infrastructure-update/Featured.webp)\n\n<!--truncate-->\n\n# moss-db\n\nDuring the development of the Serpent OS tooling we've been exploring the possibilities of D Lang, picking up new\npractices and refining our approach as we go. Naturally, some of our older modules are somewhat ... _smelly_.\nMost noticeable is our `moss-db` module, which was initially intended as a lightweight wrapper around [RocksDB](http://rocksdb.org/).\n\nIn practice that required an encapsulation API written in D around the C API, and our own wrapping on top of that. Naturally,\nit resulted in a very allocation-heavy implementation that just didn't sit right with us, and due to the absurd complexity\nof RocksDB was still missing quite a few features.\n\n## Enter LMDB\n\nWe're now using the [Lightning Memory-Mapped Database](https://www.symas.com/lmdb) as the driver implementation\nfor moss-db. In short, we get rapid reads, ACID transactions, bulk inserts, you name it. Our implementation takes\nadvantage of multiple database indexes (`MDB_dbi`) in LMDB to partition the database into internal components,\nso that we can provide \"buckets\", or collections. These internal DBs are used for bucket mapping to permit a\nkey-compaction strategy - iteration of top level buckets and key-value pairs within a bucket.\n\n## Hat tip, boltdb\n\nThe majority of the API was designed with the [boltdb](https://github.com/boltdb/bolt) API in mind. Additionally\nit was built with `-preview=dip1000` and `-preview=in` enabled, ensuring safely scoped memory use and no\nroom for memory lifetime issues. While we prefer the use of generics, the API is built with `immutable(ubyte[]`)\nas the internal key and value type.\n\nCustom types can simply implement `mossEncode` or `mossDecode` to be instantly serialisable into the database\nas keys, values or bucket identifiers.\n\nExample API usage:\n\n```d\nDatabase db;\n/* setup the DB with lmdb:// URI */\n\n/* Write transaction */\nauto err = db.update((scope tx) @safe\n{\n    auto bucket = tx.bucket(\"letters\");\n    return tx.set(bucket, \"a\", 1);\n});\n\n/* do something with the error */\n\nerr = db.view((in tx) @safe\n{\n    foreach (name, bucket ; tx.buckets!int)\n    {\n        foreach (key, value ; tx.iterator!(string,string)(bucket))\n        {\n            /* do something with the key value pairs, decoded as strings */\n        }\n    }\n\n    /* WILL NOT COMPILE. tx is const scope ref :) */\n    tx.removeBucket(\"numbers\");\n\n    return NoDatabaseError;\n}\n```\n\n## Next for moss\n\nMoss will be ported to the new DB API and we'll gather some performance metrics,\nwhile implementing features like expired state garbage collection (disk cleanup),\nsearching for names/descriptions, etc.\n\n# Avalanche\n\n![Early version of Avalanche, in development](/img/blog/infrastructure-update/Featured.webp)\n\nAvalanche is a core component of our upcoming infrastructure, providing the\nservice for running builds on a local node, and a controller to coordinate\na group of builders.\n\nSummit will be the publicly accessible project dashboard, and will be responsible\nfor coordinating incoming builds to Avalanche controllers and repositories.\nDevelopers will submit builds to Summit and have them dispatched correctly.\n\nSo far we have the core service process in place for the Controller + Node,\nand now we're working on persistence and handshake. TLDR; fancy use of\nmoss-db and JSON Web tokens over mandated SSL. This means our build infra\nwill be scalable from day 1 allowing multiple builders to be online very\nearly on.\n\n# Timescale\n\nWe're planning to get an early version of our infrastructure up and running\nwithin the next 2 weeks, and get builds flowing =)"
    },
    {
      "id": "2022/06/22/packaging-automation-next-steps",
      "metadata": {
        "permalink": "/blog/2022/06/22/packaging-automation-next-steps",
        "source": "@site/blog/packaging-automation-next-steps.md",
        "title": "Packaging Automation, Next Steps",
        "description": "Hot damn we've been busy lately. No, really.",
        "date": "2022-06-22T16:23:14.000Z",
        "formattedDate": "June 22, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.62,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Packaging Automation, Next Steps",
          "date": "2022-06-22T16:23:14.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/packaging-automation-next-steps/Featured.webp",
          "slug": "2022/06/22/packaging-automation-next-steps"
        },
        "prevItem": {
          "title": "Infrastructure Update",
          "permalink": "/blog/2022/07/10/infrastructure-update"
        },
        "nextItem": {
          "title": "A Word From The Founder",
          "permalink": "/blog/2022/06/06/a-word-from-the-founder"
        }
      },
      "content": "Hot damn we've been busy lately. No, [really](https://gitlab.com/groups/serpent-os/-/activity).\nThe latest development cycle saw us focus exclusively on `boulder`, our build tooling. As of\ntoday it features a proof of concept `boulder new` subcommand for the automatic generation of\npackaging templates from an upstream source (i.e. tarball).\n\n![Look at all the buildiness](/img/blog/packaging-automation-next-steps/Featured.webp)\n\n\n<!--truncate-->\n\nBefore we really start this blog post off, I'd like to thank everyone who is supporting the\nproject! All of the [OpenCollective](https://opencollective.com/serpent-os) contributions will make it easier for me to work full\ntime on Serpent OS =) Much love <3\n\n\n# But, but build _flavours_ ...\n\nAlright you got me there, certain projects prefer to abstract the configuration, build and\ninstallation of packages and be provided with some kind of hint to the build system, i.e.\nmanually setting `autotools`, etc.\n\nSerpent OS packaging is declarative and well structured, and relies on the use of RPM-style\n\"macros\" for distribution integration and common tasks to ensure consistent packaging.\n\nWe prefer a self documenting approach that can be machine validated rather than depending\non introspection at the time of build. Our `stone.yml` format is very flexible and powerful,\nwith automatic runtime dependencies and package splitting as standard.\n\n..Doesn't mean we can't make packaging **even easier** though.\n\n# Build discovery\n\nPointing boulder at an upstream source will perform a deep analysis of the sources to determine\nthe build system type, build dependencies, metadata, licensing etc. Right now it's just getting\nready to leave POC stage so it has a few warts, however it does have support for generating\npackage skeletons for the following build systems:\n\n - `cmake`\n - `meson`\n - `autotools`\n\nWe're adding automation for Perl and Python packaging (and Golang, Rust, etc) so we can enforce consistency,\nintegration and ease without being a burden on developers. This will greatly reduce the friction\nof contribution - allowing anyone to package for Serpent OS.\n\nWe're also able to automatically discover build time dependencies during analysis and add those\nto the skeleton `stone.yml` file. We'll enhance support for other build systems as we go, ensuring\nthat each new package is as close to done on creation as possible, with review and iteration left\nto the developer.\n\n# License compliance\n\nA common pain in the arse when packaging for *any* Linux distribution is ensuring the package\ninformation is *compliant* in terms of licensing. As such we must know all of the licensing\ninformation, as well as FSF and OSI compliance for our continuous integration testing.\n\n...Finding all of that information is truly a painful process when conducted manually.\nThankfully `boulder` can perform analysis of all licensing files within the project to\ngreatly improve compliance and packaging.\n\nEvery license listed in a `stone.yml` file must use a valid [SPDX](https://spdx.dev/) identifier,\nand be accurate. boulder now scans all license files, looking for matches with both SPDX IDs\nas well as fuzzy-matching the text of input licenses to make a best-guess at the license ID.\n\nThis has so far been highly accurate and correctly identifies many hundreds of licenses,\nensuring a compliant packaging process with less pain for the developers. Over time we'll\noptimise and improve this process to ensure compliance *for* our developers rather than\nblocking them.\n\nAs of today we support the [REUSE](https://reuse.software/) specification for expressing software licenses too!\n\n# Next on the list\n\nThe next steps are honest-to-goodness exciting for us. Or should I say.. exiting?\n\n## bill\n\nWork formally begins now on Bootstrap Bill (Turner). Whilst we did successfully bootstrap Serpent OS\nand construct the `Protosnek` repository, the process for that is **not** reproducible as `boulder`\nhas gone through massive changes in this time.\n\nThe new [project](https://gitlab.com/serpent-os/core/bill) will leverage `boulder` and a newly\ndesigned bootstrap process to eliminate all host contamination and bootstrap Serpent OS from\n`stone.yml` files, emitting an _immutable_ bootstrap repository.\n\nLayering support will land in `moss` and `boulder` to begin the infrastructure projects.\n\n## Build Submission (\"Let's Roll\")\n\nThe aim is to complete `bill` in a very short time so we can bring some initial infrastructure\nonline to facilitate the automatic build of submitted build jobs. We'll use this process\nto create our live repository, replacing the initial bootstrap repository from bill.\n\nAt this point all of the tooling we have will come together to allow us all to very\nquickly iterate on packaging, polish up `moss` and race towards installed systems with\nonline updates."
    },
    {
      "id": "2022/06/06/a-word-from-the-founder",
      "metadata": {
        "permalink": "/blog/2022/06/06/a-word-from-the-founder",
        "source": "@site/blog/a-word-from-the-founder.md",
        "title": "A Word From The Founder",
        "description": "Well well, it's been a long time since I personally wrote a post.. :) So let's keep",
        "date": "2022-06-06T19:36:51.000Z",
        "formattedDate": "June 6, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.22,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "A Word From The Founder",
          "date": "2022-06-06T19:36:51.000Z",
          "draft": false,
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2022/06/06/a-word-from-the-founder"
        },
        "prevItem": {
          "title": "Packaging Automation, Next Steps",
          "permalink": "/blog/2022/06/22/packaging-automation-next-steps"
        },
        "nextItem": {
          "title": "RELR Brings Smaller Files, More Performance?",
          "permalink": "/blog/2022/04/05/relr-brings-smaller-fixes-more-performance"
        }
      },
      "content": "Well well, it's been a long time since I personally wrote a post.. :) So let's keep\nthis short and sweet, shall we? I'm returning to full time work on Serpent OS.\n\n<!--truncate-->\n\nThe 6th of July will be my last day at my current employment having tendered my\n30 day notice today. Despite having enjoyment at my current position, the reality\nis that my passion and focus is Serpent OS.\n\nI'm now in a transition process and will ramp up my efforts with Serpent OS.\nRealistically I need to reduce the outgoing costs of the project and with\nyour help I can gain some level of financial support as we move through the\nnext stages of development. Worst case, I will only take on any part-time or\ncontractual gigs, allowing my primary focus to be Serpent OS.\n\nI'll begin accelerating works and enabling community contribution so we can\nget the derailed-alpha train back on the tracks.\n\nI have absolute faith in this project, the community and our shared ability\nto deliver the OS and tooling. To achieve it will require far more of my time\nand I'm perfectly willing to give it.\n\nThank you all to everyone who has been supporting the project, it is now\ntime to deliver. Not just another run of the mill distribution but a technically\ncompetent and usable distribution that is not only different but *better*.\n\nLet's do this in the most grassroots and enjoyable way possible =)"
    },
    {
      "id": "2022/04/05/relr-brings-smaller-fixes-more-performance",
      "metadata": {
        "permalink": "/blog/2022/04/05/relr-brings-smaller-fixes-more-performance",
        "source": "@site/blog/relr-brings-smaller-files-more-performance.md",
        "title": "RELR Brings Smaller Files, More Performance?",
        "description": "RELR is an efficient method of storing relative relocations (but is not yet available in glibc upstream). This has a",
        "date": "2022-04-05T01:12:54.000Z",
        "formattedDate": "April 5, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 7.145,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "RELR Brings Smaller Files, More Performance?",
          "date": "2022-04-05T01:12:54.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/relr-brings-smaller-fixes-more-performance/Featured.webp",
          "slug": "2022/04/05/relr-brings-smaller-fixes-more-performance"
        },
        "prevItem": {
          "title": "A Word From The Founder",
          "permalink": "/blog/2022/06/06/a-word-from-the-founder"
        },
        "nextItem": {
          "title": "Making Deltas Great Again! (Part 1)",
          "permalink": "/blog/2022/02/11/making-deltas-great-again-1"
        }
      },
      "content": "`RELR` is an efficient method of storing relative relocations (but is not yet available in `glibc` upstream). This has a\nsignificant reduction on file size often in the vicinity of 5% for libraries and even higher for PIE binaries. We also\ntake a look at the performance impact on enabling `RELR` and that looks really good too! Smaller files with more\nperformance - you **can** have your cake and eat it too!\n\n<!--truncate-->\n\n# Delicious Size Savings\n\nEveryone enjoys smaller files, especially when it's for free! `RELR` provides a very efficient method of storing\nrelative relocations where it only requires a few percent compared to storing them in the `.rela.dyn` section which is\nwhat currently happens. However, it can't store everything so the `.rela.dyn` section remains (though much smaller).\n\nHere's an example of the sections of `libLLVM-13.so` with and without `RELR`. Here we see the `.rela.dyn` section taking\nup a whole 2MB! When enabling `RELR`, `.rela.dyn` shrinks down to just under 100KB while adding a new section\n`.relr.dyn` which is just over 20KB! That's nearly a 1.9MB file size reduction, so you'll get smaller packages, smaller\nupdates and it will be even faster to create delta packages from our servers. For reference, some of the biggest files\nhave a `.rela.dyn` section over 10MB!\n\n| Section   | Without RELR | With RELR  |\n|:---------:|--------------|------------|\n| .dynstr   |  2,285,738   |  2,285,723 |\n| .rela.dyn |  2,006,472   |    97,464  |\n| .relr.dyn |      -       |    21,688  |\n| .text     | 28,708,290   | 28,708,386 |\n| .dynamic  |      592     |      608   |\n|           |              |            |\n| Total     | 44,853,987   | 42,966,764 |\n\n# Smaller, But at What Cost?\n\nWhile most of the discussion about `RELR` is around the size savings, there's been very little in terms of the\nperformance numbers of enabling `RELR`. For most things, it's not going to make a noticeable difference, as it should\nonly really impact loading of binaries. There's one rule we have and that's to measure everything! We care about every\nlittle detail where many 1-2% improvements can add up to significant gains.\n\nFirst, we require a test to determine if we could detect changes between an `LLVM` built with `RELR` and one without.\nThe speed of the compiler is vital to a distro, where lackluster performance of the build system hurts all contributors\nand anyone performing source based builds. `clang` in this example was built using a shared `libLLVM` so that it would\nload the `RELR` section and it's large enough to be able to measure a difference in load times (if one exists). Building\n`gettext` was the chosen test (total time includes tarball extraction, configure, make and make install stages), rather\nthan a synthetic binary loading test to reflect real world usage. The configure stage is very long when building\n`gettext` so `clang` is called many times for short compiles. Lets take a look at the results:\n\n```\nno RELR:\n[Build] Finished: 1 minute, 57 secs, 80 ms, 741 μs, and 2 hnsecs\n[Build] Finished: 1 minute, 57 secs, 691 ms, 586 μs, and 4 hnsecs\n[Build] Finished: 1 minute, 56 secs, 861 ms, 31 μs, and 8 hnsecs\n\nRELR:\n[Build] Finished: 1 minute, 55 secs, 244 ms, 213 μs, and 8 hnsecs\n[Build] Finished: 1 minute, 55 secs, 400 ms, 158 μs, and 8 hnsecs\n[Build] Finished: 1 minute, 55 secs, 775 ms, 40 μs, and 8 hnsecs\n\nRELR+startup patch:\n[Build] Finished: 1 minute, 54 secs, 979 ms, 166 μs, and 8 hnsecs\n[Build] Finished: 1 minute, 54 secs, 820 ms, and 675 μs\n[Build] Finished: 1 minute, 54 secs, 713 ms, 440 μs, and 3 hnsecs\n```\n\nHere we see the base configuration was able to build `gettext` in 117.21s on average. When we enabled `RELR` in our\n`LLVM` build (all other packages were without `RELR` still), the average build time decreased by 1.74s! That does not\nsound like a lot, but the time spent loading `clang` would only be a portion of the total, yet still gives a 1-2%\nperformance lift over the whole build. While we were reducing start up time, I ran another test, but this time adding a\npatch to reduce paths searched on startup as well as enabling `RELR`. This patch reduced the average build time by a\nfurther 0.63s!\n\nThat's a 2.37s reduction in the build just from improving the `clang` binary's load time.\n\n# What This Means - RELR by Default\n\nSo what actually is `RELR`? I can't really do the topic justice, so will point you to a great blog post about RELR,\n[Relative Relocations and RELR](https://maskray.me/blog/2021-10-31-relative-relocations-and-relr). It's quite technical\nfor the average reader, but definitely worth a read if you like getting into the details. To no surprise the author\n(Fangrui Song) started the initial push for getting `RELR` support upstream in `glibc` (at the time of this post the\npatch series has not yet been committed to `glibc` git).\n\nWhat I can tell you, is that we've applied the requisite patches for `RELR` support and enabled `RELR` by default in\n`boulder` for builds. Our container has been rebuilt and all is working well with `RELR` enabled. More measurements will\nbe done in future in the same controlled manner, particularly around PIE load times.\n\n# Caveats - The Hidden Details\n\nThe performance benchmark was quite limited in terms of being an optimal case for `RELR` as `clang` is called thousands\nof times in the build so on average improved load time by about 0.6-0.7ms. We can presume that using `RELR` on smaller\nfiles is unlikely to regress load times. It definitely gives us confidence that it would be about the same or better in\nmost situations, but not noticeable or measurable in most use cases. Minimizing build times is a pretty significant\ntarget for us, so even these small gains are appreciated.\n\nThe size savings can vary between packages and not everything can be converted into the `.relr.dyn` section. The current\ndefault use of `RELR` is not without cost as it adds a version dependency on `glibc`. We will ensure we ship a sane\nimplementation that minimizes or removes such overhead.\n\nIt was also not straight forward to utilize `RELR` in Serpent. The pending upstream `glibc`\n[patch series](https://sourceware.org/pipermail/libc-alpha/2022-February/136290.html) included a patch which caused\nissues when enabling `RELR` in Serpent OS (patch 3/5). As we utilize two toolchains, `gcc/bfd` and `clang/lld`, both\nneed to function independently to create outputs of a functional OS. However the part \"Issue an error if there is a\nDT_RELR entry without GLIBC_ABI_DT_RELR dependency nor GLIBC_PRIVATE definition.\" meant that `glibc` would **refuse to\nload files linked by `lld` despite having the ability to load them**. `lld` has supported `RELR` for some time already,\nbut does not create the `GLIBC_ABI_DT_RELR` dependency that is required by `glibc`. I have added my\n[feedback](https://sourceware.org/pipermail/libc-alpha/2022-March/136773.html) to the patch set upstream. `lld` now has\nsupport for this version dependency upstream if we ever decide to use it in future.\n\nAfter dropping the patch and patching `bfd` to no longer generate the `GLIBC_ABI_DT_RELR` dependency either, I was\nfinally able to build both `glibc` and `LLVM` with the same patches. With overcoming that hurdle, rebuilding the rest of\nthe repository went without a hitch, so we are now enjoying `RELR` in all of our builds and is enabled by default.\n\nThere is even further scope for more size savings, by switching the `rela.dyn` section for the `rel.dyn` section (this\nis what is used for 32-bit builds and one of the reasons files are smaller!). `lld` supports switching the section type,\nbut I don't believe `glibc` will read the output as it expects the `psABI` specified section (something `musl` can\nhandle though).\n\n![34 wasted bytes with GLIBC_ABI_DT_RELR](/img/blog/relr-brings-smaller-files-more-performance/Featured.webp)\n\n# The Cost of Adding GLIBC_ABI_DT_RELR\n\nA quick check of two equivalent builds (one adding the `GLIBC_ABI_DT_RELR` version dependency and one not), there was an\nincrease of 34 bytes to the file's sections (18 bytes to `.dynstr` and 16 bytes to `.gnu.version_r`). It also means\nhaving to validate that the `GLIBC_ABI_DT_RELR` version is present in the `libc` and that the file using `RELR` includes\nthis version dependency. This may not sound like much but it is completely unnecessary! Note that the testing provided\nin this blog post is without `GLIBC_ABI_DT_RELR`.\n\nRegardless of what eventuates, these negatives won't ship in Serpent OS. This will allow for us to support files that\ninclude the version dependency (when appimage and other distros catch up) as it will still exist in `libc`, but we won't\nhave the version check in files, nor will `glibc` check that the version exists before loading for packages built by\n`boulder`."
    },
    {
      "id": "2022/02/11/making-deltas-great-again-1",
      "metadata": {
        "permalink": "/blog/2022/02/11/making-deltas-great-again-1",
        "source": "@site/blog/making-deltas-great-again-1.md",
        "title": "Making Deltas Great Again! (Part 1)",
        "description": "In Optimising Package Distribution we discussed some early",
        "date": "2022-02-11T09:45:12.000Z",
        "formattedDate": "February 11, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 14.445,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Making Deltas Great Again! (Part 1)",
          "date": "2022-02-11T09:45:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/making-deltas-great-again-1/Featured.webp",
          "slug": "2022/02/11/making-deltas-great-again-1"
        },
        "prevItem": {
          "title": "RELR Brings Smaller Files, More Performance?",
          "permalink": "/blog/2022/04/05/relr-brings-smaller-fixes-more-performance"
        },
        "nextItem": {
          "title": "Pedal to the Metal",
          "permalink": "/blog/2022/01/25/pedal-to-the-metal"
        }
      },
      "content": "In [Optimising Package Distribution](/blog/2021/03/16/optimising-package-distribution) we discussed some early\nfindings for implementing binary deltas in Serpent OS. While discussing the implementation we have found the\nrequirements to be suboptimal for what we were after. We provide a fresh look at the issue and what we can do to make it\nuseful in almost all situations without the drawbacks.\n\n<!--truncat-->\n\n# Deltas Have a Bad Reputation\n\nI remember back in the early 2000s on Gentoo when someone set up a server to produce delta patches from source tarballs\nto distribute to users with small data caps such as myself. When requesting the delta, the job would be added to the\nqueue (which occasionally could be minutes) and then created a small patch to download. This was so important at the\ntime to reduce the download size that the extra time was well worth it!\n\nToday things are quite different. The case for deltas has reduced for users as internet speeds have increased. Shrinking\n20MB off a package size may be a second reduction for some, but 10 seconds for others. The largest issue is that deltas\nhave typically pushed extra computational effort onto the users computer in compensation for the smaller size. With a\nfast internet connection that cost is a real burden where deltas take longer to install than simply downloading the full\npackage.\n\n# What's so Bad About the Old Method?\n\nThe previous idea of using the full payload for deltas was very efficient in terms of distribution, but required changes\nin how `moss` handles packages to implement it. Having the previous payload available (and being fast) means storing the\nold payloads on disk. This increases the storage requirements for the base system, although that can be reduced by\ncompressing the payloads to reduce disk usage (but increasing CPU usage at install time).\n\nTo make it work well, we needed the following requirements:\n- Install all the files from a package and recreate the payload from the individual files. However, Smart System\n  Management allows users to avoid installing unneeded locale files and we would not be able to recreate the full\n  payload without them.\n- Alternatively we can store the full payloads on disk. Then there becomes a tradeoff from doubling storage space or\n  additional overhead from compressing the payloads to reduce it.\n- Significant memory requirements to use a delta when a package is large.\n\nIn short we weren't happy with having to increase the disk storage requirements (potentially more than 2x) or the\nincrease in CPU time to create compressed payloads to minimize it. This was akin to the old delta model, of smaller\ndownloads but significantly slower updates.\n\n# Exploring an Alternative Approach\n\nOptimal compression generally benefits from combining multiple files into one archive than to compress each file\nindividually. With `zstd` deltas, since you read in a dictionary (the old file), you already have good candidates for\ncompression matching. The real question was simply whether creating a delta for each file was a viable method for delta\ndistribution (packaged into a single payload of course).\n\nAs the real benefit of deltas is reducing download size, the first thing to consider is the size impact. Using the same\npackage as the previous blog post (but with newer versions of `QtWebEngine` and `zstd`) we look at what happens when you\ndelta each file individually. Note that the package is quite unique in that the largest file is 76% of the total package\nsize and that delta creation time increased a lot for files larger than 15-20MB at maximum compression.\n\n|                 | Full Tarball | Individual Files | Full Tarball --zstd=chainLog=30 | Individual Files --zstd=chainLog=30 |\n|-----------------|--------------|------------------|---------------------------------|-------------------------------------|\n| Time to create  | 134.6s       | 137.6s           | 157.9s                          | 150.9s                              |\n| Size of delta   | 30.8MB       | 29.8MB           | 28.3MB                          | 28.6MB                              |\n| Peak Memory     | 1.77GB       | 1.64GB           | 4.77GB                          | 2.64GB                              |\n\nQuite surprisingly, the delta sizes were very close! Most surprising was that without increasing the size of the\nchainLog in `zstd`, the individual file approach actually resulted in smaller deltas! We can also see how much lower the\nmemory requirements were (and they would be much smaller when there isn't one large file in the package!). Our locale\nand doc trimming of files will still work nicely, as we don't need to recreate the locale files that aren't installed\n(as we still don't want them!).\n\nThe architecture of `moss` allows us to cache packages, where all cached packages are available for generating multiple\nsystem roots including with our build tool `boulder` without any need for the original package file. Therefore any need\nto retain old payloads or packages is no longer required or useful, eliminating the drawbacks of the previous delta\napproach. The memory requirements are also reduced as the maximum memory requirement scales with the size of the largest\nfile, rather than the entire package (which is generally a lot bigger). There are many packages containing hundreds of\nMBs of uncompressed data and a few into the GBs. But the largest file I could find installed locally was only 140MB, and\nonly a handful over 100MB. This smaller increase in memory requirements is a huge improvement and the small increase in\nmemory use to extract the deltas is likely to go unnoticed by users.\n\nWell everything sounds pretty positive so far, there must be some drawback?\n\n![](/img/blog/making-deltas-great-again-1/Featured.webp)\n\n# The Impact on Package Cache Time\n\nAs the testing method for this article is quite simplistic (`bash` loops and calls to `zstd` directly), the additional\noverhead from creating deltas for individual files I estimated to be about 20ms compared to a proper solution. The main\ndifference from the old delta method is how we extract the payloads and recreate the files of the new package. Using the\nfull package you simply extract the content payload and split it into its corresponding files. The new approach requires\ntwo steps, extracting the payload (we could in theory not compress it) and then applying patches to the original files\nto recreate the new ones. Note that times differ slightly from the previous table due to minor variations between test\nruns.\n\n|                                      | Normal Package   | Individual Delta File Package |\n|--------------------------------------|------------------|-------------------------------|\n| Time to Delta Files                  | -                | 148.0s (137 files)            |\n| Time to Compress Payload             | 78.6s            | 4.0s                          |\n| Size of Uncompressed Payload         | 165.8MB          | 28.9MB                        |\n| Size of Compressed Payload           | 51.3MB           | 28.6MB                        |\n| Instructions to Extract Payload      | 2,876.9m (349ms) | 33.1m (34ms)                  |\n| Instructions to Recreate Files       | -                | 1,785.6m (452ms)              |\n|                                      |                  |                               |\n| Total instructions to Extract        | 2,876.9m (349ms) | 1,818.7m (486ms)              |\n\nWhat's important to note is that is this reflects a worst case scenario for the delta approach, where all 137 files were\ndifferent between the old and new version of the package. Package updates where files remain unchanged allows us to omit\nthem from the delta package altogether! So the delta approach not only saves time downloading files, but also requires\nfewer CPU instructions to apply the update. It's not quite a slam dunk though as reading the original file as a\ndictionary results in an increase in elapsed time of extraction (though the extra time is likely much less than the time\nsaved downloading 20MB less!).\n\nIn Part 2 we will look at some ways we can tweak the approach to balance the needed resources for creating delta\npackages and to reduce the time taken to apply them.\n\n```\n\nNote: This was intended to be a 2 part series as it contains a lot of information to digest.\nHowever, Part 2 was committed and follows below.\n\n```\n\nThere's more than one way to create a delta! This post follows on from the earlier analysis of creating some basic\ndelta packages. Where this approach, and the use of `zstd`, really thrives is that it gives us options in how to manage\nthe overhead, from creating the deltas to getting them installed locally. Next we explore some ideas of how we can\nminimize the caching time of delta files.\n\n# Improving Delta Cache Time\n\nTo get the best bang for your buck with deltas, it is essential to reduce the size of the larger files. My experience in\ntesting was that there wasn't a significant benefit from creating deltas for small files. In this example, we only\ncreate delta files when they are larger than a specific size while including the full version of files that are under\nthe cutoff. This reduces the number of delta files created without impacting the overall package size by much.\n\n| Only Delta Files Greater Than        | Greater than 10KB | Greater than 100KB | Greater than 500KB |\n|--------------------------------------|-------------------|--------------------|--------------------|\n| Time to Delta Files                  | 146.1s (72 files) | 146.3s (64 files)  | 139.4s (17 files)  |\n| Time to Compress Payload             | 3.9s              | 4.0s               | 8.3s               |\n| Size of Uncompressed Payload         | 28.9MB            | 29.3MB             | 42.4MB             |\n| Size of Compressed Payload           | 28.6MB            | 28.7MB             | 30.5MB             |\n| Instructions to Extract Payload      | 37.8m (36ms)      | 34.7m (29ms)       | 299.1m (66ms)      |\n| Instructions to Recreate Files       | 1,787.7m (411ms)  | 1,815.0m (406ms)   | 1,721.4m (368ms)   |\n|                                      |                   |                    |                    |\n| Total instructions to Extract        | 1,825.5m (447ms)  | 1,849.7m (435ms)   | 2,020.5m (434ms)   |\n\nHere we see that by not creating deltas for files under 100KB, it barely impacts the size of the delta at all, while\nreducing caching time by 50ms compared to creating a delta for every file (486ms from the previous blog post). It even\nleads to up to a 36% reduction in CPU instructions to undertake caching through the delta than using the full package.\nIn terms of showing how effective this delta technique really is, I chose one of the worst examples and I would expect\nthat many deltas would be faster to cache when there's files that are exact matches between the old and new package. The\nlargest file alone took 300ms to apply the delta, where overheads tend to scale a lot when you start getting to larger\nfiles.\n\nThere are also some steps we can take to make sure that caching a delta is almost always faster than the full package\n(solving the only real drawback to users), only requiring Serpent OS resources to create these delta packages.\n\n# Creating More Efficient Deltas\n\nFor this article, all the tests have been run with `zstd --ultra -22 --zstd=chainLog=30`...until now! The individual\nfile delta approach is more robust at lower compression levels to keep package size small while reducing how long they\ntake to create. Lets take a look at the difference while also ensuring `--long` is enabled. This testing combined with\nthe results above for only creating deltas for files larger than 10KB.\n\n| Individual Delta File Package        | zstd -12         | zstd -16         | zstd -19         | zstd -22         |\n|--------------------------------------|------------------|------------------|------------------|------------------|\n| Time to Delta Files                  | 6.7s             | 113.9s           | 142.3s           | 148.3s           |\n| Time to Compress Payload             | 0.5s             | 3.2s             | 5.3s             | 4.0s             |\n| Size of Uncompressed Payload         | 41.1MB           | 30.6MB           | 28.9MB           | 28.9MB           |\n| Size of Compressed Payload           | 40.9MB           | 30.3MB           | 28.6MB           | 28.6MB           |\n| Instructions to Extract Payload      | 46.5m (35ms)     | 51.2m (28ms)     | 42.6m (33ms)     | 37.8m (36ms)     |\n| Instructions to Recreate Files       | 1,773.7m (382ms) | 1,641.5m (385ms) | 1,804.2m (416ms) | 1,810.9m (430ms) |\n|                                      |                  |                  |                  |                  |\n| Total instructions to Extract        | 1,820.2m (417ms) | 1,692.7m (413ms) | 1,846.8m (449ms) | 1,848.7m (466ms) |\n\nCompression levels 16-19 look quite interesting where you start to reduce the time taken to apply the delta as well\nand only seeing a small increase in size. For comparison, at `-19` it only took 9s to delta the remaining 39.8MB of\nfiles when excluding the largest file (it was 15s at `-22`). While the time taken between 19 and 22 was almost the same,\nat `-19` it took 27% fewer instructions to create the deltas than at `-22` (`-16` uses 64% fewer instructions than\n`-22`). It will need testing across a broader range of packages to see the overall impact and to evaluate a sensible\ndefault.\n\nAs a side effect of reducing the compression level, you also get another small decrease in the time to cache a package.\nThe best news of all is that these numbers are already out of date. Testing was performed last year with `zstd` 1.5.0,\nwhere there have been notable speed improvements to both compression and decompression that have been included in newer\nreleases. Great news given how fast it is already! Here's a quick summary of how it all ties together.\n\n# Delta's Are Back!\n\nThis blog series has put forward a lot of data that might be difficult to digest...but what does it mean for users of\nSerpent OS? Here's a quick summary of the advantages of using this delta method on individual files when compared to\nfetching the full packages:\n\n- Package update sizes are greatly reduced to speed up fetching of package updates.\n- `moss` architecture means that we have no need to leave packages on disk for later use, reducing the storage\n  footprint. In fact, we could probably avoid writes (except for the extracted package of course!) by downloading\n  packages to `tmpfs` where you have sufficient free memory.\n- Delta's can be used for updating packages for packaging **and** your installed system. There's no need for a full\n  copy of the full original package for packaging. A great benefit when combined with the source repository.\n- Delta's are often overlooked due to being CPU intensive while most people have pretty decent internet speeds. This has\n  a lot to do with how they are implemented.\n- With the current vision for Serpent OS deltas they will require fewer CPU instructions to use than full packages, but\n  may slightly increase the time to cache some packages (but we are talking ms). But we haven't even considered the\n  reduction in time taken to download the delta vs the full package which more than makes up the difference!\n- The memory requirements are reduced compared to the prior delta approach, especially if you factor in extracting the\n  payload in memory (possibly using `tmpfs`) as part of installation.\n\n# Getting More Bang for Your Buck\n\nThere's still plenty more work to be done for implementing delta's in Serpent OS and they likely aren't that helpful\nearly on. To make delta packages sustainable and efficient over the long run, we can make them even better and reduce\nsome wastage. Here are some more ideas in how to make deltas less resource intensive and better for users:\n\n- As we delta each file individually, we could easily use two or more threads to speed up caching time. Using this\n  package as an example, two threads would reduce the caching time to 334ms, the time the largest file took to recreate\n  plus the time to extract the payload. Now the delta takes less time and CPU to cache than the full package!\n- `zstd` gives us options to tradeoff some increase in delta size to reduce the resources needed to create delta\n  packages. This testing was performed with `--ultra -22 --zstd=chainLog=30` which is quite slow, but produces the\n  smallest files. Even reducing the compression level to `-16 --long` didn't result in a large increase in delta size.\n- We always have the option to not create deltas for small packages to ease the burden, but in reality the biggest\n  overhead is created from large files.\n- When creating deltas, you typically generate them for multiple prior releases. We can use smart criteria when to stop\n  delta generation from earlier releases for instance if they save less than 10% total size or less than 100KB. A delta\n  against an earlier release will almost always be larger than versus a more recent release.\n\n# Even Better than These Numbers Suggest\n\nWhile the numbers included have been nothing short of remarkable, they don't quite reflect how good this approach will\nbe. The results shown lack some of the key advantages of our delta approach such as excluding files that are unchanged\nbetween the two packages. Other things that will show better results are:\n\n- When package updates include minimal changes between versions (and smaller files), we would expect the average package\n  to be much closer in elapsed time than indicated in these results.\n- A quick test using a delta of two package builds of the same source version resulted in a 13MB delta (rather than the\n  28.6MB delta we had here). On top of that it took 62% fewer CPU instructions and less time (295ms) than the full\n  package to extract (349ms) without resorting to multi-threading.\n- Delta creation of the average package will be quite fast where the largest files are <30MB. In our example, one file\n  is 76% of the package size (126MB) but can take nearly 95% of the total creation time!\n- We will be applying features to our packages that will reduce file sizes (such as the much smaller RELR relocations\n  and identical code folding), making the approach even better, but that will be for a future blog post."
    },
    {
      "id": "2022/01/25/pedal-to-the-metal",
      "metadata": {
        "permalink": "/blog/2022/01/25/pedal-to-the-metal",
        "source": "@site/blog/pedal-to-the-metal.md",
        "title": "Pedal to the Metal",
        "description": "Wow - what a roller coaster. The most recent development cycle has seen us really",
        "date": "2022-01-25T11:31:30.000Z",
        "formattedDate": "January 25, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.02,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Pedal to the Metal",
          "date": "2022-01-25T11:31:30.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2022/01/25/pedal-to-the-metal"
        },
        "prevItem": {
          "title": "Making Deltas Great Again! (Part 1)",
          "permalink": "/blog/2022/02/11/making-deltas-great-again-1"
        },
        "nextItem": {
          "title": "Can Hardly Contain Myself, Plus a Bonus",
          "permalink": "/blog/2022/01/20/can-hardly-contain-myself-plus-a-bonus"
        }
      },
      "content": "Wow - what a roller coaster. The most recent development cycle has seen us really\nbegin to realise the potential of `moss` and `boulder`, our software management\nand builder tooling. In fact, work has progressed so far that we're happy to\nmake something of a milestone announcement.. So happy in fact, I personally\nwrote this post. (I know, right?)\n\n![Installing remote package and dependencies with moss](/img/blog/pedal-to-the-metal/Featured.webp)\n\n<!--truncate-->\n\n\n(TLDR; We're gonna build images for packaging now kthxbai)\n\n\n# Release ... The Packages\n\nWe're a **highly** tool oriented project, and in fact, virtually every announcement\nsince our inception has been in relation to tooling. But.. aren't we also building\na Linux distribution..?\n\nDespite the raw state of some of the tooling, we now feel we're at a point where\ntruly building the \"OS\" side of Serpent can happen. As it stands, we have a small\nnumber of items to complete before we launch the build pipeline.\n\nHere's the plan, quite simple, really. We're going to rapidly fix some last blockers\nin enabling a pipeline to onboard contributors during the early stages of development\nto form a proper feedback cycle for the package manager development. As such - we'll\nbe entering a parallel development phase where the distribution and package manager\nwill grow in tandem allowing us to rapidly build up.\n\n\n# Images when?\n\nAlong with packages being available, we're going to soon be building preliminary\nSerpent OS images. These are not meant for use in production or daily use in any\ncapacity and will be provided so that the developer community around Serpent OS\ncan begin implementing the distro itself. Anyone will be free to download and test\nthe images - just remember, they will eat your homework and are devcycle images\nto figure out how to provide the final user friendly images :)\n\n# The Mandatory Bullet Point List\n\nOh you have to have one of these. It's mandatory.\n\n - Finish `moss-container` integration with `boulder`\n - Add `boulder` configuration support to define *repositories for the build*\n - Add some internal scripting to construct and manage our GitLab recipe repos\n - Add a primitive binary repo indexing system until full variant available\n - Add automated builder for our recipes.\n - Get everyone packaging (:O)\n - Enable packagers by implementing features and solving moss/boulder issues.\n - Start chucking out installable images, for packaging and tooling validation.\n \nTimescale? Fast as frackin possible. Welcome to our test launch party."
    },
    {
      "id": "2022/01/20/can-hardly-contain-myself-plus-a-bonus",
      "metadata": {
        "permalink": "/blog/2022/01/20/can-hardly-contain-myself-plus-a-bonus",
        "source": "@site/blog/can-hardly-contain-myself-plus-a-bonus.md",
        "title": "Can Hardly Contain Myself, Plus a Bonus",
        "description": "One of the core steps for building a package is setting up a minimal environment with only the required (and stated)",
        "date": "2022-01-20T07:08:07.000Z",
        "formattedDate": "January 20, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.085,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Can Hardly Contain Myself, Plus a Bonus",
          "date": "2022-01-20T07:08:07.000Z",
          "draft": false,
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/can-hardly-contain-myself-plus-a-bonus/Featured.webp",
          "slug": "2022/01/20/can-hardly-contain-myself-plus-a-bonus"
        },
        "prevItem": {
          "title": "Pedal to the Metal",
          "permalink": "/blog/2022/01/25/pedal-to-the-metal"
        },
        "nextItem": {
          "title": "The Joy of Contribution",
          "permalink": "/blog/the-joy-of-contribution"
        }
      },
      "content": "One of the core steps for building a package is setting up a minimal environment with only the required (and stated)\ndependencies. Currently we have been building our stones in an `systemd-nspawn` container, where the root contains every\npackage that's been built so far. This makes the environment extremely difficult to reproduce!\n\n<!--truncate-->\n\nToday we announce `moss-container`, a simple but flexible container creator that we can integrate for proper\ncontainerized builds.\n\n# Versatile For Many Use Cases\n\nContainers have a multitude of uses for a Linux distro, but our immediate use case is for reproducible container builds\nfor `boulder`. However, we have plans to use `moss-container` for testing, validation and benchmarking purposes as well.\nTherefore it's important to consider all workloads, so features like `fakeroot` and `networking` can be toggled on or\noff depending on what features are needed.\n\n`moss-container` takes care of everything, the device nodes in the `/dev` tree, mounting directories as `tmpfs` so the\nenvironment is left in a clean state, and mounting the `/sys` and `/proc` special file-systems. These are essential for\na fully functioning container where programs like `python` and even `clang` won't work without them. And best of all,\nit's very fast so fits in well with the rest of our tooling!\n\nThe next step is integrating `moss-container` into `boulder`, so that builds become reproducible across machines, and\nmakes it much easier for users to run builds on their host machines.\n\n# moss Now Understands Repositories\n\nPreviously (but not covered in the blogs) work was also done on `moss` so that it can understand and fetch `stone`\npackages from an online repo. This ties in nicely with the `moss-container` work and is a requirement for finishing up a\nproper build process for Serpent OS. We are now one step closer to having a full distribution cycle from building\npackages and pushing those packages as system updates!\n\n![Container with functioning device nodes](/img/blog/can-hardly-contain-myself-plus-a-bonus/Featured.webp)\n\n# Check Out The Development\n\nIn case you've missed it, `ikey` has been streaming some of the development of the tooling on his\n[Twitch channel](https://www.twitch.tv/ikeydoherty). DLang is not as commonly used as other languages, so check it out\nto see the advantages it brings. Streams are typically announced on twitter, or give him a follow to see when he next\ngoes live!\n\n# Bonus Content Refresh\n\nThis year we've had a considerable number of new visitors and interest in Serpent OS. Unfortunately the content on the\nwebsite had been a bit stale and untouched for some time. There was some confusion and misunderstanding over some of the\nterms and content. Some of the common issues were:\n\n- Subscriptions is a loaded term relating to software\n- Subscriptions only referred to a fraction of the smart features\n- Seemed targeted at advanced users with too many technical terms\n- Lack of understanding around what moss and boulder do\n- That features would add complexity when the tools were actually removing the complexity\n\nThe good news is that a good chunk of it has been redone, including two new pages for our core tools `boulder` and\n`moss`. `Subscriptions` has been renamed to `Smart System Management` to reflect its broader nature (which you can read\nabout [here](/smart)).\n\nMuch of the content has also had a refresh or a rewrite, so if you've seen it before, it will likely be a lot easier to\ndigest now. But this isn't the final state of the content, as more features will need to be added and there's still a\nfew rough edges (and I like to rewrite things every once in awhile). Many ideas have been raised by our community in the\n[matrix channel](https://matrix.to/#/#serpentos:matrix.org), so a shout-out to the good folks we have hanging out there."
    },
    {
      "id": "/the-joy-of-contribution",
      "metadata": {
        "permalink": "/blog/the-joy-of-contribution",
        "source": "@site/blog/the-joy-of-contribution.md",
        "title": "The Joy of Contribution",
        "description": "​A core pillar (outside of providing a blazingly fast distribution!) of Serpent OS is to create an enjoyable environment",
        "date": "2022-01-04T01:03:23.000Z",
        "formattedDate": "January 4, 2022",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.01,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "The Joy of Contribution",
          "date": "2022-01-04T01:03:23.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ]
        },
        "prevItem": {
          "title": "Can Hardly Contain Myself, Plus a Bonus",
          "permalink": "/blog/2022/01/20/can-hardly-contain-myself-plus-a-bonus"
        },
        "nextItem": {
          "title": "Performance Corner: Small Changes Pack a Punch",
          "permalink": "/blog/2021/12/14/performance-corner-small-changes-pack-a-punch"
        }
      },
      "content": "​A core pillar (outside of providing a blazingly fast distribution!) of Serpent OS is to create an enjoyable environment\nfor users, contributors and the team. For the team there are two needs from the project, that we are creating a\ndistribution that we ourselves want to use and one that we will enjoy contributing for a long time to come.\n\n<!--truncate-->\n\n# To Contribute or Not\n\nNew users come and go, many asking what they can do to help. Now I could come up with a really long list of things to\ndo, but that is rarely fulfilling to users. They might get a few things done, but tend to drop off quite quickly. The\nproblem is that contributing becomes more like unpaid work, and no-one really wants more of that! Once the joy is gone,\nthe contributions stop as it doesn't make sense spending your time doing something you don't enjoy. Most projects will\nsay to find something you enjoy doing, rather than tell users what to work on. This is why making Serpent OS fun and\nenjoyable with a thriving community is the core foundation on which it is built.\n\n# Experience is a Great Teacher\n\nAs the saying goes, if we fail to learn from our mistakes, we are doomed to repeat them. The team are well versed in\ndistro tooling and the sore points where improvements are needed. This is also true of managing a distro and community.\nSome things work well and some didn't. We have gone to great lengths to discuss the good and the bad so that we can\ndevelop Serpent OS into what we envision for the project. Mistakes have been made, so we must learn from them.\n\n# Making it Fun for Other People\n\nIt takes more than a team to achieve great things! A distribution is no different, with the contributions from the\ncommunity providing a key building block for the project. Therefore we need to think beyond ourselves and see things\nfrom other perspectives. Frictions can arise from all parts so we are open to feedback and making changes to reduce\nthem.\n\nThere is a good balance of levels within the team and community, from maintainers, contributors and meme posters to keep\nthings light. The ideal approach is making it easier for contributors even at a slight cost to the team. Taking it a\nstep further to advocate on the behalf of contributors to make their lives easier for further contributions.\n\n# A Word From `sunnyflunk`\n\nWe all have had varying experiences with previous projects and the major thing missing was that they weren't fun to work\non after a while (often the opposite). We have a good understanding of why this happened and how it can be prevented.\nI can't speak for every user and contributor, but I want to be doing this for a long time. To ensure this occurs, issues\nin the community will be addressed early and not left to fester.\n\n`moss` and `boulder` provide the tooling I've always wanted to speed through the work efficiently to spend more time on\nthe parts I enjoy most. It also makes it a less demanding time commitment, which is great for the longevity of the\nproject. Something that I didn't do previously was advocating for improvements to make things better for everyone. That\nalso means providing support and not being demanding of other people. Lesson learned!\n\nWe are still finishing up on the tooling, so there isn't too much to contribute yet. To the huge influx of users and\ninterest, hold on to your hats and enjoy the ride!"
    },
    {
      "id": "2021/12/14/performance-corner-small-changes-pack-a-punch",
      "metadata": {
        "permalink": "/blog/2021/12/14/performance-corner-small-changes-pack-a-punch",
        "source": "@site/blog/performance-corner-small-changes-pack-a-punch.md",
        "title": "Performance Corner: Small Changes Pack a Punch",
        "description": "Here we have another round of changes to make packages smaller and show just how much we care about performance and",
        "date": "2021-12-14T07:14:23.000Z",
        "formattedDate": "December 14, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 7.725,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Performance Corner: Small Changes Pack a Punch",
          "date": "2021-12-14T07:14:23.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/performance-corner-small-changes-pack-a-punch/Featured.webp",
          "slug": "2021/12/14/performance-corner-small-changes-pack-a-punch"
        },
        "prevItem": {
          "title": "The Joy of Contribution",
          "permalink": "/blog/the-joy-of-contribution"
        },
        "nextItem": {
          "title": "Out of the Bootstrap - Towards Serpent OS",
          "permalink": "/blog/2021/12/02/out-of-the-bootstrap-towards-serpent-os"
        }
      },
      "content": "Here we have another round of changes to make packages smaller and show just how much we care about performance and\nefficiency! Today we are focusing mainly on `moss-format` changes to reduce the size of its payloads. The purpose of\nthese changes is to reduce the size of packages, DB storage for transactions and memory usage for `moss`. These changes\nwere made just before we moved out of bootstrap, so we wouldn't have to rebuild the world after changing the format.\nLets get started!\n\n<!--truncate-->\n\n# Squeezing Out the Last Blit of Performance\n\n`Blitting` in Serpent OS is the process of setting up a root, using hardlinks of files installed in the `moss` store to\nconstruct the system `/usr` directory. Some initial testing showed `buildPath` using about 3% of the total events\nwhen installing a package with many files. As a system is made up of over 100,000 files, that's a lot of paths that need\nto be calculated! Performance is therefore important to minimize time taken and power consumed.\n\nAfter running a few benchmarks of `buildPath`, there were a few tweaks which could improve the performance. Rather than\ncalculating the full path of the file, we can reuse the constant root path for each file instead, reducing `buildPath`\nevents by 15%. Here we see some callgrind data from this small change (as it was difficult to pickup in the noise).\n\n```\nBefore:\n   48,766,347 ( 1.50%)  /usr/include/dlang/ldc/std/path.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   34,241,805 ( 1.05%)  /usr/include/dlang/ldc/std/utf.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   14,363,684 ( 0.44%)  /usr/include/dlang/ldc/std/range/package.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n\nAfter:\n   40,357,800 ( 1.25%)  /usr/include/dlang/ldc/std/path.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   29,523,966 ( 0.91%)  /usr/include/dlang/ldc/std/utf.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   12,814,283 ( 0.40%)  /usr/include/dlang/ldc/std/range/package.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n```\n\n# Only Record What's Needed!\n\nOur `Index Payload` is used for extracting the `Content Payload` into its hashed file names. As it has one job (and then\ndiscarded), we could hone in on including only the information we needed. Previously an entry was a 32 byte key and\nstoring a 33 byte hash. We have now integrated the hash as a `ubyte[16]` field, cut other unneeded fields so that we can\nfit the whole entry into 32 bytes. That's a 51% reduction in the size of the `Index Payload` and about 25% smaller when\ncompressed.\n\n```\nBefore: Index [Records: 3688 Compression: Zstd, Savings: 60.61%, Size: 239.72 KB]\nAfter:  Index [Records: 3688 Compression: Zstd, Savings: 40.03%, Size: 118.02 KB]\n```\n\n# Too Many Entries Makes for a Large Payload\n\nOne of the bugbears about the Layout Payload, was the inclusion of `Directory` paths for every single directory. This is\nhandy in that directories can be created easily before any files are included (to ensure they exist), but it comes at a\nprice. The issue was twofold, the extra entries made inspecting the file list take longer and also made the `Layout\nPayload` larger than it needed to be. Therefore directories are no longer included as Layout entries with the exceptions\nof empty directories and directories with different permissions. Lets compare `nano` and `glibc` builds before and\nafter the change:\n\n```\nnano\nBefore: Layout [Records: 174 Compression: Zstd, Savings: 80.58%, Size: 13.78 KB]\nAfter:  Layout [Records:  93 Compression: Zstd, Savings: 73.49%, Size:  9.15 KB]\n\nglibc\nBefore: Layout [Records: 7879 Compression: Zstd, Savings: 86.88%, Size: 755.00 KB]\nAfter:  Layout [Records: 6813 Compression: Zstd, Savings: 86.24%, Size: 689.20 KB]\n```\n\nA surprisingly large impact from a small change, with 1,000 fewer entries for `glibc` and cutting the Layout Payload\nsize of `nano` by a third. What it shows is that it's hugely beneficial where locales are involved and % size reduction\nincreases where you have fewer files. To give an example of how bad it could be, the KDE Frameworks package\n`ktexteditor` would have produced 300 entries in the Layout Payload, where 200 of those would have been for directories!\nI'd estimate a 50% reduction in the Layout Payload size for this package! Here's an example of how a locale file used\nto be stored (where only the last line is added now).\n\n```\n  - /usr/share/locale [Directory]\n  - /usr/share/locale/bg [Directory]\n  - /usr/share/locale/bg/LC_MESSAGES [Directory]\n  - /usr/share/locale/bg/LC_MESSAGES/nano.mo -> ec5b82819ec2355d4e7bbccc7d78ce60 [Regular]\n```\n\nThis will be exceptionally useful for keeping the `LayoutDB` slimmer and faster as the number of packages grows.\n\n# Cutting Back 1 Byte at a Time\n\nNext we removed recording the timestamps of files, which for reproducible builds, is often a number of little relevance\nas you have to force them all to a fixed number. As `moss` is de-duplicating for files, there's a second issue where two\npackages could have different timestamps for the same hash. Therefore it was considered an improvement to simply exclude\ntimestamps altogether. This improved install time as we no longer overwrite the timestamps and made the payload more\ncompressible due to replacing it with 8 bytes of padding. Unfortunately we weren't quite able to free up 16 bytes to\nreduce the size of each entry, but will be something to pursue in future.\n\nAnother quick improvement was reducing the lengths of paths for each entry. `moss` creates system roots and switches\nbetween them by changing the `/usr` symlink. Therefore, all system files need to be installed into `/usr` or they will\nnot make up your base system. Therefore we have no need to store `/usr` in the Payload so we strip `/usr/` from the\npaths (the extra / gives us another byte off!) which we recreate on install. This improved the uncompressed size of the\nPayload, with only a minor reduction when compressed.\n\nCombined these result in about a 5% decrease in the compressed and uncompressed size of the `Layout Payload`.\n\n```\nBefore: Layout [Records: 6812 Compression: Zstd, Savings: 86.24%, Size: 689.10 KB]\nAfter: Layout [Records: 6812 Compression: Zstd, Savings: 86.20%, Size: 655.00 KB]\n```\n\n# Storing the Hash Efficiently\n\nUsing the same method for the `Index Payload`, we are now storing the hash as `ubyte[16]`, but not directly in the\n`Payload Entry`. This gives us a sizeable reduction of 17 bytes per entry which is the most significant of all the\n`Layout Payload` changes. As the extra space was unneeded, it compressed well so only resulted in a small reduction in\nthe compressed Payload size.\n\n```\nBefore: Layout [Records: 6812 Compression: Zstd, Savings: 86.20%, Size: 655.00 KB]\nAfter: Layout [Records: 6812 Compression: Zstd, Savings: 83.92%, Size: 539.26 KB]\n```\n\n![Planning out the payload changes](/img/blog/performance-corner-small-changes-pack-a-punch/Featured.webp)\n\n# Hang On, Why am I Getting Faster Installation?\n\nAs a side-effect of small code rewrites to implement these changes, we've seen a nice decrease in time to install\npackages. There are fewer entries to iterate over with the removal of directories and `buildPath` is now only called\nonce for each path. It goes to show that focusing on the small details leads to less, more efficient and faster code.\nHere we find that we have now essentially halved the number of events related to `buildPath` with all the changes\nresulting in about a 5% reduction in install time. Note that for this test, over 80% of time is spent in `zstd`\ndecompressing the package which we haven't optimized (yet!). Here's another look the `buildPath` numbers factoring in\nall the changes:\n\n```\nBefore:\n   48,766,347 ( 1.50%)  /usr/include/dlang/ldc/std/path.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   34,241,805 ( 1.05%)  /usr/include/dlang/ldc/std/utf.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   14,363,684 ( 0.44%)  /usr/include/dlang/ldc/std/range/package.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n\nAfter:\n   25,145,625 ( 0.79%)  /usr/include/dlang/ldc/std/path.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n   17,967,216 ( 0.57%)  /usr/include/dlang/ldc/std/utf.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n    7,761,417 ( 0.24%)  /usr/include/dlang/ldc/std/range/package.d:pure nothrow @safe immutable(char)[] std.path.buildPath\n```\n\n# Summing it All Up\n\nIt was a pretty awesome weekend of work (a few weeks ago now), making some quick changes that will improve `Serpent OS`\na great deal for a long time to come. This also means we have integrated all the quick format changes so we won't have\nto rebuild packages while bringing up the repos.\n\nHere's a quick summary of the results of all these small changes:\n\n- 51% reduction in the uncompressed size of the Index Payload\n- 25% reduction in the compressed size of the Index Payload\n- 29-50% reduction in the uncompressed size of the Layout Payload (much more with fewer files and more locales)\n- 12-15% reduction in the compressed size of the Layout Payload\n- 5% faster package installation for our benchmark (800ms to 760ms)\n\nThese are some pretty huge numbers and even excluded the massive improvements we made in the previous blog!\n\n# What if We Include the Previous Changes?\n\nI'm glad you asked, cause I was curious too! Here we see a before and after with all the changes included. For the\n`Layout Payload` we see a ~45% reduction in the compressed and uncompressed size. For the `Index Payload` we have\nreduced the uncompressed size by 67% and the compressed size by 56%. Together resulting in halving the compressed and\nuncompressed size of the metadata of our stone packages!\n\n```\nBefore:\nPayload: Layout [Records: 5441 Compression: Zstd, Savings: 83.13%, Size: 673.46 KB] (113.61 KB compressed)\nPayload: Index [Records: 2550 Compression: Zstd, Savings: 55.08%, Size: 247.35 KB] (111.11 KB compressed)\n\nAfter:\nPayload: Layout [Records: 4718 Compression: Zstd, Savings: 83.62%, Size: 374.42 KB] (61.33 KB compressed)\nPayload: Index [Records: 2550 Compression: Zstd, Savings: 39.85%, Size: 81.60 KB] (49.01 KB compressed)\n```\n\nNow we proceed towards bringing up repos to enjoy our very efficient packages!"
    },
    {
      "id": "2021/12/02/out-of-the-bootstrap-towards-serpent-os",
      "metadata": {
        "permalink": "/blog/2021/12/02/out-of-the-bootstrap-towards-serpent-os",
        "source": "@site/blog/out-of-the-bootstrap-towards-serpent-os.md",
        "title": "Out of the Bootstrap - Towards Serpent OS",
        "description": "The initial stone packages that will seed the first Serpent OS repo have now been finalized! This means that work",
        "date": "2021-12-02T07:07:12.000Z",
        "formattedDate": "December 2, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.285,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Out of the Bootstrap - Towards Serpent OS",
          "date": "2021-12-02T07:07:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/out-of-the-bootstrap-towards-serpent-os/Featured.webp",
          "slug": "2021/12/02/out-of-the-bootstrap-towards-serpent-os"
        },
        "prevItem": {
          "title": "Performance Corner: Small Changes Pack a Punch",
          "permalink": "/blog/2021/12/14/performance-corner-small-changes-pack-a-punch"
        },
        "nextItem": {
          "title": "It All Depends",
          "permalink": "/blog/2021/11/23/it-all-depends"
        }
      },
      "content": "The initial `stone` packages that will seed the first Serpent OS repo have now been finalized! This means that work\ntowards setting up the infrastructure for live package updates begins now. We plan on taking time to streamline the\nprocesses with a focus of fixing issues at the source. In this way we can make packaging fast and efficient so we can\nspend time working on features rather than package updates.\n\n<!--truncate-->\n\n# The End of Bootstrap\n\nBootstrapping a distribution involves building a new toolchain and many packages needed to support it. For us bootstrap\nwas getting us to the point where we have built `stone` packages that we can use to start an initial repository with\nfull working dependencies. This has been enabled by integrating dependencies into `moss`, creating the first repo index.\nOf note is that it is already enabled for 32bit support, so we have you covered there. While this is the end of\nbootstrap, the fun has only just begun!\n\n![The first install from the bootstrap](/img/blog/out-of-the-bootstrap-towards-serpent-os/Featured.webp)\n\n# Where to Next?\n\nThe next goal is to make Serpent OS self-hosting, where we can build packages in a container and update the repo index\nwith the newly built packages. It is essentially a live repository accessible from the internet. There's still plenty of\nimprovements to be made with the tooling, but will soon enable more users to participate in bringing Serpent OS into\nfruition.\n\n# Becoming More Inclusive\n\nWhile there's a strong focus in Serpent OS on performance, the decision has been made to lower the initial requirements\nfor users. Despite AVX2 being an older technology, there are still computers sold today that don't support it. Because\nof this (and already having interested users who don't have newer machines), the baseline requirement for Serpent OS\nwill be `x86_64-v2`, which only requires SSE4.2.\n\nIt was always the plan to add support for these machines, just later down the track. In reality, this makes a lot more\nsense, as there will be many cases where building 2 versions of a package provides little value. This is where a package\ntakes a long time to build and doesn't result in a notable performance improvement. We will always need the `x86_64-v2`\nversion of a package to be compatible with the older machines. With this approach we can reduce the build server\nrequirements without a noticeable impact to users as only a few packages you use will be without extra optimizations\n(and probably don't benefit from them anyway).\n\nI want to make it clear that this will be temporary, with impactful `x86_64-v3+` packages rolling out as soon as\npossible. This change paves the way to integrate our technology stack taking care of your system for you and increases\nits priority. Users meeting the requirements of the `x86_64-v3+` instruction set (this includes additional instructions\nbeyond `x86_64-v3`) will automatically start installing these faster packages as they become available. Our\n`subscriptions` model will seamlessly take care of everything for you behind the scenes so you don't need to read a\nwiki or forum to learn how to get these faster packages. We can utilize the same approach in future for our `ARM` stack,\noffering more optimized packages where it provides the most benefit.\n\nNote that from the bootstrap, most packages built in under 15s and only three took longer than 2 minutes.\n\n# Trying Out Some Tweaks From the Get Go\n\nWhile the project is young is a great time to test out new technologies. The use of `clang` and `lld` open up new\npossibilities to reduce file sizes, shorten load times and decrease memory usage. Some of these choices may have\nimpacts on compatibility, so testing them out will be the best way to grasp that. Making sure that you can run apps like\nSteam is vital to the experience, so whatever happens we will make sure it works. The good news is that due to the\nunique architecture of Serpent OS, we can push updates that break compatibility with just a reboot, so if we ever feel\nthe need to change the `libc`, we can make the change without you having to reinstall! More importantly, we can test\nmajor stack updates by rebooting into a staged update and go straight back to the old system, regardless of your file\nsystem.\n\n# Speed Packaging!\n\nIn the early days of the repository, tooling to make creating new packages as simple as possible is vital for\nefficiency. Spending some time automating as much of the process as possible will take weeks off bringing up Serpent OS.\nBy making packaging as easy as possible will also help users when creating their own packages. While it would be faster\nto work around issues, the build tooling upgrades will benefit everyone.\n\nThe other way we'll be speeding up the process is by holding back some of the tuning options by default. `LTO` for\ninstance can result in much longer build times so will not initially be the default. The same is true for debug\npackages, where it slows down progress without any tangible benefit.\n\n# Things Are Happening!\n\nWe hope you are as excited as we are!"
    },
    {
      "id": "2021/11/23/it-all-depends",
      "metadata": {
        "permalink": "/blog/2021/11/23/it-all-depends",
        "source": "@site/blog/it-all-depends.md",
        "title": "It All Depends",
        "description": "It all depends.. it really does. On shared libraries.. interpreters.. pkg-config providers",
        "date": "2021-11-23T23:45:06.000Z",
        "formattedDate": "November 23, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 5.32,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "It All Depends",
          "date": "2021-11-23T23:45:06.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/it-all-depends/Featured.webp",
          "slug": "2021/11/23/it-all-depends"
        },
        "prevItem": {
          "title": "Out of the Bootstrap - Towards Serpent OS",
          "permalink": "/blog/2021/12/02/out-of-the-bootstrap-towards-serpent-os"
        },
        "nextItem": {
          "title": "Performance Corner: Faster Builds, Smaller Packages",
          "permalink": "/blog/2021-11-05/performance-corner-faster-builds-smaller-packages"
        }
      },
      "content": "It all depends.. it really does. On shared libraries.. interpreters.. `pkg-config` providers\nand packages. It's the same story for all \"package managers\", how do we ensure that the\ninstalled software has everything it needs at runtime?\n\nOur merry crew has been involved in designing and building Linux distributions for a very, very\nlong time, so we didn't want to simply repeat history.\n\n![Using updated moss](/img/blog/it-all-depends/Featured.webp)\n\n<!--truncate-->\n\n\nThanks to many improvements across our codebase, including `moss-deps`, we automatically analyse\nthe assets in a package (rapidly too) to determine any dependencies we can add without requiring\nthe maintainer to list them. This is similar in nature to RPM's behaviour.\n\nAs such we encode dependencies into our (endian-aware, binary) format which is then stored in the\nlocal installation database. Global providers are keyed for quick access, and the vast majority\nof packages will not explicitly depend on another package's *name*, rather, they'll depend on\na capability or provider. For subpackage dependencies that usually depend on \"NEVRA\" equality\n(i.e. matching against a name with a minimum version, release, matching epoch and architecture),\nwe'll introduce a lockstep dependency that can only be resolved from its origin source (repo).\n\nLastly, we'll always ensure there is no possibility for \"partial update\" woes. With these\nconsiderations, we have no need to support `>=` style dependencies, and instead rely on\nstrict design goals and maintainer responsibility.\n\n# First and foremost!\n\nThe rapid move we're enjoying from concept, to prototype, and soon to be fully fledged Linux distribution,\nis only possible with the amazing community support. The last few months have seen us pull off some amazing\nfeats, and we're now executing on our first public milestones. With your help, more and more hours can be\nspent getting us ready for release, and would probably help to insulate my shed office! (Spoiler: its\nplastic and electric heaters are expensive =))\n\n# The Milestones\n\nWe have created our initial milestones that our quite literally our escape trajectory from\nbootstrap to distro. We're considerably closer now, hence this open announcement.\n\n## [v0.0](https://gitlab.com/groups/serpent-os/-/milestones/1]): Container (`systemd-nspawn`)\n\nOur first release medium will be a `systemd-nspawn` compatible container image. Our primary\ndriver for this is to allow us to add encapsulation for our build tool, `boulder`, permitting\nus to create distributable builder images to seed our infrastructure and first public binary\nrepository.\n\n## [v0.1](https://gitlab.com/groups/serpent-os/-/milestones/2): Bootable \"image\"\n\nOnce our build infra is up and running (honestly a lot of work has been completed for this in\nadvance) we'll work towards our first 0.1 image. This will initially target VM usage, with\na basic console environment and tooling (`moss`, `boulder`, etc).\n\n## And then..\n\nWe have a clear linear path ahead of us, with each stage unlocking the next. During the development\nof `v0.0` and `v0.1` we'll establish our build and test infrastructure, and begin hosting our\npackage sources and binaries. At this point we can enter a rapid development cycle with\nincremental, and considerable improvements. Such as a usable desktop experience and installer.. :)\n\n# Recent changes\n\nI haven't blogged in quite a while, as I've been deep in the trenches working on our core features.\nAs we've expressed before, we tend to work on the more complex systems *first* and then glue them\ntogether after to form a cohesive hole. The last few days have involved plenty of glue, and we now\nhave distinct package management features.\n\n## Refactor\n\n - Replaced defunct InstallDB with reusable MetaDB for local installation of archives as well as\n   forming the backbone of repository support.\n - Added `ActivePackagesPlugin` to identify installed packages\n - Swapped non cryptographic hash usage with `xxhash`\n\n## Dependencies\n\n - Introduced new Transaction type to utilise a directed acyclical graph for dependency solving.\n - Reworked moss-deps into plugins + registry core for all resolution operations.\n - Locally provided `.stone` files handled by `CobblePlugin` to ensure we depsolve from this\n   set too.\n - New Transaction set management for identifying state changes and ensuring full resolution\n   of target system state.\n - Shared library and interpreter (DT_INTERP) dependencies and producers automatically encoded\n   into packages and resolved by depsolver.\n\n\n## Package Installation\n\nWe handle locally provided `.stone` packages passed to the `install` command identically to\nthose found in a repository.  This eliminates a lot of special casing for local archives and\nallows us to find dependencies within the provided set, before looking to the system and the\nrepositories.\n\n![Install](/img/blog/it-all-depends/Install.webp \"Installing packages\")\n\nDependency resolution is performed now for our package installation and is validated at\nmultiple points, allowing a package like nano to depend on compact automatic dependencies:\n\n```d\n\tDependency(DependencyType.SharedLibraryName, \"libc.so.6(x86_64)\");\n```\n\nNote our format and database are binary and endian aware. The dependency type only requires\n1 byte of storage and no string comparisons.\n\n## List packages\n\nThanks to the huge refactor, we can now trivially access the installed packages as a list.\nThis code will be reused for a `list available` command in future.\n\nExample `list installed` output:\n\n```\n                   file (5.4) - File type identification utility\n             file-32bit (5.4) - Provides 32-bit runtime libraries for file\n       file-32bit-devel (5.4) - Provides development files for file-32bit\n             file-devel (5.4) - Development files for file\n                   nano (5.5) - GNU Text Editor\n```\n\n![ListInstalled](/img/blog/it-all-depends/ListInstalled.webp \"Listing installed packages\")\n\n## Inspect archives\n\nFor debugging and development purposes, we've moved our old \"info\" command to a new\n\"inspect\" command to work directly on local `.stone` files. This displays extended\ninformation on the various payloads and their compression stats.\n\nFor general users - the new `info` command displays basic metadata and package\ndependencies.\n\n![Info](/img/blog/it-all-depends/Info.webp \"Display package information\")\n\n## Package Removal\n\nUpon generating a new system state, \"removed\" packages are simply no longer installed. As such\nno live mutation is performed. As of today we can now request the removal of packages from the\ncurrent state, which generates a new filtered state. Additionally we remove all reverse dependencies,\ndirect and transitive. This is accomplished by utilising a transposed copy of the directed acyclical\ngraph, identifying the relevant subgraph and occluding the set from the newly generated state.\n\n![Remove](/img/blog/it-all-depends/Remove.webp \"Remove packages\")\n\n## Lastly\n\nThe past few weeks have been especially enjoyable. I've truly had a fantastic time working on the project\nand cannot wait for the team and I to start offering our first downloads, and iterate as a truly new\nLinux distribution that borrows some ideas from a lot of great places, and fuses them into something\nawesome.\n\nKeep toasty - this train isn't slowing down."
    },
    {
      "id": "2021-11-05/performance-corner-faster-builds-smaller-packages",
      "metadata": {
        "permalink": "/blog/2021-11-05/performance-corner-faster-builds-smaller-packages",
        "source": "@site/blog/performance-corner-faster-builds-smaller-packages.md",
        "title": "Performance Corner: Faster Builds, Smaller Packages",
        "description": "Performance Corner is a new series where we highlight to you some changes in Serpent OS that may not be obvious, but",
        "date": "2021-11-05T08:23:32.000Z",
        "formattedDate": "November 5, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 5.065,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Performance Corner: Faster Builds, Smaller Packages",
          "date": "2021-11-05T08:23:32.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/performance-corner-faster-builds-smaller-packages/Featured.webp",
          "slug": "2021-11-05/performance-corner-faster-builds-smaller-packages"
        },
        "prevItem": {
          "title": "It All Depends",
          "permalink": "/blog/2021/11/23/it-all-depends"
        },
        "nextItem": {
          "title": "Optimal File Locality",
          "permalink": "/blog/2021/10/04/optimal-file-locality"
        }
      },
      "content": "`Performance Corner` is a new series where we highlight to you some changes in Serpent OS that may not be obvious, but\nshow a real improvement. Performance is a broad term that also covers efficiency, so things like making files smaller,\nmaking things faster or reducing power consumption. In general things that are unquestionably improvements with little\nor no downside. While the technical details may be of interest to some, the main purpose is to highlight the real\nbenefit to users and/or developers that will make using Serpent OS a more enjoyable experience. Show me the numbers!\n\n<!--truncate-->\n\nHere we focus on a few performance changes `Ikey` has been working on to the build process that are showing some pretty\nawesome results! If you end up doing any source builds, you'll be thankful for these improvements. Special thanks to\n`ermo` for the research into hash algorithms and enabling our ELF processing.\n\n# Start From the Beginning\n\nWhen measuring changes, it's always important to know where you're starting from. Here are some results from a recent\n`glibc` build, but before these latest changes were incorporated.\n\n```\nPayload: Layout [Records: 5441 Compression: Zstd, Savings: 83.13%, Size: 673.46 KB]\nPayload: Index [Records: 2550 Compression: Zstd, Savings: 55.08%, Size: 247.35 KB]\nPayload: Content [Records: 2550 Compression: Zstd, Savings: 81.46%, Size: 236.72 MB]\n ==> 'BuildState.Build' finished [4 minutes, 6 secs, 136 ms, 464 μs, and 7 hnsecs]\n ==> 'BuildState.Analyse' finished [21 secs, 235 ms, 300 μs, and 2 hnsecs]\n ==> 'BuildState.ProducePackages' finished [25 secs, 624 ms, 996 μs, and 8 hnsecs]\n```\n\nThe build time is a little high, but a lot of that is due to a slow compiler on the host machine. But analysing and\nproducing packages was also taking a lot longer than it needed to.\n\n# The Death of moss-jobs in boulder\n\nIn testing an equivalent build outside of `boulder`, the build stages were about 5% faster. Testing under `perf`, the\njobs system was a bit excessive for the needs of `boulder`, polling for work when we already know the times when\nparallel jobs would be useful. Removing `moss-jobs` allowed for simpler codepaths using multiprocessing techniques from\nthe core language. This work is integrated in `moss-deps` and the excess overhead of the build has now been eliminated.\n\n```\nBefore:\n ==> 'BuildState.Build' finished [4 minutes, 6 secs, 136 ms, 464 μs, and 7 hnsecs]\n ==> 'BuildState.Analyse' finished [21 secs, 235 ms, 300 μs, and 2 hnsecs]\nAfter:\n[Build] Finished: 3 minutes, 53 secs, 386 ms, 306 μs, and 4 hnsecs\n[Analyse] Finished: 8 secs, 136 ms, 22 μs, and 8 hnsecs\n```\n\nThe new results reflect a 26s reduction in the overall build time. But only 13s of this relates to the `moss-jobs`\nremoval. The other major change is making the analyse stage parallel in `moss-deps` (a key part of why we wanted\nparallelism to begin with). Decreasing the time from 21.2s to 8.1s is a great achievement despite it doing more work as\nwe've also added ELF scanning for dependency information in-between these results.\n\n# New Hashing Algorithm\n\nOne of the unique features in `moss` is using hashes for file names which allows full deduplication within packages,\nthe running system, previous system roots and for source builds with `boulder`. Initially this was hooked up using\n`sha256`, but it was proving to be a bit of a slowdown.\n\nEnter `xxhash`, the hash algorithm by Yann Collet for use in fast decompression software such as `lz4` and `zstd` (and\nnow in many places!). This is seriously fast, with the potential to produce hashes faster than RAM can feed the CPU. The\nhash is merely used as a unique identifier in the context of deduplication, not a cryptographic verification of origin.\n`XXH3_128bit` has been chosen due to it having an almost zero probability of a collision across 10s of millions of\nfiles.\n\nThe benefit is actually two-fold. First of all, the hash length is halved from `sha256`, so there's savings in the\npackage metadata. This shouldn't be understated as hash data is generally not as compressible as typical text and there\nare packages with a lot of files! Here the metadata for the Layout and Index payloads has reduced by 232KB! That's about\na 25% reduction with no other changes.\n\n```\nBefore:\nPayload: Layout [Records: 5441 Compression: Zstd, Savings: 83.13%, Size: 673.46 KB]\nPayload: Index [Records: 2550 Compression: Zstd, Savings: 55.08%, Size: 247.35 KB]\nAfter:\nPayload: Layout [Records: 5441 Compression: Zstd, Savings: 86.66%, Size: 522.97 KB]\nPayload: Index [Records: 2550 Compression: Zstd, Savings: 60.02%, Size: 165.75 KB]\n```\n\nCompressed this turns out to be about a 89KB reduction in the package size. For larger packages, this probably doesn't\nmean much but could help a lot more with delta packages. For deltas, we will be including the full metadata of the\nLayout and Index payloads, so the difference will be more significant there.\n\nThe other benefit of course is the speed and the numbers speak for themselves! A further 6.4s reduction in build time\nremoving most of the delay at the end of the build for the final package. This will also improve speeds for caching or\nvalidating a package.\n\n```\nBefore:\n ==> 'BuildState.Analyse' finished [21 secs, 235 ms, 300 μs, and 2 hnsecs]\nAfter:\n[Analyse] Finished: 1 sec, 688 ms, 681 μs, and 8 hnsecs\n```\n\nWith these changes combined, building packages can take 12x less time in the analyse stage, while reducing the size of\nthe metadata and the overall package. We do expect the analyse time to increase in future as we add more dependency\ntypes, debug handling and stripping, but with the integrated parallel model, we can minimize the increase in time.\n\n![img](/img/blog/performance-corner-faster-builds-smaller-packages/Featured.webp)\n\n# We're Not Done Yet\n\nThe first installment of `Performance Corner` shows some great wins to the Serpent OS tools and architecture. This is\njust the beginning and there will likely be a follow up soon (you may have also noticed that it takes too long to make\nthe packages), and there's a couple more tweaks to further decrease the size of the metadata. Kudos to `Ikey` for\ngetting these implemented!"
    },
    {
      "id": "2021/10/04/optimal-file-locality",
      "metadata": {
        "permalink": "/blog/2021/10/04/optimal-file-locality",
        "source": "@site/blog/optimal-file-locality.md",
        "title": "Optimal File Locality",
        "description": "File locality in this post refers to the order of files in our content payload. Yes that's right, we're focused on the",
        "date": "2021-10-04T05:49:12.000Z",
        "formattedDate": "October 4, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 7.075,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Optimal File Locality",
          "date": "2021-10-04T05:49:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/optimal-file-locality/Featured.webp",
          "slug": "2021/10/04/optimal-file-locality"
        },
        "prevItem": {
          "title": "Performance Corner: Faster Builds, Smaller Packages",
          "permalink": "/blog/2021-11-05/performance-corner-faster-builds-smaller-packages"
        },
        "nextItem": {
          "title": "Unpacking the Build Process: Part 2",
          "permalink": "/blog/2021/09/20/unpacking-the-build-process-2"
        }
      },
      "content": "File locality in this post refers to the order of files in our content payload. Yes that's right, we're focused on the\nsmall details and incremental improvements that combined add up to significant benefits! All of this came about from\ntesting the efficiency of content payload in `moss-format` and how well it compared against a plain tarball. One day\n`boulder` was looking extremely inefficient and then retesting the following day was proving to be extremely efficient\nwithout any changes made to `boulder` or `moss-format`. What on Earth was going on?\n\n![img](/img/blog/optimal-file-locality/Featured.webp)\n\n<!--truncate-->\n\n# Making Sure you Aren't Going Crazy!\n\nTo test the efficiency our content payload, the natural choice was to compare it to a tarball containing the same files.\nWhen first running the test the results were quite frankly awful! Our payload was 10% larger than the equivalent\ntarball! It was almost unbelievable in a way, so the following day I repeated the test again only this time the content\npayload was smaller than the tarball. This didn't actually make sense, I made the tarball with the same files, but\nonly changed the directory it was created from. Does it really matter?\n\n# File Locality Really Matters!\n\nOf course it does (otherwise it would be a pretty crappy blog post!). When extracting a `.stone` package it creates two\ndirectories, `mossExtract` where the sha256sum named files are stored and `mossInstall` where those files are\nhardlinked to their full path name. The first day I created the tarball from `mossInstall` and the second day I\nrealised that creating the tarball from `mossExtract` would provide the closest match to the content payload since it\nwas a direct comparison. When compressing the tarballs to match the `.stone` compression level, the tarball compressed\nfrom `mossInstall` was 10% smaller, despite the uncompressed tarball being slightly larger.\n\n# Compression Wants to Separate Apples and Oranges\n\nIn simplistic terms, the way compression works is comparing data that it's currently reading versus data that it's read\nearlier in the file. `zstd` has some great options like `--long` that increases the distance in which these matches can\nbe made at the cost of increased memory use. To limit memory use while making compression and decompression fast, it\ntakes shortcuts that reduce the compression ratio. For optimal compression, you want files that are most similar to\neach other to be as close as possible. You won't get as many matches from a text file to an ELF file as you would from a\nsimilar looking text file.\n\n# Spot the Difference\n\nFiles in `mossExtract` are listed in their sha256sum order, which is basically random, where files in `mossInstall` are\nordered by their path. Sorting files by path actually does some semblance of sorting where binaries are in `/usr/bin`\nand libraries are in `/usr/lib` bringing them closer together. This is in no way a perfect order, but is a large\nimprovement on a random order (up to 10% in our case!).\n\nOur `glibc` package has been an interesting test case for `boulder`, where an uncompressed tarball of the install\ndirectory was just under 1GB. As `boulder` stores files by their sha256sum, it is able to deduplicate files that\nare the same even when the build hasn't used symlinks or hardlinks to prevent the wasted space. In this case, the\ndeduplication reduced the uncompressed size of the payload by 750MB alone (that's a lot of duplicate locale data!). In\nthe `python` package, it removes 1,870 duplicate cache files to reduce the installation size.\n\nAs part of the deduplication process `boulder` would sort files by sha256sum to remove duplicate hashes. If two files\nhave the same sha256sum, then only one copy needs to be stored. It also felt clean with the output of `moss info`\nlooking nice where the hashes are listed in alphabetical order. But it was having a significant negative impact on\npackage sizes so that needed to be addressed by resorting the files by path order (a simple one-liner), making the\ncontent payload more efficient than a tarball once again.\n\n| Compression Level | sha256sum Order  | File path Order    |\n|:-----------------:|------------------|--------------------|\n|  1                | 72,724,389       | 70,924,858         |\n|  6                | 65,544,322       | 63,372,056         |\n|  12               | 49,066,505       | 44,039,782         |\n|  16               | 45,365,415       | 40,785,385         |\n|  19               | 26,643,334       | 24,134,820         |\n|  22               | 16,013,048       | 15,504,806         |\n\nTesting has shown that higher compression levels (and enabling `--long`) is more forgiving of a suboptimal file order\n(3-11% smaller vs only 2-5% smaller when using `--long`). The table above is without `--long` so the difference is\nlarger.\n\n# Hang On, Why Don't You...\n\nThere's certainly something to this and sorting by file order is a first step. In future we can consider creating an\nefficient order for files to improve locality. Putting all the ELF, image or text files together in the payload will\nhelp to shave a bit off our package sizes at only the cost to sort the files. However, we don't want to go crazy here,\nthe biggest impact on reducing package sizes will be using deltas as the optimal package delivery system (and there will\nbe a followup on this approach shortly). The `moss-format` content payload is quite simple and contains no filenames or\npaths in it. Therefore it's effectively costless to switch around the order of files, so we can try out a few things and\nsee what happens.\n\n# An Academic Experiment\n\nTo prove the value of `moss-format` and the content payload, I tried out some crude sorting methods and their impact on\ncompression for the package. As you want similar files chunked together, it divided the files into 4 groups, still\nsorted by their path order in their corresponding chunk:\n\n- **gz:** gzipped files\n- **data:** non-text files that weren't ELF\n- **elf:** ELF files\n- **text:** text files (bash scripts, perl etc)\n\n{{<figure_screenshot_one image=\"optimal-file-locality/Featured\" caption=\"Path order vs optimal order\">}}\n\nAs the chart shows, you can get some decent improvements from reordering files within the tarball when grouping files\nin logical chunks. At the highest compression level, the package is reduced by 0.83% without any impact on compression\nor decompression time. In the compression world, such a change would be greatly celebrated!\n\nAlso important to note was that just moving the gzipped files to the front of the payload was able to capture 40% of the\nsize improvement at high compression levels, but had slightly worse compression at levels 1-4. So simple changes to the\norder (in this case moving non-compressible files to the edge of the payload) can provide a reduction in size at the\nhigher levels that we care about. We don't want to spend a long time analyzing files for a small reduction in package\nsize, so we can start off with some basic concepts like this. Moving files that don't compress a lot such as already\ncompressed files, images and video to the start of payload meaning that the remaining files are closer together. We also\nneed to test out a broader range of packages and the impact any changes would have on them.\n\n# Food For Thought?\n\nSo ultimately the answer to the original question (was `moss-format` efficient?), the answer is yes! While there are\nsome things that we still want to change to make it even better, in its current state package creation time was faster\nand overheads were lower than with compressing an equivalent tarball. The compressed tarball at `zstd -16` was 700KB\nlarger than the full `.stone` file (which contains a bit more data than the tarball).\n\nThe unique format also proves its worth in that we can make further adjustments to increase performance, reduce memory\nrequirements and reduce package sizes. What this experiment shows is that file order really does matter, but using the\nbasic sorting method of filepath gets you most of the way there and is likely good enough for most cases.\n\nHere are some questions we can explore in future to see whether there's greater value in tweaking the file order:\n\n- Do we sort ELF files by path order, file name or by size?\n- Does it matter the order of chunks in the file? (i.e. ELF-Images-Text vs Images-Text-ELF)\n- How many categories do we need to segregate and order?\n- Can we sort by extension? (i.e. for images, all the png files will be together and the jpegs together)\n- Do we simply make a couple of obvious changes to order and leave `zstd` to do the heavy lifting?"
    },
    {
      "id": "2021/09/20/unpacking-the-build-process-2",
      "metadata": {
        "permalink": "/blog/2021/09/20/unpacking-the-build-process-2",
        "source": "@site/blog/unpacking-the-build-process-2.md",
        "title": "Unpacking the Build Process: Part 2",
        "description": "Part 2 looks at the core of the build process, turning the source into compiled code. In Serpent OS this is handled by",
        "date": "2021-09-20T06:04:12.000Z",
        "formattedDate": "September 20, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.655,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Unpacking the Build Process: Part 2",
          "date": "2021-09-20T06:04:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/unpacking-the-build-process-2/Featured.webp",
          "slug": "2021/09/20/unpacking-the-build-process-2"
        },
        "prevItem": {
          "title": "Optimal File Locality",
          "permalink": "/blog/2021/10/04/optimal-file-locality"
        },
        "nextItem": {
          "title": "Unpacking the Build Process: Part 1",
          "permalink": "/blog/2021/08/25/unpacking-the-build-process-1"
        }
      },
      "content": "Part 2 looks at the core of the build process, turning the source into compiled code. In Serpent OS this is handled by\nour build tool `boulder`. It is usually the part of the build that takes the longest, so where speed ups have the most\nimpact. How long it takes is largely down to the performance of your compiler and what compile flags you are building\nwith.\n\n<!--truncate-->\n\nThis post follows on from [Part 1](/blog/2021/08/25/unpacking-the-build-process-part-1).\n\n# Turning Source into Compiled Code\n\nThe steps for compiling code are generally quite straight-forward:\n- Setting up the build (cmake, configure, meson)\n- Compiling the source (in parallel threads)\n- Installing the build into a package directory\n\nThis will build software compiled against packages installed on your system. It's a bit more complicated when packaging\nas we first set up an environment to compile in (Part 1). But even then you have many choices to make and each can have\nan impact on how long it takes to compile the code. Do you build with Link Time Optimizations (LTO) or Profile Guided\nOptimizations (PGO), do you build the package for performance or for the smallest size? Then there's packages that\nbenefit considerably from individual tuning flags (like `-fno-semantic-interposition` with `python`). With so many\npossibilities, `boulder` helps us utilize them through convenient configuration options.\n\n# What Makes boulder so Special?\n\nAs I do a lot of packaging and performance tuning, `boulder` is where I spend most of my time. Here are some key\nfeatures that `boulder` brings to make my life easier.\n\n - Ultimate control over build C/CXX/LDFLAGS via the tuning key\n - Integrated 2 stage context sensitive PGO builds with a single line workload\n - Able to switch between `gnu` and `llvm` toolchains easily\n - Rules based package creation\n - Control the extraction locations for multiple upstream tarballs\n\n`boulder` will also be used to generate and amend our `stone.yml` files to take care of as much as possible\nautomatically. This is only the beginning for `boulder` as it will continue to be expanded to learn new tricks to make\npackaging more automated and able to bring more information to help packagers know when they can improve their\n`stone.yml`, or alert them that something might be missing.\n\nSerpent OS is focused on the performance of produced packages, even if that means that builds will take longer to\ncomplete. This is why we have put in significant efforts to speed up the compiler and setup tools in order to offset and\nminimize the time needed to enable greater performance.\n\n# Why do You Care so Much About Setup Time?\n\nMy initial testing focused on the performance of `clang` as well as the time taken to run `cmake` and `configure`. This\nlays the foundation for all future work in expanding the Serpent OS package archives at a much faster pace. On the\nsurface, running `cmake` can be a small part of the overall build. However, it is important in that it utilizes a single\nthread, so is not sped up by adding more CPU cores like the compile time is. With a more compile heavy build, our\nhighly tuned compiler can build the source in around 75s. So tuning the setup step to run in 5s rather than 10s actually\nreduces the overall build time by an additional 6%!\n\nThere are many smaller packages where the setup time is an even higher proportion of the overall build and becomes more\nrelevant as you increase the numbers of threads on the builder. For example, when building `nano` on the host, the\nconfigure step takes 13.5s, while the build itself takes only 2.3s, so there's significant gains to be had from speeding\nup the setup stage of a build (which we will absolutely be taking advantage of!).\n\n# A Closer Look at the clang Compiler's Performance\n\nA first cut of the compiler results were shared earlier in [Initial Performance Testing](../blog/2021/08/02/initial-performance-testing),\nand given the importance to overall build time, I've been taking a closer look. In the post I said that `\"At stages\nwhere I would have expected to be ahead already, the compile performance was only equal\"` and now I have identified the\ndiscrepancy.\n\nI've tested multiple configurations for the `clang` compiler and noticed that changing the default standard C++ library\nmakes a difference to the time of this particular build. The difference in the two runs is compiling `llvm-ar` with the\nLLVM libraries of `compiler-rt/libc++/libunwind` or the GNU libraries of `libgcc/libstdc++`. And just to be clear, this\nis increasing the time of compiling `llvm-ar` with `libc++` vs `libstdc++` and not to do with the performance of either\nlibrary. The `clang` compiler itself is built with `libc++` in both cases as it produces a faster compiler.\n\n![img](/img/blog/unpacking-the-build-process-2/Featured.webp)\n\n\n| Test using clang      | Serpent LLVM libs | Serpent GNU libs |  Host        |\n|-----------------------|-------------------|------------------|--------------|\n| `cmake` LLVM          | 5.89s             | 5.67s            |  10.58s      |\n| Compile -j4 `llvm-ar` | 126.16s           | 112.51s          |  155.32s     |\n| `configure` gettext   | 36.64s            | 36.98s           |  63.55s      |\n\nThe host now takes `38% longer` than the Serpent OS clang when building with the same GNU libraries and is much more in\nline with my expectations. Next steps will be getting `bolt` and `perf` integrated into Serpent OS to see if we can\nshave even more time off the build.\n\nWhat remains unclear is whether this difference is due to something specifically in the `LLVM` build or whether it would\ntranslate to other C++ packages. I haven't noticed a 10% increase in build time when performing the full compiler\nbuild with `libc++` vs `libstdc++`."
    },
    {
      "id": "2021/08/25/unpacking-the-build-process-1",
      "metadata": {
        "permalink": "/blog/2021/08/25/unpacking-the-build-process-1",
        "source": "@site/blog/unpacking-the-build-process-1.md",
        "title": "Unpacking the Build Process: Part 1",
        "description": "While the build process (or packaging as it's commonly referred to) is largely hidden to most users, it forms a",
        "date": "2021-08-25T09:04:12.000Z",
        "formattedDate": "August 25, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.585,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Unpacking the Build Process: Part 1",
          "date": "2021-08-25T09:04:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/unpacking-the-build-process-1/Featured.webp",
          "slug": "2021/08/25/unpacking-the-build-process-1"
        },
        "prevItem": {
          "title": "Unpacking the Build Process: Part 2",
          "permalink": "/blog/2021/09/20/unpacking-the-build-process-2"
        },
        "nextItem": {
          "title": "A Rolling Boulder Gathers No Moss",
          "permalink": "/blog/2021/08/10/a-rolling-boulder-gathers-no-moss"
        }
      },
      "content": "While the build process (or packaging as it's commonly referred to) is largely hidden to most users, it forms a\nfundamental and important aspect to the efficiency of development. In Serpent OS this efficiency also extends to\nusers via source based builds for packages you may want to try/use that aren't available as binaries upstream.\n\n<!--truncate-->\n\nThe build process can be thought of in three distinct parts, setting up the build environment, compiling the source\nand post build analysis plus package creation. Please note that this process hasn't been finalized in Serpent OS so\nwe will be making further changes to the process where possible.\n\n# Setting up the Build Environment\n\nSome key parts to setting up the build environment:\n\n- Downloading packages needed as dependencies for the build\n- Downloading upstream source files used in the build\n- Fetching and analyzing the latest repository index\n- Creating a reproducible environment for the build (chroot, container or VM for example)\n- Extracting and installing packages into the environment\n- Extracting tarballs for the build (this is frequently incorporated as part of the build process instead)\n\nWhile the focus of early optimization work has been on build time performance, there's more overhead to creating\npackages time than simply compiling code. Now the compiler is in a good place, we can explore the rest of the\nbuild process.\n\n# Packaging is More than just Compile Time\n\nThere's been plenty of progress in speeding up the creation of the build environment such as parallel downloads to\nreduce connection overhead and using `zstd` for the fast decompression of packages. But there's more that we can\ndo to provide an optimal experience to our packagers.\n\nSome parts of the process are challenging to optimize as while you can download multiple files at once to ensure\nmaximum throughput, you are still ultimately limited by your internet speed. When packaging regularly (or building\na single package multiple times), downloaded files are cached so become a one off cost. One part we have taken\nparticular interest in speeding up is extracting and installing packages into the environment.\n\n![img](/img/blog/unpacking-the-build-process-1/Featured.webp)\n\n# The Dynamic Duo: boulder and moss\n\nInstalling packages to a clean environment can be the most time consuming part of setting up the build (excluding\nfetching files which is highly variable). Serpent OS has a massive advantage with the design of `moss` where\npackages are cached (extracted on disk) and ready to be used by multiple roots, including the creation of clean\nbuild environments for `boulder`. Having experienced a few build systems in action, setting up the root could take\nquite some time with a large number of dependencies (even getting over a minute). `moss` avoids the cost of extracting\npackages entirely every build by utilizing its cache!\n\nThere are also secondary benefits to how `moss` handles packages via its caches where disk writes are reduced by only\nneeding to extract packages a single time. But hang on, won't you be using `tmpfs` for builds? Of course we will have\nRAM builds as an option and there are benefits there too! When extracting packages to the RAM disk, it consumes memory\nwhich can add up to more than a GB before the build even begins. `moss` allows for us to start with an empty `tmpfs` so\nwe can perform larger builds before exhausting the memory available on our system.\n\nAnother great benefit is due to the atomic nature of `moss`. This means that we can add packages to be cached as\nsoon as they're fetched while waiting for the remaining files to finish downloading (both for `boulder` and system\nupdates). Scheduling jobs becomes much more efficient and we can have the build environment available in moments after\nthe last file is downloaded!\n\n`moss` allows us to eliminate one of the bigger time sinks in setting up builds, enabling developers and\ncontributors alike to be more efficient in getting work done for Serpent OS. With greater efficiency it may become\npossible to provide a second architecture for older machines (if the demand arises).\n\n# Part 1?\n\nYes, there's plenty more to discuss so there will be more follow up posts showing the cool features Serpent OS is doing\nto both reduce the time taken to build packages and in making packages easier to create so stay tuned!"
    },
    {
      "id": "2021/08/10/a-rolling-boulder-gathers-no-moss",
      "metadata": {
        "permalink": "/blog/2021/08/10/a-rolling-boulder-gathers-no-moss",
        "source": "@site/blog/a-rolling-boulder-gathers-no-moss.md",
        "title": "A Rolling Boulder Gathers No Moss",
        "description": "We actually did it. Super pleased to announce that moss is now capable",
        "date": "2021-08-10T11:02:37.000Z",
        "formattedDate": "August 10, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.105,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "A Rolling Boulder Gathers No Moss",
          "date": "2021-08-10T11:02:37.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/a-rolling-boulder-gathers-no-moss/Featured.webp",
          "slug": "2021/08/10/a-rolling-boulder-gathers-no-moss"
        },
        "prevItem": {
          "title": "Unpacking the Build Process: Part 1",
          "permalink": "/blog/2021/08/25/unpacking-the-build-process-1"
        },
        "nextItem": {
          "title": "Moss DB Progress",
          "permalink": "/blog/2021/08/03/moss-db-progress"
        }
      },
      "content": "We actually did it. Super pleased to announce that `moss` is now capable\nof installing and removing packages. Granted, super rough, but gotta start\nsomewhere right?\n\n<!--truncate-->\n\n![Transactional system roots + installation to disk](/img/blog/a-rolling-boulder-gathers-no-moss/Featured.webp)\n\nOK let's recap. A moss archive is super weird, and consists of multiple\ncontainers, or payloads. We use a strongly typed binary format, per-payload\ncompression (Currently `zstd`), and don't store files in a typical archive\nfashion.\n\nInstead a `.stone` file (moss archive) has a Content Payload, which is\na compressed \"megablob\" of all the unique files in a given package. The\nvarious files contained within that \"megablob\" are described in an IndexPayload,\nwhich simply contains some IDs and offsets, acting much like a lookup table.\n\nThat data alone doesn't actually tell us **where files go** on the filesystem\nwhen installed. For that, we have a specialist Layout Payload, encoding the\nfinal layout of the package on disk.\n\nAs can be imagined, the weirdness made it quite difficult to install in\na trivial fashion.\n\n# Databases\n\nWell, persistence really. Thanks to `RocksDB` and our new `moss-db` project,\nwe can trivially store information we need from each package we \"precache\".\nPrimarily, we store full system states within our new StateDB, which at\npresent is simply a series of package ID selections grouped by a unique\n64-bit integer.\n\nAdditionally we remember the layouts within the LayoutDB so that we can\neventually recreate said layout on disk.\n\n# Precaching\n\nBefore we actually commit to an install, we try to precache all of the stone\nfiles in our pool. So we unpack the content payload (\"megablob\"), split it\ninto various unique files in the pool ready for use. At this point we also\nrecord the Layouts, but do not \"install\" the package to a system root.\n\n# Blitting\n\nThis is our favourite step. When our cache is populated, we gather all\nrelevant layouts for the current selections, and then begin applying them\nin a **new** system root. All directories and symlinks are created as normal,\nwhereas any regular file is hardlinked from the pool. This process takes a\nfraction of a second and gives us completely clean, deduplicated system roots.\n\nCurrently these live in `/.moss/store/root/$ID/usr`. To complete the transaction,\nwe update `/usr` to point to the new `usr` tree **atomically** assuming that\na reboot isn't needed. In future, boot switch logic will update the tree for us.\n\n# Removal\n\nRemoval is quite the same as installation. We simply remove the package IDs\nfrom the new state selections (copied from the last state) and blit a new\nsystem root, finally updating the atomic `/usr` pointer.\n\n![Removal](/img/blog/a-rolling-boulder-gathers-no-moss/Removal.webp \"Removal of packages with moss. Everything is a transaction\")\n\n# Tying it all together\n\nWe retain classic package management traits such as having granular selections,\nmultiple repositories, etc, whilst sporting advanced features like full system\ndeduplication and transactions/rollbacks.\n\nWhen we're far enough along, it'll be possible to boot back to the last working\ntransaction without requiring an internet connection. Due to the use of pooling\nand hardlinks, each transaction tree is only a few KiB, with files shared between\neach transaction/install.\n\n# On the list..\n\nWe need some major cleanups, better error handling, logging, timed functions,\nand an eventloop driven process to allow parallel fetching/precaching prior\nto final system rootfs construction.\n\nIt's taken us a very long time to get to this point, and there is still more\nwork to be done. However this is a major milestone and we can now start\nadding features and polish.\n\nOnce the required features are in place, we'll work on the much needed\npre alpha ISO :) If you fancy helping us get to that stage quicker, do\ncheck out our OpenCollective! (We won't limit prealpha availability,\ndon't worry :))"
    },
    {
      "id": "2021/08/03/moss-db-progress",
      "metadata": {
        "permalink": "/blog/2021/08/03/moss-db-progress",
        "source": "@site/blog/moss-db-progress.md",
        "title": "Moss DB Progress",
        "description": "I'll try to make this update as brief as I can but it's certainly an important one, so",
        "date": "2021-08-03T13:16:16.000Z",
        "formattedDate": "August 3, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.03,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Moss DB Progress",
          "date": "2021-08-03T13:16:16.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/moss-db-progress/Featured.webp",
          "slug": "2021/08/03/moss-db-progress"
        },
        "prevItem": {
          "title": "A Rolling Boulder Gathers No Moss",
          "permalink": "/blog/2021/08/10/a-rolling-boulder-gathers-no-moss"
        },
        "nextItem": {
          "title": "Initial Performance Testing",
          "permalink": "/blog/initial-performance-testing"
        }
      },
      "content": "I'll try to make this update as brief as I can but it's certainly an important one, so\nlet's dive right into it. The last few weeks have been rough but work on our package manager\nhas still been happening. Today we're happy to reveal another element of the equation: moss-db.\n\n![Putting moss-db to the test](/img/blog/moss-db-progress/Featured.webp)\n\n<!--truncate-->\n\n`moss-db` is an abstract API providing access to simplistic \"Key Value\" stores. We had initially used\nsome payload based files as databases but that introduced various hurdles, so we decided to take\na more abstract approach to not tie ourselves to any specific implementation of a database.\n\nOur main goal with `moss-db` is to encapsulate the [RocksDB](https://rocksdb.org/) library, providing sane, idiomatic access\nto a key value store. \n\n# High level requirements\n\nAt the highest level, we needed something that could store arbitrary keys and values, grouped\nby some kind of common key (commonly known as \"buckets\"). We've succeeded in that abstraction,\nwhich also required us to fork a `rocksdb-binding` to add the `Transform` APIs required.\n\nAdditionally we required idiomatic range behaviours for iteration, as well as generic access\npatterns. To that affect we can now `foreach` a bucket, pipe it through the awesomely powerful\n`std.algorithm` APIs, and automatically encode/decode keys and values through our generic APIs\nwhen implementing the `mossdbEncode()` and `mossdbDecode()` functions for a specific **type**.\n\nIn a nutshell, this was the old, ugly, hard way:\n\n\n```d\n\t/* old, hard way */\n\tauto nameZ = name.toStringz();\n\tint age = 100;\n\tubyte[int.sizeof] ageEncoded = nativeToBigEndian(ageEncoded);\n\tdb.setDatum(cast(ubyte[]) (nameZ[0 .. strlen(nameZ)]), ageEncoded);\n```\n\nAnd this is the new, shmexy way:\n\n```d\n    db.set(\"john\", 100);\n    db.set(\"user 100\", \"bobby is my name\");\n\n    auto result = db.get!int(\"john\");\n    if (result.found)\n    {\n        writeln(result.value);\n    }\n\n    auto result2 = db.get!string(\"user 100\");\n    if (result2.found)\n    {\n        writeln(result2.value);\n    }\n```\n\nIt's quite easy to see the new API lends itself robustly to our needs, so that\nwe may implement stateful, strongly typed databases for moss.\n\n# Next Steps\n\nEven though some APIs in `moss-db` may still be lacking (remove, for example)\nwe're happy that it can provide the foundation for our next steps. We now need\nto roll out the new `StateDB`, `MetaDB` and `LayoutDB`, to record system states,\npackage metadata, and filesystem layout information, respectively.\n\nWith those 3 basic requirements in place we can combine the respective works\ninto installation routines. Which, clearly, warrants another blog post ... :)\n\nFor now you can see the relevant projects on our [GitLab](https://gitlab.com/serpent-os/core) project."
    },
    {
      "id": "/initial-performance-testing",
      "metadata": {
        "permalink": "/blog/initial-performance-testing",
        "source": "@site/blog/initial-performance-testing.md",
        "title": "Initial Performance Testing",
        "description": "With further progress on boulder, we can now build native stone packages with some easy tweaks such as profile",
        "date": "2021-08-02T01:28:24.000Z",
        "formattedDate": "August 2, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.855,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Initial Performance Testing",
          "date": "2021-08-02T01:28:24.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/initial-performance-testing/Featured.webp"
        },
        "prevItem": {
          "title": "Moss DB Progress",
          "permalink": "/blog/2021/08/03/moss-db-progress"
        },
        "nextItem": {
          "title": "Boulder Keeps On Rolling",
          "permalink": "/blog/2021/07/27/boulder-keeps-on-rolling"
        }
      },
      "content": "With further progress on `boulder`, we can now build native stone packages with some easy tweaks such as profile\nguided optimizations (PGO) and link time optimizations (LTO). That means we can take a first look at what the\nperformance of the first cut of Serpent OS shows for the future. The tests have been conducted using\n[`benchmarking-tools`](https://github.com/sunnyflunk/benchmarking-tools) with Serpent OS measured in a `chroot` on\nthe same host with the same kernel and config.\n\n![img](/img/blog/initial-performance-testing/Featured.webp)\n\n<!--truncate-->\n\nOne of the key focuses for early in the project is on reducing build time. Every feature can either add or subtract\nfrom the time it takes to produce a package. With a source/binary hybrid model, users will greatly benefit from the\nfaster builds as well. In terms of what I've targeted in these tests is the performance of `clang` and testing some\ncompiler flag options on `cmake`.\n\n# Clang Shows its Promise\n\n`clang` has always been a compiler with a big future. The performance credentials have also been improving each release\nand are now starting to see it perform strongly against its `GNU` counterpart. It is common to hear that `clang` is slow\nand produces less optimized code. I will admit that most distros provide a slow build of `clang`, but that will not be\nthe case in Serpent OS.\n\nIt is important to note that in this comparison the Host distro has pulled in some patches from `LLVM-13` that greatly\nimprove the performance of `clang`. Prior to this, their tests actually took `50% longer` for `cmake` and `configure`\nbut only `10% longer` for compiling. `boulder` does not yet support patching in builds so the packages are completely\nvanilla.\n\n| Test using clang      | Serpent      | Host        | Difference |\n|-----------------------|--------------|-------------|------------|\n| `cmake` LLVM          | 5.89s        | 10.58s      | 79.7%      |\n| Compile -j4 `llvm-ar` | 126.16s      | 155.32s     | 23.1%      |\n| `configure` gettext   | 36.64s       | 63.55s      | 73.4%      |\n\nBased on the results during testing, the performance of `clang` in Serpent OS still has room to improve and was just a\nquick tuning pass. At stages where I would have expected to be ahead already, the compile performance was only equal\n(but `cmake` and `configure` were still well ahead).\n\n# GCC Matters Too!\n\nWhile `clang` is the default compiler in Serpent OS, there may be instances where the performance is not quite where it\ncould be. It is common to see software have more optimized code paths where they are not tested with `clang` upstream. As\nan example, here's a couple of patches in flac ([1](https://github.com/xiph/flac/commit/67ea8badadd3e63b8e8af5fe837d075104569330),\n[2](https://github.com/xiph/flac/commit/d4a1b345dd16591ff6f17c67ee519afebe2f9792)) that demonstrate this being improved.\nUsing `benchmarking-tools`, it is easy to see where `gcc` and `clang` builds are running different functions via `perf`\nresults.\n\nIn circumstances where the slowdown is due to hitting poor optimization paths in `clang`, we always have the option to\nbuild packages using `gcc`, where the `GNU` toolchain is essential for building `glibc`. Therefore having a solid `GNU`\ntoolchain is important but small compile time improvements won't be noticed by users or developers as much.\n\n| Test using gcc      | Serpent      | Host        | Difference |\n|---------------------|--------------|-------------|------------|\n| `cmake` LLVM        | 7.00s        | 7.95s       | 13.6%      |\n| Compile `llvm-ar`   | 168.11s      | 199.07s     | 18.4%      |\n| `configure` gettext | 45.45s       | 51.93s      | 14.3%      |\n\n# An OS is More Than Just a Compiler\n\nWhile the current bootstrap exists only as a starting point for building the rest of Serpent OS, there are some other\npackages we can easily test and compare. Here's a summary of those results.\n\n| Test                              | Serpent      | Host        | Difference |\n|-----------------------------------|--------------|-------------|------------|\n| Pybench                           | 1199.67ms    | 1024.33ms   | -14.6%     |\n| `xz` Compress Kernel (-3 -T1)     | 42.67s       | 46.57s      | 9.1%       |\n| `xz` Compress Kernel (-9 -T4)     | 71.25s       | 76.12s      | 6.8%       |\n| `xz` Decompress Kernel            | 8.03s        | 8.18s       | 1.9%       |\n| `zlib` Compress Kernel            | 12.60s       | 13.17s      | 4.5%       |\n| `zlib` Decompress Kernel          | 5.14s        | 5.21s       | 1.4%       |\n| `zstd` Compress Kernel (-8 -T1)   | 5.77s        | 7.06s       | 22.3%      |\n| `zstd` Compress Kernel (-19 -T4)  | 51.87s       | 66.52s      | 28.3%      |\n| `zstd` Decompress Kernel          | 2.90s        | 3.08s       | 6.3%       |\n\n# State of the Bootstrap\n\nFrom my experiences with testing the bootstrap, it is clear there's some cobwebs in there that require some more iterations\nof the toolchain.\nThere also seems to be some slowdowns in not including all the dependencies of some packages. Once more packages are included,\nnaturally all the testing will be redone and help influence the default compiler flags of the project.\n\nIt's not yet clear the experience of using `libc++` vs `libstdc++` with the `clang` compiler. Once the cobwebs are out and\nSerpent OS further developed, the impact (if any) should become more obvious. There are also some parts not yet included in\n`boulder` such as stripping files, LTO and other flags by default that will speed up loading libraries. At this stage this is\ndeliberate until integrating outputs from builds (such as symbol information).\n\nBut this provides an excellent platform to build out the rest of the OS. The raw speed of the `clang` compiler will make\niterating and expanding the package set a real joy!\n\n# Hang On, What's Going on With Python?\n\nVery astute of you to notice! `python` in its current state is an absolute minimal build of `python` in order to run `meson`.\nHowever, I did an `analyze` run in `benchmarking-tools` where it became obvious that they were doing completely different\nthings.\n\n{{<figure_screenshot_one image=\"initial-performance-testing/Featured\" caption=\"Apples and Oranges comparison\">}}\n\nFor now I'll simply be assuming this will sort itself out when `python` is built complete with all its functionality. And\nbefore anyone wants to point the finger at `clang`, you get the same result with `gcc`."
    },
    {
      "id": "2021/07/27/boulder-keeps-on-rolling",
      "metadata": {
        "permalink": "/blog/2021/07/27/boulder-keeps-on-rolling",
        "source": "@site/blog/boulder-keeps-on-rolling.md",
        "title": "Boulder Keeps On Rolling",
        "description": "Squirrelling away in the background has been some great changes to bring boulder closer to its full potential. Here's",
        "date": "2021-07-27T05:05:23.000Z",
        "formattedDate": "July 27, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.32,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Boulder Keeps On Rolling",
          "date": "2021-07-27T05:05:23.000Z",
          "draft": false,
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/boulder-keeps-on-rolling/Featured.webp",
          "slug": "2021/07/27/boulder-keeps-on-rolling"
        },
        "prevItem": {
          "title": "Initial Performance Testing",
          "permalink": "/blog/initial-performance-testing"
        },
        "nextItem": {
          "title": "Yet Another Website Revamp",
          "permalink": "/blog/2021/06/22/yet-another-website-revamp"
        }
      },
      "content": "Squirrelling away in the background has been some great changes to bring `boulder` closer to its full potential. Here's\na quick recap of some of the more important ones.\n\n![boulder hard at work](/img/blog/boulder-keeps-on-rolling/Featured.webp)\n\n<!--truncate-->\n\n# Key Changes to Boulder\n\n - Fixed a path issue that prevented manifests from being written for 32bit builds\n - Added keys to control where the tarballs are extracted to\n   - This results in a greatly simplified setup stage when using multiple upstreams\n - More customizations to control the final c{,xx}flags exported to the build\n - Added a key to run at the start of every stage so definitions can be exported easily in the `stone.yml` file\n - Fixed an issue where duplicate hash files were being included in the Content Payload\n   - This resulted in reducing the Content Payload size by 750MB of a glibc build with duplicate locale files\n - Finishing touches on profile guided optimization (PGO) builds - including clang's context-sensitive PGO\n   - Fixed a few typos in the macros to make it all work correctly\n   - Profile flags are now added to the build stages\n   - Added the llvm profile merge steps after running the workload\n   - Recreate a clean working directory at the start of each PGO phase\n\nWith all this now in place, the build stages of `boulder` are close to completion. But don't worry, there's plenty more\ngreat features to come to make building packages for Serpent OS simple, flexible and performant. Next steps will be testing\nout these new features to see how much they can add to the overall `stage4` performance."
    },
    {
      "id": "2021/06/22/yet-another-website-revamp",
      "metadata": {
        "permalink": "/blog/2021/06/22/yet-another-website-revamp",
        "source": "@site/blog/yet-another-website-revamp.md",
        "title": "Yet Another Website Revamp",
        "description": "Yeah we did it again. Project is back with a bang and working",
        "date": "2021-06-22T12:11:06.000Z",
        "formattedDate": "June 22, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.52,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Yet Another Website Revamp",
          "date": "2021-06-22T12:11:06.000Z",
          "authors": "ikey",
          "categories": [
            "news"
          ],
          "tags": [
            "news"
          ],
          "image": "/img/blog/yet-another-website-revamp/Featured.webp",
          "slug": "2021/06/22/yet-another-website-revamp"
        },
        "prevItem": {
          "title": "Boulder Keeps On Rolling",
          "permalink": "/blog/2021/07/27/boulder-keeps-on-rolling"
        },
        "nextItem": {
          "title": "Migrating to GitLab",
          "permalink": "/blog/2021/06/15/migrating-to-gitlab"
        }
      },
      "content": "Yeah we did it again. Project is back with a bang and working\naway so we revamped our website during our port to Bootstrap 5.\n\n![Shiny updated website revamp](/img/blog/yet-another-website-revamp/Featured.webp)\n\n<!--truncate-->\n\n\nThis wasn't just a visual update however, we've revamped much of\nthe **content** too. Look around the new homepage, we've linked new\npages explaining our project goals and vision.\n\nSure, we're not quite there **yet** but we're definitely seeing\nexciting times here at Serpent OS. This is officially my 2nd week\nback to full time work and we're making steady progress.\n\n_Special thanks to the team and caasi (Jacob) for streamling the revamp_\n\nTtfn!"
    },
    {
      "id": "2021/06/15/migrating-to-gitlab",
      "metadata": {
        "permalink": "/blog/2021/06/15/migrating-to-gitlab",
        "source": "@site/blog/migrating-to-gitlab.md",
        "title": "Migrating to GitLab",
        "description": "Hot on the heels of yesterday's transition from IRC to Matrix, we're",
        "date": "2021-06-15T12:26:39.000Z",
        "formattedDate": "June 15, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.45,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Migrating to GitLab",
          "date": "2021-06-15T12:26:39.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/migrating-to-gitlab/Featured.webp",
          "slug": "2021/06/15/migrating-to-gitlab"
        },
        "prevItem": {
          "title": "Yet Another Website Revamp",
          "permalink": "/blog/2021/06/22/yet-another-website-revamp"
        },
        "nextItem": {
          "title": "Snake Pit Is Live",
          "permalink": "/blog/2021/06/14/snake-pit-is-live"
        }
      },
      "content": "Hot on the heels of yesterday's transition from IRC to Matrix, we're\nrevamping our infrastructure to further enable success for our project.\n\nRecently we heard the company behind Phabricator is [winding down operations](https://admin.phacility.com/phame/post/view/11/phacility_is_winding_down_operations/).\nWe can't really complain, as we used it freely and really appreciated\nthe project while using it.\n\n![Shiny new GitLab project](/img/blog/migrating-to-gitlab/Featured.webp)\n\n<!--truncate-->\n\n\nGoing forwards, we decided to collapse the disparity between our code hosting\nsolutions. A major requirement for us is a support for many repositories, namespace\nnesting, and global overview of issues outside of a specific subproject or repository.\n\nIn our testing, only GitLab ticked all the boxes. As such, we've migrated\nour GitHub projects over to our new, nicely organised, [GitLab home](https://gitlab.com/serpent-os).\n\nI've switched our Phabricator instance to read-only, and imported all of our\nprojects, and most of the issues, to the new public GitLab project. It will\ntake a couple of days to get the migration completed, updating submodule URLs, etc,\nbut we've already archived all of our GitHub projects.\n\n# Other Items\n\nIn keeping with naming consistency, the official Twitter account was renamed\nto [Serpent_OS](https://twitter.com/Serpent_OS). Additionally, my shiny new\nbroadband connection will be installed on the 28th of this month, unlocking\nfurther development powah.\n\nPeter has been working on a messaging overhaul for the website, which will be\nlaunched Fairly Soonish. After that, a visual update will follow. In parallel\nto this, we'll be putting moss to work.\n\n# Repurposing our sponsored server\n\n[Fosshost](https://fosshost.org) kindly sponsored us very early on with a\nserver to host our Phabricator instance and downloads. For now we'll explicitly\nrepurpose it to be a high speed package and ISO origin for the mirror network,\nto ensure constant access for clients. Huge thanks to Fosshost for the continued\nsponsorship!"
    },
    {
      "id": "2021/06/14/snake-pit-is-live",
      "metadata": {
        "permalink": "/blog/2021/06/14/snake-pit-is-live",
        "source": "@site/blog/snake-pit-is-live.md",
        "title": "Snake Pit Is Live",
        "description": "It took quite some effort, but I've purchased and built a dedicated office",
        "date": "2021-06-14T11:07:06.000Z",
        "formattedDate": "June 14, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.92,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Snake Pit Is Live",
          "date": "2021-06-14T11:07:06.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/snake-pit-is-live/Featured.jpeg",
          "slug": "2021/06/14/snake-pit-is-live"
        },
        "prevItem": {
          "title": "Migrating to GitLab",
          "permalink": "/blog/2021/06/15/migrating-to-gitlab"
        },
        "nextItem": {
          "title": "Migrating to Matrix",
          "permalink": "/blog/2021/06/14/migrating-to-matrix"
        }
      },
      "content": "It took quite some effort, but I've purchased and built a dedicated office\nspace where I'll be working on Serpent OS daily. As my eldest son has just\nstarted full time school, I have considerably more usable hours to do so!\n\nOnce the PVC shed was built, my wife and I deep cleaned it, fitted carpet,\nadded a chair and desk, and honestly it's an awesome little workspace.\nFuture plans will involve a dedicated power supply (twin 16A RCD split\non mobile home supply), ethernet connection, fans, etc.\n\nThis office space is what I was trying to build when I set out with the\nOffice Shed event goal on Open Collective, which we can now finally deem\nsuccessful.\n\n# Broadband\n\nI've signed up to broadband with ~40mbps min download, ~69mbps average,\njust waiting for the engineer appointment to install a new master socket.\n\nNew day time availability, high speed internet and a dedicated office space\nwill mean drastically improved productivity as well as permitting live streams!\n\nRemember, you can now join us on Matrix to have real time discussions or talk\nwith the team!"
    },
    {
      "id": "2021/06/14/migrating-to-matrix",
      "metadata": {
        "permalink": "/blog/2021/06/14/migrating-to-matrix",
        "source": "@site/blog/migrating-to-matrix.md",
        "title": "Migrating to Matrix",
        "description": "Hello all! Apologies for the long, long delays. I'm writing this",
        "date": "2021-06-14T10:24:04.000Z",
        "formattedDate": "June 14, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.91,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Migrating to Matrix",
          "date": "2021-06-14T10:24:04.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2021/06/14/migrating-to-matrix"
        },
        "prevItem": {
          "title": "Snake Pit Is Live",
          "permalink": "/blog/2021/06/14/snake-pit-is-live"
        },
        "nextItem": {
          "title": "Let There Be Databases",
          "permalink": "/blog/2021/05/18/let-there-be-databases"
        }
      },
      "content": "Hello all! Apologies for the long, long delays. I'm writing this\nblog post in my new office, which will be detailed in a follow up\nblog post. On with the juicy - we're migrating to Matrix.\n\n<!--truncate-->\n\nEffective immediately, we're switching away from our IRC channels to\na more modern, streamlined experience, specifically Matrix. Among a few\nmotivations is offline persistence - keeping messages around when you're\nnot connected.\n\nWhile this might not seem terribly important to many, not everyone is able\nor willing to run an IRC bouncer for this purpose. We also want to ensure\nwe have a friendly and accessible way to engage with the team from a variety\nof OS/device combinations, empowering the team with a level of mobility and\nstability.\n\n# Main Channels\n\nYou can find us at [#serpentos](https://app.element.io/#/room/#serpentos:matrix.org) on `matrix.org` or in our development channel,\n[#serpentos-dev](https://app.element.io/#/room/#serpentos-dev:matrix.org)  on `matrix.org`. We're already there, and will begin disbanding\nthe IRC channel today.\n\n# Recommended Clients\n\n..There are loads, I really can't recommend one above another. Due to the nice\nUX and my epic laziness, I'm quite happy with [element](https://app.element.io)"
    },
    {
      "id": "2021/05/18/let-there-be-databases",
      "metadata": {
        "permalink": "/blog/2021/05/18/let-there-be-databases",
        "source": "@site/blog/let-there-be-databases.md",
        "title": "Let There Be Databases",
        "description": "We haven't been too great on sharing progress lately, so welcome to an overdue update on",
        "date": "2021-05-18T09:31:32.000Z",
        "formattedDate": "May 18, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.47,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Let There Be Databases",
          "date": "2021-05-18T09:31:32.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/let-there-be-databases/Featured.webp",
          "slug": "2021/05/18/let-there-be-databases"
        },
        "prevItem": {
          "title": "Migrating to Matrix",
          "permalink": "/blog/2021/06/14/migrating-to-matrix"
        },
        "nextItem": {
          "title": "Moss Unlocked",
          "permalink": "/blog/2021/03/20/moss-unlocked"
        }
      },
      "content": "We haven't been too great on sharing progress lately, so welcome to an overdue update on\ntimelines, progress, and database related shmexiness.\n\n<!--truncate-->\n\n![Emerging DB design](/img/blog/let-there-be-databases/Featured.webp)\n\nOK, so you may remember `moss-format`, our module for reading and writing moss binary archives.\nIt naturally contains much in the way of binary serialisation support, so we've extended the\nformat to support \"database\" files. In reality, they are more like tables binary encoded into\na single file, identified by a filepath.\n\nThe DB archives are currently stored **without** compression to ensure 0-copy mmap() access\nwhen loading from disk, as a premature optimisation. This may change in future if we find the\nDB files taking up too much disk space.\n\nSo far we've implemented a \"StateMetaDB\", which stores metadata on every recorded State on\nthe system, and right now I'm in the progress of implementing the \"StateEntriesDB\", which is\nsomething akin to a binary encoded dpkg selections file with candidate specification reasons.\n\nNext on the list is the LayoutsDB (file manifests) and the CacheDB, for recording refcounts\nof every cached file in the OS pool.\n\n# Integration with Serpent ECS\n\nAn interesting trial we're currently implementing is to hook the DB implementation up to\nour Entity Component system from the Serpent Engine, in order to provide fast, cache coherent,\nin memory storage for the DB. It's implemented using many nice DLang idioms, allowing the full\nuse of `std.algorithm` APIs:\n\n```d\n    auto states()\n    {\n        import std.algorithm : map;\n\n        auto view = View!ReadOnly(entityManager);\n        return view.withComponents!StateMetaArchetype\n            .map!((t) => StateDescriptor(t[1].id, t[3].name, t[4].description,\n                    t[1].type, t[2].timestamp));\n    }\n    ...\n    \n\t/* Write the DB back in ascending numerical order */\n\tdb.states\n\t\t.array\n\t\t.sort!((a, b) => a.id < b.id)\n\t\t.each!((s) => writeOne(s));\n```\n\n# Tying it all together\n\nOk, so you can see we need basic DB types for storing the files for each moss archive, plus each\ncache and state entry. If you look at the ECS code above, it becomes quite easy to imagine how this\nwill impact installation of archives. Our new install code will simply modify the existing state,\ncache the incoming package, and apply the layout from the DB to disk, before committing the new\nDB state.\n\nIn essence, our DB work is the current complex target, and installation is a <50 line trick\ntying it all together.\n\n```d\n\n\t/* Psuedocode */\n\t\n\tState newState...\n\n\tforeach (pkgID; currentState.filter!((s) => s.reason == SelectionReason.Explicit))\n\t{\n\t\tauto fileSet = layoutDB.get(pkgID);\n\t\tfileSet.array.sort!((a, b) => a.path < b.path).each!((f) => applyLayout(f));\n\t\t/* Record into new state */\n\t\t...\n\t}\n```\n\nI know we've gone the long way around creating the package manager, and many times there have\nbeen delays over the past year. However, we're no longer looking at light at the end of the\ntunnel, we're actively walking out of it. The remaining minor puzzles will unblock the\ninstallation routine, which in time will enable first alpha images of dubious quality.\nHowever rough our jumping off point, it will be an awesome journey.\n\nTil next time - \n\n Ikey"
    },
    {
      "id": "2021/03/20/moss-unlocked",
      "metadata": {
        "permalink": "/blog/2021/03/20/moss-unlocked",
        "source": "@site/blog/moss-unlocked.md",
        "title": "Moss Unlocked",
        "description": "Well, it's not all doom and gloom these days. We've actually made some",
        "date": "2021-03-20T14:07:30.000Z",
        "formattedDate": "March 20, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.065,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Moss Unlocked",
          "date": "2021-03-20T14:07:30.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/moss-unlocked/Featured.webp",
          "slug": "2021/03/20/moss-unlocked"
        },
        "prevItem": {
          "title": "Let There Be Databases",
          "permalink": "/blog/2021/05/18/let-there-be-databases"
        },
        "nextItem": {
          "title": "Optimising Package Distribution",
          "permalink": "/blog/2021/03/16/optimising-package-distribution"
        }
      },
      "content": "Well, it's not all doom and gloom these days. We've actually made some\nsignificant progress in the last few days, so it seems a good time to\nshare a progress update.\n\n![Extracting content from moss archives](/img/blog/moss-unlocked/Featured.webp)\n\n<!--more-->\n\n# moss can now extract\n\nOh yeah, that totally happened. So, we can now successfully build moss\npackages from `boulder` and then extract them to disk once again with\n`moss`. This might sound totally uninteresting, but it demonstrates\nthat our format is actually working as intended.\n\nAdmittedly the code is super rough within `moss` and somewhat proof\nof concept, however we're able to extract the contents of the moss\narchive and rebuild the layout on disk.\n\n# What makes extraction difficult?\n\nWell, quirky new format for one. A moss archive currently consists of\n4 \"payloads\", or major sections:\n\n - `MetaPayload`\n\n\tContains all package **information**, with strongly typed keys.\n\n - `IndexPayload`\n\n\tContains the IDs of all unique files (hash) and their offsets within\n\tthe ContentPayload\n\n - `LayoutPayload`\n\n\tA sequence of specialised structs describing the final \"layout\" of the\n\tpackage on disk, with attributes, paths, and for regular files, the ID\n\tof the file in the ContentPayload to build this file **from**.\n\n - `ContentPayload`\n \n\tA binary blob containing every unique file from the package, in an order\n\tdescribed by the `IndexPayload`. The files are stored sequentially with\n\tno gaps.\n\nAdditionally, each payload is independently compressed using `zstd`. In order\nto extract files to disk, we must first decompress `ContentPayload` to a\ntemporary file. Next, we blit each file from the \"megablob\" to the cache store,\nusing the `IndexPayload` to understand the offsets. Finally, we apply the\ninstructions in `LayoutPayload` to construct the final layout on disk, hardlinking\nthe cache assets into their final locations, setting attributes, etc.\n\nNet result? Deduplication on a per package basis, and a system-wide deduplication\npolicy allowing sharing of identical assets on disk between multiple packages.\nThis will also power our core update mechanism, whereby each update is atomic,\nand is simply the difference on disk from the previous version, permitting a\npowerful rollback mechanism.\n\n# Room for improvement\n\n![Multiple](/img/blog/moss-unlocked/Multiple.webp \"Extracting multiple moss archives\")\n\nThere are areas where we're doing things inefficiently, and we'll certainly improve\nthat in future revisions of the important. For example, `IndexPayload` actually\nwastes some bytes by storing redundant information that can be calculated at\nruntime. Additionally, we want to use the zstd C APIs directly to gain the level\nof control we **actually need**. We're also going to wrap the `copy_file_range`\nsyscall to make extraction of the content payload more efficient and not rely on\nuserspace copies.\n\nHowever, we're working towards a prealpha, and some inefficiencies are OK. Our\nfirst port of call will be a prealpha image constructed from `.stone` files, produced\nby `boulder`, installed by `moss`. This will validate our toolchain and tooling\nand serve as a jumping off point for the project.\n\nStay tuned, there is a whole bunch of awesome coming now that moss is officially\nunlocked and progressing.\n\n# And finally\n\nI want to thank everyone who is currently supporting the project. I also want to personally\nthank you for your understanding of the setbacks of real life, given the difficult times myself\nand my family have been going through. I hope it is clear that I remain committed to the\nproject and it's future, which is why we're transparently run and funded via [OpenCollective](https://opencollective.com/serpent-os).\n\nDespite the rough times, work continues, and awesome people join our ranks on a regular basis.\nStability is on the immediate horizon and work with Serpent OS grows exponentially. You can\nbe part of our journey, and help us build an amazing community and project that outlives us\nall."
    },
    {
      "id": "2021/03/16/optimising-package-distribution",
      "metadata": {
        "permalink": "/blog/2021/03/16/optimising-package-distribution",
        "source": "@site/blog/optimising-package-distribution.md",
        "title": "Optimising Package Distribution",
        "description": "Getting updates as fast as possible to users has made deltas a popular and sought after feature for distributing",
        "date": "2021-03-16T22:19:12.000Z",
        "formattedDate": "March 16, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 5.46,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Peter O'Connor",
            "key": "sunnyflunk"
          }
        ],
        "frontMatter": {
          "title": "Optimising Package Distribution",
          "date": "2021-03-16T22:19:12.000Z",
          "authors": "sunnyflunk",
          "tags": [
            "news"
          ],
          "image": "/img/blog/optimising-package-distribution/Featured.webp",
          "slug": "2021/03/16/optimising-package-distribution"
        },
        "prevItem": {
          "title": "Moss Unlocked",
          "permalink": "/blog/2021/03/20/moss-unlocked"
        },
        "nextItem": {
          "title": "Moss Format: Read Write Support",
          "permalink": "/blog/2021/02/17/moss-format-read-write-support"
        }
      },
      "content": "Getting updates as fast as possible to users has made deltas a popular and sought after feature for distributing\npackages. Over the last couple of days, I've been investigating various techniques we can look at to support deltas in\n`moss`.\n\n<!--truncate-->\n\n# Trade-offs Between Packages and Deltas\n\nMinimising the size of updates is particularly valuable where files are downloaded many times and even better if they're\nupdated infrequently. With a rolling release, packages will be updated frequently, so creating deltas can become\nresource intensive, especially if supporting updates over a longer period of time. Therefore it's important to get the\nright balance between compression speed, decompression memory and minimising file size.\n\n|              | Package priorities           | How best to meet these needs |\n|--------------|------------------------------|------------------------------|\n| Developers   | Creation speed               | Quickly created packages     |\n| Users        | File size and update speed   | Size minimised deltas        |\n\nFrom the users point of view, minimising file size and upgrade time are important priorities, but for a developer, the\nspeed at which packages are created and indexed is vital to progression. Deltas are different to packages in that they\naren't required immediately, so there's minimal impact in taking longer to minimise their size. By getting deltas right,\nwe can trade-off the size of packages to speed up development, while users will not be affected and fetch only size\noptimised deltas.\n\n# Test Case - QtWebEngine\n\nQtWebEngine provides a reasonable test case where the package is a mix of binaries, resources and translations, but\nabove average in size (157.3MB uncompressed). The first trade-off for speed over size has already been made by\nincorporating `zstd` in moss over `xz`, where even with max compression `zstd` is already 5.6% larger than using `xz`.\nThis is of course due to the amazing decompression speeds where `zstd` is magnitudes faster.\n\n![Compression levels with zstd](/img/blog/optimising-package-distribution/Featured.webp)\n\nWith maximum compression, large packages can take over a minute to compress. With a moderate increase in size, one can\nreduce compression time by 2-10x. While making me happier as a developer, it does create extra network load during\nupdates.\n\n| Full Package    | zstd -16 -T0 | zstd -16 | zstd -19 -T0 | zstd -19 | zstd -22 | xz -9  |\n|-----------------|--------------|----------|--------------|----------|----------|--------|\n| Time to create  | 5.4s         | 26.8s    | 27.8s        | 56.0s    | 70.6s    | 66.5s  |\n| Size of package | 52.6MB       | 52.6MB   | 49.2MB       | 49.2MB   | 48.4MB   | 45.9MB |\n\n# Deltas to the Rescue!\n\nThere are two basic methods for deltas. One simple method is to include only files that have been changed since the\nearlier release. With reproducible builds, it is typical to create the same file from the same inputs. However, with a\nrolling release model, files will frequently have a small change from dependency changes and new versions of the package\nitself. In these circumstances the delta size starts to get closer to the full package anyway. As a comparison to other\ndelta techniques, this approach resulted in a 38.2MB delta as it was a rebuild of the same version at a different time\nso the resources and translations were unchanged (and therefore omitted from the delta).\n\nAn alternative is a binary diff, which is a significant improvement when files have small changes between releases.\n`bsdiff` has long been used for this purpose and trying it out (without knowing much about it) I managed to create a\ndelta of 33.2MB, not a bad start at all.\n\nTo highlight the weakness of the simple method, when you compare the delta across a version change, the simple delta was\nonly a 7% reduction of the full package (as most files have changed), while using an optimal binary diff, it still\nmanaged to achieve a respectable 31% size reduction.\n\n# A New Contender\n\nWhile looking into alternatives, I happened to [stumble across](https://github.com/facebook/zstd/releases/tag/v1.4.5)\na new feature in `zstd` which can be used to create deltas. As we already use `zstd` heavily it should make integration\neasier. `--patch-from` allows `zstd` to use the old uncompressed package as a dictionary to create the newer package. In\nthis way common parts between the releases will be reused in order to reduce the file size. Playing around I quickly\nachieved the same size as `bsdiff`, and with a few tweaks was able to further reduce the delta by a **further 23.5%!**\nThe best part is that it has the same speedy decompression as `zstd`, so it will recreate most packages from deltas in\nthe blink of an eye!\n\n| Delta           | only changed files | bsdiff | zstd -19 | zstd -22 | zstd -22 --zstd=chainLog=30 |\n|-----------------|--------------------|--------|----------|----------|-----------------------------|\n| Time to create  | 60.8s              | 153.0s | 85.5s    | 111.6s   | 131.8s                      |\n| Size of delta   | 38.2MB             | 33.2MB | 33.3MB   | 28.5MB   | 25.4MB                      |\n\n\n# Next Steps\n\nThere's certainly a lot of information to digest, but the next step is to integrate a robust delta solution into the\n`moss` format. I really like the `zstd` approach, where you can tune for speed with an increase in size if desired. With\nminimising on delta size, users can benefit from smaller updates while developers can benefit from faster package\ncreation times.\n\nSome final thoughts for future consideration:\n\n- `zstd` has seen many improvements over the years, so I believe that ratios and performance will see incremental\nimprovements over time. Release [1.4.7](https://github.com/facebook/zstd/releases/tag/v1.4.7) already brought\nsignificant improvements to deltas (which are reflected in this testing).\n- The highest compression levels (`--ultra`) are single threaded in `zstd`, so delta creation can be done in parallel to\nmaximise utilisation.\n- Over optimising the tunables can have a negative impact on both speed and size. As an example,\n`--zstd=targetLength=4096` did result in a 2KB reduction in size at the same speed, but when applied to different inputs\n(kernel source tree), it not only made it larger by a few hundred bytes, but added **4 minutes** to delta creation!\n- Memory usage of applying deltas can be high for large packages (1.74GB for the kernel source tree) as it has to ingest\nthe full size of the original uncompressed package. It is certainly possible to split up payloads with some delta users\neven creating patches on a per file basis. It is a bit more complicated when library names change version numbers each\nrelease with only the SONAME symlink remaining unchanged.\n- There's always the option to repack packages at higher compression levels later (when deltas are created). This solves\ngetting the package 'live' ASAP and minimises the size (eventually), but adds some complication."
    },
    {
      "id": "2021/02/17/moss-format-read-write-support",
      "metadata": {
        "permalink": "/blog/2021/02/17/moss-format-read-write-support",
        "source": "@site/blog/moss-format-read-write-support.md",
        "title": "Moss Format: Read Write Support",
        "description": "It's been 8 days since our last blogpost and a lot of development work has happened",
        "date": "2021-02-17T22:35:11.000Z",
        "formattedDate": "February 17, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.665,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Moss Format: Read Write Support",
          "date": "2021-02-17T22:35:11.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/moss-format-read-write-support/Featured.webp",
          "slug": "2021/02/17/moss-format-read-write-support"
        },
        "prevItem": {
          "title": "Optimising Package Distribution",
          "permalink": "/blog/2021/03/16/optimising-package-distribution"
        },
        "nextItem": {
          "title": "Unlocking Moss",
          "permalink": "/blog/2021/02/09/unlocking-moss"
        }
      },
      "content": "It's been 8 days since our last blogpost and a lot of development work has happened\nin that time. Specifically, we've completely reworked the internals of the `moss-format`\nmodule to support read/write operations.. Which means installation is coming soon (TM)\n\n![Development work on moss-format](/img/blog/moss-format-read-write-support/Featured.webp)\n\n<!--truncate-->\n\nSo, many commits have been made to the core repositories, however the most\nimportant project to focus on right now is `moss-format`, which we used to\ndefine and implement our binary and source formats. This module is shared\nbetween `boulder`, our build tool, and `moss`, our package manager.\n\nWe've removed the old enumeration approach from the `Reader` class, instead\nrequiring that it processes **data** payloads in-place, deferring reading the\n**content** payload streams. We've also enforced strong typing to allow\nsafe and powerful APIs:\n\n```d\n        import moss.format.binary.payload.meta : MetaPayload;\n\n        auto reader = new Reader(File(argv[0], \"rb\"));\n        auto metadata = reader.payload!MetaPayload();\n```\n\nRight now we can read and write our `MetaPayload` from and to the stream,\nallowing us to encode & decode the metadata associated with the package,\nwith strong type information (i.e. `Uint64`, `String`, etc.)\n\n# Next Steps\n\nWe need to restore the `IndexPayload`, `LayoutPayload` and `ContentPayload`\ndefinitions. The first two are simply **data** payloads and will largely\nfollow the design of the newly reimplemented `MetaPayload`. Then we restore\n`ContentPayload` support, and this will allow the next steps: unpack, install.\n\nMany of the babysteps required are done now, which power our binary format.\nThe design of the API is done in a way which will allow powerful manipulation\nvia the `std.algorithm` and `std.range` APIs, enabling extremely simple and\nreliable installation routines.\n\nIt might seem odd that we've spent so much time on the core **format**,\nhowever I should point out that the design of the format is central to the\nOS design. Our installation routine is **not** unpacking of an archive.\n\nWith our binary format, the stream contains multiple PayloadHeaders,\nwith fixed lengths, type, tag, version, and compression information.\nIt is up to each Payload implementation to then parse the binary\ndata contained within. Currently our Payloads are compressed using ZSTD, though\nother compression algorithms are supported.\n\nSo, we have the `MetaPayload` for metadata. Additionally, we encode all **unique**\nfiles in the package in a single compressed payload, the `ContentPayload`. The\noffset to each unique file (by hash) is stored within the `IndexPayload`, and\nthe filesystem layout is encoded as data within the `LayoutPayload`.\n\nIn order to unpack a moss package, the `ContentPayload` blob will be decompressed\nto a temporary location. Then, each struct within the `IndexPayload` can be used\nto copy portions (`copy_file_range`) of the blob to the cache store for permanence. We skip each Index\nif its already present within the cache. Finally, the target filesystem is populated\nwith a view of all installed packages using the `LayoutPayload` structs, creating\na shallow installation using hardlinks and directories.\n\nThe net result is deep deduplication, atomic updates, and flexibility for the user.\nOnce we add transactions it'll be possible to boot an older version of the OS using\nthe deduplication capabilities for offline recovery. Additionally there is no requirement\nfor file deletion, rename or modification for an update to succeed.\n\n# Nutshell\n\nHuge progress. Major excitement. Such wow. Soon alphas."
    },
    {
      "id": "2021/02/09/unlocking-moss",
      "metadata": {
        "permalink": "/blog/2021/02/09/unlocking-moss",
        "source": "@site/blog/unlocking-moss.md",
        "title": "Unlocking Moss",
        "description": "Wait, what? Another blog post? In the same WEEK? Yeah totally doing that",
        "date": "2021-02-09T12:45:34.000Z",
        "formattedDate": "February 9, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.475,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Unlocking Moss",
          "date": "2021-02-09T12:45:34.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/unlocking-moss/Featured.webp",
          "slug": "2021/02/09/unlocking-moss"
        },
        "prevItem": {
          "title": "Moss Format: Read Write Support",
          "permalink": "/blog/2021/02/17/moss-format-read-write-support"
        },
        "nextItem": {
          "title": "Cleanups Complete",
          "permalink": "/blog/2021/02/08/cleanups-complete"
        }
      },
      "content": "Wait, what? Another blog post? In the same WEEK? Yeah totally doing that\nnow. So, this is just another devlog update but there have been some interesting\nupdates that we'd like to share.\n\n<!--truncate-->\n\n# LDC present in the bootstrap\n\nThanks to some awesome work from Peter, we now have LDC (The LLVM D Lang Compiler)\npresent in the stage3 bootstrap. To simplify the process we use the official\nbinary release of LDC to bootstrap LDC for Serpent OS.\n\nIn turn, this has allowed us to get to a point where we can now build `moss` and\n`boulder` within stage3. This is super important, as we'll use the stage3 chroot\nand `boulder` to produce the binary packages that create stage4.\n\nSome patching has taken place to prevent using `ld.gold` and instead use `lld`\nto integrate with our toolchain decisions.\n\n![A wild LDC appears](/img/blog/unlocking-moss/Featured.webp)\n\n# Reworking the Payload implementation\n\nOriginally our prototype moss format only contained a `ContentPayload` for files, and\na `MetaPayload` for metadata, package information, etc. As a result, we opted for simple\nstructs, basic case handling and an iterable `Reader` implementation.\n\nAs the format expanded, we bolted deduplication in as a core feature. To achieve this,\nwe converted the `ContentPayload` into a \"megablob\", i.e. every **unique** file in the\npackage, one after the other, all compressed in one operation. We then store the offsets\nand IDs of these files within an `IndexPayload` to allow splitting the \"megablob\" into\nseparate, unique assets. Consequently, we added a `LayoutPayload` which describes the\nfinal file system layout of the package, referencing the unique ID (hash) of the asset\nto install.\n\nSo, while the format grew into something we liked, the code supporting it became very\nlimiting. After many internal debates and discussions, we're going to approach the\nformat from a different angle on the code front.\n\nIt will no longer be necessary/possible to iterate payloads in _sequence_ within a\nmoss archive, instead we'll preload the data (unparsed) and stick it aside when reading\nthe file, winding it to the `ContentPayload` segment if found. After initial loading of\nthe file is complete, the `Reader` API will support retrieval (and lazy unpacking ) of\na data segment. In turn this will allow code to \"grab\" the content, index and layout\npayloads and use advanced APIs to cache assets, and apply them to disk in a single\nblit operation.\n\nIn short, we've unlocked the path to installing moss packages while preserving the\nadvanced features of the format. The same new APIs will permit introspection of the\narchives metadata, and of course, storing these records in a stateful system database.\n\n# Thank you for the unnecessary update\n\nOh you're quite welcome :P Hopefully now you can see our plan, and that we're on track\nto meet our not-28th target. Sure, some code needs throwing away, but all codebases\nare evolutionary. Our major hurdle has been solved (mentally) - now it's just time\nto implement it and let the good times roll."
    },
    {
      "id": "2021/02/08/cleanups-complete",
      "metadata": {
        "permalink": "/blog/2021/02/08/cleanups-complete",
        "source": "@site/blog/cleanups-complete.md",
        "title": "Cleanups Complete",
        "description": "Well, we're officially back to working around the clock. After spending",
        "date": "2021-02-08T11:50:46.000Z",
        "formattedDate": "February 8, 2021",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.615,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Cleanups Complete",
          "date": "2021-02-08T11:50:46.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/cleanups-complete/Featured.webp",
          "slug": "2021/02/08/cleanups-complete"
        },
        "prevItem": {
          "title": "Unlocking Moss",
          "permalink": "/blog/2021/02/09/unlocking-moss"
        },
        "nextItem": {
          "title": "Build System Functional",
          "permalink": "/blog/2020/10/build-system-functional"
        }
      },
      "content": "Well, we're officially back to working around the clock. After spending\nsome time optimising my workflow, I've been busy getting the entire codebase\ncleaned up, so we can begin working towards an MVP.\n\n![Lots and lots of work](/img/blog/cleanups-complete/Featured.webp)\n\n<!--truncate-->\n\n# Codebase hygiene\n\nSince Friday, I've been working extensively on cleaning up the codebase for the\nfollowing projects:\n\n - boulder\n - moss\n - moss-core\n - moss-format\n\nAs it now stands, 1 lint issue (a non-issue, really) exists across all 4\nprojects. A plethora of issues have been resolved, ranging from endian correctness\nin the format to correct idiomatic D-lang integration and module naming.\n\n# Bored\n\nGranted, cleanups aren't all that sexy. Peter has been updating many parts of\nthe bootstrap project, introducing systemd 247.3, LLVM 11.0.1, etc. We now have\nall 3 stages building correctly again for the x86_64 target.\n\n# And then....\n\nYikes, tough audience! So we've formed a new working TODO which will make its\nway online as a public document at some point. The first stage, cleanups, is\ndone. Next is to restore feature development, working specifically on the\nextraction and install routines. From there, much work and cadence will be\nunlocked, allowing us to work towards an MVP.\n\n![TODO](/img/blog/cleanups-complete/TODO.webp \"See, we organised stuff\")\n\n# You keep saying MVP..\n\nI know, it makes me feel all ... cute and professional. At the moment we're cooking up\na high level description of how an MVP demonstration could be deployed. While most of\nthe features are ambiguous, our current ambition is to deploy a preinstalled QCOW2\nSerpent OS image as a prealpha to help validate moss/boulder/etc.\n\nIt'll be ugly. Likely slow. Probably insecure as all hell. But it'll be **something**.\nIt'll allow super basic interaction with moss, and a small collection of utilities that\ncan be used in a terminal environment. No display whatsoever. That can come in a future\niteration :)\n\nETA? ~~Definitely not by the 28th of February~~. When it's done."
    },
    {
      "id": "2020/10/build-system-functional",
      "metadata": {
        "permalink": "/blog/2020/10/build-system-functional",
        "source": "@site/blog/build-system-functional.md",
        "title": "Build System Functional",
        "description": "Wow, has it been a hectic few weeks, and it definitely shows: last time",
        "date": "2020-10-11T09:17:25.000Z",
        "formattedDate": "October 11, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.08,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Build System Functional",
          "date": "2020-10-11T09:17:25.000Z",
          "draft": false,
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/build-system-functional/Featured.webp",
          "slug": "2020/10/build-system-functional"
        },
        "prevItem": {
          "title": "Cleanups Complete",
          "permalink": "/blog/2021/02/08/cleanups-complete"
        },
        "nextItem": {
          "title": "Rebootstrapped With Glibc",
          "permalink": "/blog/2020/09/25/rebootstrapped-with-glibc"
        }
      },
      "content": "Wow, has it been a hectic few weeks, and it definitely shows: last time\nwe blogged it was about re-bootstrapping with glibc. Feels like ancient news\nalready! So, what's new in the world of Serpent OS? Apart from yours truly\nnow being proud parent to a beautiful baby girl, work has resumed on\nthe development of Moss, our package manager. And it builds stuff. Awesomely.\nLet's quickly catch up with those updates and see where we're headed next.\n\n<!--truncate-->\n\n![Look at all the buildiness](/img/blog/build-system-functional/Featured.webp)\n\n\n# It lives!\n\nWe're now able to build **the majority** of a moss package. Notice I've made\na distinction there. So, we're able to run the build instructions, and use\nall of the metadata, configuration and macros available. The only thing\nwe're not actually doing is dumping the built files into a binary package.\n\n\nWe're able to do the build-system part _rather well_, now. Right now\nwe support the following features in the build system:\n\n - Multiple, profile-based architecture support\n - Automatic `-m32` profile based cross compilation support for 32-bit libraries on 64-bit OS\n - Basic Profile Guided Optimisation via the `workload` key. Set a workload, optimise accordingly.\n - LLVM Context Sensitive Profile Guided Optimisation. This is default for LLVM with the `workload` key, and comes for free, with multiple build stages.\n - Profile based tuning options, such as `optimize` for speed, size, disabling hardening, etc.\n - Trivially switch between `gnu` and `llvm` toolchain in `stone.yml`, with profiles knowing the right flags to use with tuning options.\n - Recursive macro support in build scripts, defined on per-profile level\n - Architecture specific profiles support in stone.yml, i.e. `profiles -> aarch64 -> setup:` to avoid if/else spaghetti.\n\n# Next on the agenda\n\nNow that the huge amount of scaffolding work has been done, we can actually turn the results of builds\ninto installable binary packages using our `moss.format.binary` module. We'll add some magic sauce to\nhave automatic subpackages + inter-package dependencies, along with the expected automatic runtime\ndependencies + such. Cue some linting, et voila, build tool that's a pleasure to work with.\n\nOnce we have all those packages building, we'll need a way to install them. Luckily some scaffolding\nis in place for this already, and it won't take much effort to support `moss install somepkg.stone`.\nThen we throw a few dozen packages in the mix, add some dependency handling, repo support, Bob's your\nuncle, and your aunt is downloading exclusive early-access images from our [Open Collective](https://opencollective.com/serpent-os) as soon\nas they're available. :O"
    },
    {
      "id": "2020/09/25/rebootstrapped-with-glibc",
      "metadata": {
        "permalink": "/blog/2020/09/25/rebootstrapped-with-glibc",
        "source": "@site/blog/rebootstrapped-with-glibc.md",
        "title": "Rebootstrapped With Glibc",
        "description": "Only a few days ago we told you of our switch from musl to glibc. That has now been implemented",
        "date": "2020-09-25T13:23:28.000Z",
        "formattedDate": "September 25, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.42,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Rebootstrapped With Glibc",
          "date": "2020-09-25T13:23:28.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/rebootstrapped-with-glibc/Featured.webp",
          "slug": "2020/09/25/rebootstrapped-with-glibc"
        },
        "prevItem": {
          "title": "Build System Functional",
          "permalink": "/blog/2020/10/build-system-functional"
        },
        "nextItem": {
          "title": "Results Of The Experiment",
          "permalink": "/blog/2020/09/22/results-of-the-experiment"
        }
      },
      "content": "Only a few days ago we told you of our switch from `musl` to `glibc`. That has now been implemented\nin the `bootstrap-scripts` project. The rebootstrap is complete and we now have GCC, LLVM, binutils and\nglibc offering a hybrid toolchain. Most software is built and linked with clang, however linking with\nlibgcc is possible both dynamically and statically.\n\n![Semi functional systemd-nspawn](/img/blog/rebootstrapped-with-glibc/Featured.webp))\n\n<!--truncate-->\n\nOur next steps are fairly logical, but feel free to have a read.\n\n# Enable early multilib support\n\nHaving established our goals, it is clear we also need to support 32-bit binaries on a 64-bit\nSerpent OS installation, such as Steam. We'll add multilib support to our bootstrap to ensure\nwe ship with support for these 32-bit binaries very quickly.\n\n# Validate AArch64 support\n\nWe made a bit of a mess in switching to glibc, so, we'll go back and build for aarch64 and fix any fallout.\nIt is still important we target this architecture, as the community explicitly requested support for the\nRaspberry Pi 4!\n\n# Pivot back to moss\n\nWe had to get this bootstrap out of the way before it became a growing issue. While it's still \"dirty\" and\nsome paths or configurations are not to our liking, the groundwork is sufficient to enable developing moss\nnow.\n\nOur first port of call is to develop the build side of moss, i.e. emitting binary packages that can then\nbe installed to form a rootfs.\n\n# Timescale\n\nIt actually all depends on moss now. Once moss can produce packages we can firmly put ourselves into stage4\nand have package git repositories on our infrastructure, and open up contributions from the community. We\nintend to build a **sustainable** contribution led ecosystem so we can all get back to doing what we love\nmost, using Linux on a daily basis to get stuff done.\n\nSome of you may know my significant other is expecting a child, or rather, she was expected yesterday.\nWhen this situation changes I will of course take some downtime to help the growing family and then\nreturn to work.\n\n# Support\n\nAt the time of writing, 18 amazing individuals have contributed to us on our [Open Collective](https://opencollective.com/serpent-os).\nOn behalf of the team, I want to thank you from the bottom of our hearts. Once we leave bootstrap, we'll\ndedicate ourselves to providing you with a reliable, up to date operating system that is always on the edge\nand always safe to upgrade, enabling you to do what **you** want to do. Our team has extensive and\nsomewhat unique experience in building custom systems with high performance and reliability that\ncaters for everything from devops to gaming.\n\nWhen the project funds are high enough we'll order Raspberry Pis for myself and Peter to fully enable\nAArch64, and a new build box as my laptop is somewhat dated now. With your help, we can achieve\nthe impossible, as a community."
    },
    {
      "id": "2020/09/22/results-of-the-experiment",
      "metadata": {
        "permalink": "/blog/2020/09/22/results-of-the-experiment",
        "source": "@site/blog/results-of-the-experiment.md",
        "title": "Results Of The Experiment",
        "description": "It seems like only yesterday we announced to the world a Great Experiment.",
        "date": "2020-09-22T17:05:39.000Z",
        "formattedDate": "September 22, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.695,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Results Of The Experiment",
          "date": "2020-09-22T17:05:39.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/results-of-the-experiment/Featured.webp",
          "slug": "2020/09/22/results-of-the-experiment"
        },
        "prevItem": {
          "title": "Rebootstrapped With Glibc",
          "permalink": "/blog/2020/09/25/rebootstrapped-with-glibc"
        },
        "nextItem": {
          "title": "Source Format Defined",
          "permalink": "/blog/2020/09/21/source-format-defined"
        }
      },
      "content": "It seems like only yesterday we announced to the world a [Great Experiment](https://serpentos.com/blog/2020/07/01/the-great-experiment/).\nIt was in fact 2 months ago, and a whole lot of work has happened since that point. A few take-homes are immediately clear, the primary\none being the need to be a community-oriented Linux distribution.\n\n<!--truncate-->\n\nTo quote ourselves 2 months ago:\n\n\n```\n If the experiment is a success, which of course means having tight controls on scope and timescale,\n then one would assume the primary way to use Serpent OS would be through some downstream repackaging\n or reconfiguration, i.e. basing upon the project, to meet specific user demand.\n```\n\nIt turns out that so far the experiment has been successful, and being forkable is still at the very\nheart of our goals. Others have joined us on our journey, and expressed the same passion in our goals\nas we have. A community has formed around the project, with individuals sharing the same ambitions\nfor a reliable, rolling operating system with a powerful package manager.\n\n![Toolchain bootstrap](/img/blog/results-of-the-experiment/Featured.webp)\n\n\n# An open community\n\nOver the past 2 months we've transformed from set of ideas into a transparent, community-first organisation\nwith clear leadership and open goals. I've stepped into the Benevolent Dictator For Life position, and Peter\nhas taken on daily responsibilities for the project running. Aydemir is now our treasurer on Open Collective,\nand many individuals contribute to our project.\n\n# Transactional package management\n\nNo need to rehash this, but the defining feature of Serpent OS has clearly become moss, something initially\nnot anticipated when we started. A read-only root filesystem, transactional operations, rollbacks, deduplicating\nthroughout and atomic updates. Combine that with a rolling release model, stateless policy and ease-of-use,\nthe core feature-set is already powerful.\n\n# Deferral of musl integration\n\nIn recent weeks we've been working on `libwildebeest` and `libc-support`, primarily as a stop-gap to provide\nglibc compatibility without using glibc. While musl has many advantages, it is clear to us now that writing\nanother libc through our support projects isn't what we originally planned. With that in mind we're adopting\nglibc and putting our musl works under community ownership, until such time as reevaluation shows that musl is\nwhat is needed in Serpent OS. Note the primary motivator here is investing our efforts where it makes sense,\nand obtaining the best results in the most manageable fashion for our users.\n\nOur new toolchain configuration will be as follows:\n\n - LLVM/clang as primary toolchain\n - libc++ as C++ library\n - glibc as C library\n - gcc+binutils (`ld.bfd`) provided to build glibc, elfutils, etc.\n - Permitting per-package builds using GCC, i.e. kernel to alleviate Clang IAS issues.\n\nThus our toolchain will in fact be a hybrid GNU/LLVM one. This will allow both source and binary compatibility\nwith the majority of desktop + server Linux distributions, facilitating choice of function for our users.\n\nIt should be noted this decision has been made after much discussion internally, on our IRC, on our OpenCollective,\netc. Our bootstrap-scripts is being improved to support both `glibc` and `musl`, so that the decision can continuously\nbe reviewed. If we reach a position whereby musl inclusion once again makes sense, thanks to atomic updates\nfrom moss, it will be possible to switch.\n\n# Final thoughts\n\nInitially Serpent OS emerged as a collective agreement on IRC as a set of notions as opinions. Over the past few\nmonths those opinions have solidified into tangible ideas, and a sense of community. In keeping with what is\nright for the community, our messaging has been reworked.\n\nIt is fair to say our initial stance appeared quite hostile, as a bullet-point list of exhaustion with past\nexperiences. As we've pivoted to being a fully community-oriented distribution, we've established our goals\nof being a reliable, flexible, open, rolling Linux distribution with powerful features imbued by the\npackage manager, and an upstream-first approach.\n\nAs such we've agreed to not let our own pet-peeves interfere with the direction of the project, and instead\nenable users to do what they wish on Serpent OS, be it devops, engineering, browsing, gaming, you name it.\nWe're a general purpose OS with resilience at the core.\n\nOur focus is on the usability and reliability of the OS - thus our efforts will be invested in areas such\nas the package manager, hardware enabling, the default experience, etc.\n\nSo, strap yourself in, as we're fond of saying. Development of Serpent OS is about to accelerate rapidly."
    },
    {
      "id": "2020/09/21/source-format-defined",
      "metadata": {
        "permalink": "/blog/2020/09/21/source-format-defined",
        "source": "@site/blog/source-format-defined.md",
        "title": "Source Format Defined",
        "description": "Following quickly on the heels of yesterday's announcement that the binary format has been defined, we've",
        "date": "2020-09-21T15:09:46.000Z",
        "formattedDate": "September 21, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.2,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Source Format Defined",
          "date": "2020-09-21T15:09:46.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/source-format-defined/Featured.webp",
          "slug": "2020/09/21/source-format-defined"
        },
        "prevItem": {
          "title": "Results Of The Experiment",
          "permalink": "/blog/2020/09/22/results-of-the-experiment"
        },
        "nextItem": {
          "title": "Moss Format Defined",
          "permalink": "/blog/2020/09/20/moss-format-defined"
        }
      },
      "content": "Following quickly on the heels of yesterday's announcement that the binary format has been defined, we've\nnow implemented the initial version of our source format. The source format provides metadata on a package\nalong with instructions on how to build the package.\n\n![A very trivial package spec](/img/blog/source-format-defined/Featured.webp)\n\n<!--truncate-->\n\nThe next step of course, is to implement the build tool, converting the source specification into a binary package\nthat the end user can install. With our 2 formats defined, we can now go ahead and implement the build routines. \n\n# A YAML based format\n\nThe eagle-eyed among you will already see this is a derivation of the `package.yml` format I originally created\nwhile at [Solus](https://getsol.us). Minor adaptations to the format have been made to support multiple architectures\nvia the `profiles` key, and package splitting behaviour has now been grouped under a `packages` key to make\nthe structure more readable.\n\nIn `package.yml`, one would have to redefine subpackage summaries as a key in a list of the primary `summary` key,\nsuch as:\n\n```yaml\n\n    rundeps:\n        - primary-run-dep\n        - dev: secondary-run-dep\n    summary:\n        - Some Summary\n        - dev: Some different summary\n```\n\nWe've opted to group \"Package Definition\" behaviour into core structs, which are allowed to appear in the root-level\npackage and subpackages:\n\n```yaml\n\n    summary: Top-level summary\n    packages:\n        - dev:\n            summary: Different summary\n            rundeps: Different rundeps\n```\n\n# Multiple architecture support\n\nIn keeping with the grouping behaviour, we're baking multiple architecture configurations into the YML file. A common\nissue encountered with the older format was how to handle `emul32`:\n\n```yaml\n\n    setup: |\n        if [[ -z \"${EMUL32BUILD}\" ]]; then\n            %configure --some-emul32-option\n        else\n            %configure\n        fi\n\n    build: |\n    %make\n```\n\nOur new approach is to group Build Definitions into the root level struct, which may then individually be overridden for\neach architecture. For example:\n\n```yaml\n\n    profiles:\n        - ia32:\n            setup: |\n        %configure --some-emul32-option\n    setup: |\n    %configure\n```\n\n![Permutations](/img/blog/source-format-defined/Permutations.webp \"More advanced uses of the spec\")\n\n# Differences\n\nAs you can see it is highly similar to package.yml - which is a great format. However, with our tooling and aims being\nslightly different, it was time to reevaluate the spec and bolster it where appropriate. We're happy to share our\nchanges, but in the interest of not causing a conflict between the 2 variants, we'll be calling ours \"stone.yml\".\n\nOur main motivation came from the tooling, which is written in the D language. With D we were able to create a strongly\ntyped parser and explicit schema, and with a struct-based approach it made it more trivial to group similar\ndefinitions.\n\nOther than that, we have the same notions with the format, intelligent automatic package splitting, ease of developer\nexperience, etc."
    },
    {
      "id": "2020/09/20/moss-format-defined",
      "metadata": {
        "permalink": "/blog/2020/09/20/moss-format-defined",
        "source": "@site/blog/moss-format-defined.md",
        "title": "Moss Format Defined",
        "description": "The core team have been hard at work lately implementing the Moss package manager.",
        "date": "2020-09-20T12:48:50.000Z",
        "formattedDate": "September 20, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.55,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Moss Format Defined",
          "date": "2020-09-20T12:48:50.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/moss-format-defined/Featured.webp",
          "slug": "2020/09/20/moss-format-defined"
        },
        "prevItem": {
          "title": "Source Format Defined",
          "permalink": "/blog/2020/09/21/source-format-defined"
        },
        "nextItem": {
          "title": "Website Revamp",
          "permalink": "/blog/2020/09/13/website-revamp"
        }
      },
      "content": "The core team have been hard at work lately implementing the [Moss package manager](https://github.com/serpent-linux/moss).\nWe now have an initial version of the binary format that we're happy with, so we thought we'd share a progress update\nwith you.\n\n![Developing wotk on moss](/img/blog/moss-format-defined/Featured.webp)\n\n<!--truncate-->\n\n# Explaining the format\n\nBriefly, the binary container format consists of 4 payloads:\n\n - Meta (Information on the package)\n - Content (a binary blob containing all files)\n - Index (indices to files within the binary blob)\n - Layout (How to apply the files to disk)\n\nEach payload is verified internally using a CRC64-ISO, and contains basic information such as the length of the payload\nboth compressed and uncompressed, the compression algorithm used (`zstd` and `zlib` supported) as well as the type and\nversion of the payload. All multiple-byte values are stored in Big Endian order (i.e. Network Byte Order).\n\n![Payloads](/img/blog/moss-format-defined/Payloads.webp \"All relevant payloads\")\n\nInternally the representation of a Payload is defined as a 32-byte struct:\n\n```d\n    @autoEndian uint64_t length = 0; /* 8 bytes */\n    @autoEndian uint64_t size = 0; /* 8 bytes */\n    ubyte[8] crc64 = 0; /* CRC64-ISO */\n    @autoEndian uint32_t numRecords = 0; /* 4 bytes */\n    @autoEndian uint16_t payloadVersion = 0; /* 2 bytes  */\n    PayloadType type = PayloadType.Unknown; /* 1 byte  */\n    PayloadCompression compression = PayloadCompression.Unknown; /* 1 byte */\n```\n\nWe merge all unique files in a package rootfs into the `Content` payload, and compress that using zstd. The offsets to\neach unique file (i.e. the sha256sum) are stored within the `Index` payload, allowing us to extract relevant portions\nfrom the \"megablob\" using `copy_file_range()`.\n\nThese files will become part of the system hash store, allowing another level of deduplication between all system\npackages. Finally, we use the `Layout` payload to **apply** the layout of the package into a transactional rootfs.\nThis will define paths, such as `/usr/bin/nano`, along with permissions, types, etc. All regular files will actually\nbe created as hard links from the hash store, allowing deduplication and snapshots.\n\nThe `Meta` payload consists of a number of records, each with strongly defined types (such as `String` or `Int64`) along\nwith the tag, i.e. `Name` or `Summary`. The entire format is binary to ensure greater resilience and a more compact\nrepresentation. For example, each metadata key is only 8 bytes.\n\n```d\n    @autoEndian uint32_t length; /** 4 bytes per record length*/\n    @autoEndian RecordTag tag; /** 2 bytes for the tag */\n    RecordType type; /** 1 byte for the type */\n    ubyte[1] padding = 0;\n```\n\n\n# TLDR that for me...\n\nBinary format that is self deduplicating at several layers, permitting fast transactional operations.\n\n# Up Next\n\nBefore we work any more on the binary format, we now need to pivot to the source format. Our immediate goal is to now\nhave moss actually **build** packages from source, with resulting `.stone` packages. Once this step is complete we can\nwork on installation, upgrades, repositories, etc, and race to becoming a self hosting distribution.\n\nNote, the format may still change before it goes into production, as we encounter more cases for optimisation or\nimprovement."
    },
    {
      "id": "2020/09/13/website-revamp",
      "metadata": {
        "permalink": "/blog/2020/09/13/website-revamp",
        "source": "@site/blog/website-revamp.md",
        "title": "Website Revamp",
        "description": "Over the last few days we've been working to refresh the stuffy website design with something much",
        "date": "2020-09-13T12:16:50.000Z",
        "formattedDate": "September 13, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.665,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Website Revamp",
          "date": "2020-09-13T12:16:50.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/website-revamp/Featured.webp",
          "slug": "2020/09/13/website-revamp"
        },
        "prevItem": {
          "title": "Moss Format Defined",
          "permalink": "/blog/2020/09/20/moss-format-defined"
        },
        "nextItem": {
          "title": "Defining Moss",
          "permalink": "/blog/2020/09/10/defining-moss"
        }
      },
      "content": "Over the last few days we've been working to refresh the stuffy website design with something much\ncleaner and inviting. It's still created using Hugo, but we've dropped MDBootstrap in favour for\nthe lighter vanilla Bootstrap option.\n\n![Refreshed website with core values](/img/blog/website-revamp/Featured.webp)\n\n\n<!--truncate-->\n\nWe're still working out a few kinks here and there with mobile rendering, but in the mean time please\ntake a look around, and make yourself at home. We're fast building a community around Serpent OS, so\nit's important we all share the same values and goals.\n\nIn short, our new tagline and motto is:\n\n# We're building a pioneering new Linux distribution based on modern technologies, enabling stellar features for everyone, without the price tag\n\n\nNow, it's time for us to get back to work, so, see you soon!"
    },
    {
      "id": "2020/09/10/defining-moss",
      "metadata": {
        "permalink": "/blog/2020/09/10/defining-moss",
        "source": "@site/blog/defining-moss.md",
        "title": "Defining Moss",
        "description": "Over the past few weeks, throughout the entire bootstrap process, we've",
        "date": "2020-09-10T15:02:48.000Z",
        "formattedDate": "September 10, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.675,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Defining Moss",
          "date": "2020-09-10T15:02:48.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/defining-moss/Featured.webp",
          "slug": "2020/09/10/defining-moss"
        },
        "prevItem": {
          "title": "Website Revamp",
          "permalink": "/blog/2020/09/13/website-revamp"
        },
        "nextItem": {
          "title": "Stage3 Complete",
          "permalink": "/blog/2020/09/07/stage3-complete"
        }
      },
      "content": "Over the past few weeks, throughout the entire bootstrap process, we've\nbeen deliberating on what our package manager is going to look like. We\nnow have a very solid idea on what that'll be, so it's time for a blog\npost to share with the community.\n\n![Initial moss prototype CLI](/img/blog/defining-moss/Featured.webp)\n\n<!--truncate-->\n\n# Old, but new\n\nThe team has been very clear in wanting a traditional package management solution,\nwhereby repositories and packages compose the OS as a whole. However, we also\nwant modern features, such as being stateless by default. A common theme\nis wanting to reduce the complex iterations of an OS into something that\nis sanely versioned, but also flexible, to ensure a consistent, tested\nexperience with multiple end targets.\n\nAdditionally, the OS must be incredibly easy for contributors and team members\nto maintain, with intelligent tooling and simple, but powerful formats.\n\n# Atomic updates\n\nOne of the most recent software update trends of recent years is atomic updates.\nIn essence this allows applying an update in a single operation, and importantly,\nreversing an update in a single operation, without impacting the running system.\n\nThis is typically achieved using a so-called `A/B switch`, which is what we will\nalso do with moss. We won't rely on any specific filesystem for this implementation,\ninstead relying on a smart layout, `pivot_root` and a few other tricks.\n\nPrimarily we'll update a single `/usr` symlink to point to the current OS image,\nwith `/` being a read-only faux rootfs, populated with established mountpoints\nand symlinks. Mutation will be possible only via `moss` transactions, or in\nworld-writable locations (`/opt`, `/usr/local`, `/etc` ...)\n\n# Deduplication\n\nThe moss binary package format will be deduplicated in nature, containing hash-keyed\nblobs in an `zstd` compressed payload. Unique blobs will be stored in a global cache,\nand hard-linked into their final location (i.e. `/moss/root/$number/usr/...`) to\ndeduplicate the installed system too. This allows multiple system snapshots with\nminimal overhead, and the ability to perform an offline rollback.\n\n# Implications\n\nWe'll need to lock kernels to their relevant transactions (or repo versions) to prevent\nuntested configurations. Additionally the boot menu will need to know about older versions\nof the OS that are installed, so they can be activated in the `initrd` at boot. This\nwill require us doing some work with `clr-boot-manager` to achieve our goals.\n\n# Difference from existing solutions\n\nWe're trying to minimise the iterations of the OS to what is available in a given version\nof the package repositories. Additionally we wish to avoid extensive \"symlink farms\"\nas we're not a profile-oriented distribution. Instead we focus on deduplication, atomic\nupdates and resolution performance.\n\n# Subscriptions\n\nKeeping a system slim is often a very difficult thing to achieve, without extensive\npackage splitting and effort on the user's part. An example might be enabling SELinux\nsupport on a system, or culling the locales to only include the used ones.\n\nIn Serpent OS (and moss, more specifically) we intend to address this through \"subscriptions\".\nWell defined names will be used by moss to filter (or enable) certain configurations\nin packages + dependencies. In some instances this will toggle subpackages/dependencies\nand in others it will control which paths of a package are actually installed to disk.\n\nGoing further with this concept, we will eventually introduce modalias based capabilities\nto automatically subscribe users to required kernel modules, allowing slim or fullfat\ninstallations as the user chooses. This in turn takes the burden of maintenance away\nfrom developers + users, and enables an incredibly flexible, approachable system.\n\n# Mandatory reboots\n\nWhere possible we will limit mandatory reboots, preferring an in-place atomic update\nto facilitate high uptime. However, there are situations where a reboot is absolutely\nunavoidable, and the system administrator should plan some downtime to handle this\ncase.\n\nCertain situations like a kernel update, or security fix to a core library, would\nrequire a reboot. In these instances, the atomic update will be deferred until the\nnext boot. In most situations, however, reboots will not be mandatory.\n\n# Until the next time\n\nWell, we've given a brief introduction to our aims with `moss` and associated tooling,\nand you can get more information by checking the moss [README.md](https://github.com/serpent-linux/moss/blob/main/README.md).\n\nThe takeaway is we want a package-based software update mechanism that is\nreliable and trusted, and custom-built to handle Serpent OS intricities,\nwith a simple approach to building and maintaining the distribution.\n\nFor now, we're gonna stop talking, and start coding."
    },
    {
      "id": "2020/09/07/stage3-complete",
      "metadata": {
        "permalink": "/blog/2020/09/07/stage3-complete",
        "source": "@site/blog/stage3-complete.md",
        "title": "Stage3 Complete",
        "description": "Another week, another milestone completed. We're pleased to announce that",
        "date": "2020-09-07T17:39:24.000Z",
        "formattedDate": "September 7, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.115,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Stage3 Complete",
          "date": "2020-09-07T17:39:24.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/stage3-complete/Featured.webp",
          "slug": "2020/09/07/stage3-complete"
        },
        "prevItem": {
          "title": "Defining Moss",
          "permalink": "/blog/2020/09/10/defining-moss"
        },
        "nextItem": {
          "title": "Looking Stage4 In The Eye",
          "permalink": "/blog/2020/08/28/looking-stage4-in-the-eye"
        }
      },
      "content": "Another week, another milestone completed. We're pleased to announce that\nwe've completed the initial Stage3 bootstrap. While we have some parallel\nworks to complete for Stage 4, we can now begin to work on the exciting\npieces! We're now bootstrapped on ARMv8(-a) and x86_64\n\n![Complete build-target for x86_64](/img/blog/stage3-complete/Build.webp)\n\n<!--truncate-->\n\n# Announcing Moss\n\nOur immediate focus is now to implement our package manager: [moss](https://github.com/serpent-linux/moss).\nCurrently it only has a trivial CLI and does absolutely nothing. We will now shift our attention\nto implement this as a core part of the Stage 4 bootstrap. Moss is so-called as we're a rolling\nrelease, _a rolling stone gathers no moss_. The package manager is being implemented in the D Programming Language.\n\nMoss does not aim to be a next generation package management solution, it instead inspired by\neopkg, RPM and swupd, aiming to provide a reliable modern package management and update\nsolution with stateless design at the core. Core features and automatic dependencies will be managed\nthrough capability subscriptions. The binary format will be versioned and deduplicated, with multiple\ninternal lookup tables and checksums. Our chief focus with moss is a reliable system\nmanagement tool that is accessible even in situations where internet access is limited.\n\n# Ongoing Works\n\nIn order to complete stage3, we provided linker stubs in `libwildebeest` to ensure\nwe can build. Obviously this introduces runtime errors in systemd and our stage3 isn't\nbootable, just chrootable. This will be resolved when systemd is properly packaged by\nmoss in stage4.\n\n# Docker Image\n\nWe now have a test container available on Docker Hub. You can install and run the\nsimple bash environment by executing the following command with a Docker-enabled user:\n\n```bash\n    docker run -it --rm serpentos/staging:latest\n```\n\nCurrently we only have an x86_64 image, but may experiment with multiarch builds later\nin stage4.\n\n**IMPORTANT**: The `staging` image is currently only a dump of the stage3 tree with\nminor cleaning + tweakups. It is not, in any way, shape or form, representative of\nthe final quality of Serpent OS. Additionally zero performance work or security\npatching has been done, so do **not** use in a production environment.\n\nThe image is provided currently as a means to validate the LLVM/musl toolchain.\n\n![Serpent OS stage3 Docker Image](/img/blog/stage3-complete/Featured.webp)\n\n# Future ISO\n\nWe cannot currently say when we'll **definitely** have an ISO, however we do\nknow that some VM-specific images will arrive first. After that we'll focus\non an installer (package selection based) and a default developer experience.\nAll we can say is strap in, and enjoy the ride."
    },
    {
      "id": "2020/08/28/looking-stage4-in-the-eye",
      "metadata": {
        "permalink": "/blog/2020/08/28/looking-stage4-in-the-eye",
        "source": "@site/blog/looking-stage4-in-the-eye.md",
        "title": "Looking Stage4 In The Eye",
        "description": "Well, we've made an awful lot of progress in these last few days. It wasn't",
        "date": "2020-08-28T20:35:16.000Z",
        "formattedDate": "August 28, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 3.535,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Looking Stage4 In The Eye",
          "date": "2020-08-28T20:35:16.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/looking-stage4-in-the-eye/FeaturedSD.webp",
          "slug": "2020/08/28/looking-stage4-in-the-eye"
        },
        "prevItem": {
          "title": "Stage3 Complete",
          "permalink": "/blog/2020/09/07/stage3-complete"
        },
        "nextItem": {
          "title": "Scaling Our Infrastructure",
          "permalink": "/blog/2020/08/22/scaling-our-infrastructure"
        }
      },
      "content": "Well, we've made an awful lot of progress in these last few days. It wasn't\nthat long ago that we introduced some of the new projects required to get\nstage3 off the ground.\n\n![Building systemd the easy way](/img/blog/looking-stage4-in-the-eye/FeaturedSD.webp)\n<!--truncate-->\n\n# libc-support\n\nOur [libc-support](https://dev.serpentos.com/source/libc-support/) project has been growing, thanks primarily to the contributions\nof [Jouni Roivas](https://dev.serpentos.com/p/joroi/). We now have initial working versions\nof `getent` and `getconf`. The `getconf` program is considered feature-complete\nfor our current requirements, and the focus is now on cleaning up `getent` making\nit more modular and easy to maintain in the long run.\n\n# libwildebeest\n\nWe began work on [libwildebeest](https://dev.serpentos.com/source/libwildebeest/) to quickly unlock building `systemd`.\nRemember, our ambition with this project is to provide a sane, centralised way of\nmaintaining source compatibility with various projects that rely on features currently\nonly available in the GNU toolchain + runtime. This is our alternative vision to\npatching every single package that fails to build against our LLVM+musl toolchain,\nensuring our work scales.\n\nRight now, `libwildebeest` implements some missing APIs, and indeed, some **replacement** APIs,\nfor when GNU behaviours are expected. It does so in a way that doesn't impact the\nresulting binary's license, or any of the system ABI. We provide some `pkg-config` files\nwith explicit `cflags` and `libs` fields set, such as:\n\n```bash\n\n      -L${libdir} -lwildebeest-ftw -Wl,--wrap=ftw -Wl,--wrap=nftw -I${includedir}/libwildebeest --include=lwb_ftw.h\n```\n\nOur headers take special care to **mask** the headers provided by musl to avoid redefinitions,\nand instruct the linker to replace calls to these functions with our own versions, i.e.:\n\n```c\n\nint __wrap_ftw(const char *dir, int (*funcc)(const char *, const struct stat *, int),\n               int descriptors)\n\n```\n\nIt then becomes trivial to enable wildebeest for a package build, i.e.:\n\n```bash\n        export CFLAGS=\"${CFLAGS} $(pkg-config --cflags --libs libwildebeest)\"\n```\n\nRight now - we've only provided stubs in `libwildebeest` to ensure we can build our packages.\nOur next focus is to actually implement those stubs using MIT licensed code so that applications\nand libraries can rely on `libwildebeest` to provide a basic level of GNU compatibility in a\nreliable fashion.\n\nUntil such point as all the APIs are fully and safely implemented, it would be highly ill-advised\nto **use** libwildebeest in any project. We'll announce stability in the coming weeks.\n\n# systemd\n\nWe've made great progress in enabling systemd in Serpent OS. Where `libwildebeest` is in\nplace, it now enables our currently required level of source compatibility to a point where\nsystemd is building with networkd, resolved and a number of other significant targets enabled.\n\nIn a small number of cases, we've had to patch systemd, but not in the traditional sense\nexpected to make it work with musl.\n\nThe only non-upstreamable patching we've done (in combination with libwildebeest enabling)\nwas to the UAPI headers, as the musl provided headers clash with the upstream kernel headers\nin certain places (`if_ether.h`, `if_arp.h`) - but this is a tiny cost to bear.\n\nThe other patches, were simply portability fixes, ensuring all headers were included:\n\n - [partition/makefs: Include missing sys/file.h header #16876](https://github.com/systemd/systemd/pull/16876)\n - [login/logind: Include sys/stat.h for struct stat usage #16887](https://github.com/systemd/systemd/pull/16887)\n\nIt should be noted both of these (very trivial) pull requests were accepted and merged upstream,\nand will be part of the next systemd release.\n\n# Next On The Agenda\n\nOur major ticket items involve fleshing out stage3 with some missing libraries to further\nenable systemd, rebuilds of util-linux to be systemd-aware, and continue fleshing out\n`dbus`, `systemd` and `libwildebeest` support to the point we have a bootable disk image.\n\nAt that point we'll move into stage4 with package management, boot management, and\na whole host of other goodies. And, if there is enough interest, perhaps some early\naccess ISOs!\n\nAfter having engaged in discussions with a variety of developers using musl as their\nprimary libc, we've catalogued common pain points. We therefore encourage developers\nto contribute to our `libwildebeest` and `libc-support` projects to complete the\ntooling and experience around musl-based distributions.\n\nOur aim for Serpent OS is a **full fat experience**, which means we have large ticket\nitems on our horizon, such as NSS/IDN integration, performance improvements, increasing\nthe default stack size, along with source compatibility for major upstream projects.\n\nUntil the next blog post, you can keep up to date on our IRC channel. Join `#serpentOS` on freenode!"
    },
    {
      "id": "2020/08/22/scaling-our-infrastructure",
      "metadata": {
        "permalink": "/blog/2020/08/22/scaling-our-infrastructure",
        "source": "@site/blog/scaling-our-infrastructure.md",
        "title": "Scaling Our Infrastructure",
        "description": "While it might look like we haven't been up to much lately, the exact opposite is true.",
        "date": "2020-08-22T13:25:56.000Z",
        "formattedDate": "August 22, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.6,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Scaling Our Infrastructure",
          "date": "2020-08-22T13:25:56.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/scaling-our-infrastructure/featured.webp",
          "slug": "2020/08/22/scaling-our-infrastructure"
        },
        "prevItem": {
          "title": "Looking Stage4 In The Eye",
          "permalink": "/blog/2020/08/28/looking-stage4-in-the-eye"
        },
        "nextItem": {
          "title": "Stage 3 Progress",
          "permalink": "/blog/2020/08/20/stage3-progress"
        }
      },
      "content": "While it might look like we haven't been up to much lately, the exact opposite is true.\nWe've been busy scaling our infrastructure in preparation for the upcoming `stage4`\nbootstrap..\n\n<!--truncate-->\n\n# fosshost.org sponsors Serpent OS\n\nRecently we've been in talks with [fosshost.org](https://fosshost.org) who have very kindly\nagreed to sponsor the project through additional hosting and mirrors. We've been provided\nwith a server in Maidenhead, UK, which we've just finished configuring. Per our talks, as\nour project grows, so will the hosting support.\n\nThis has enabled to get our infrastructure ready in anticipation of the `stage4` bootstrap,\nwhich is only around the corner now.\n\n# Wildcard SSL Certificate\n\nAs part of our expansion, Aydemir has purchased a wildcard SSL certificate for our domains lasting\n2 years, as part of ensuring our longevity. This has been deployed across two of our subdomains,\nand will soon be deployed on this website too.\n\n# Phabricator\n\nWe now have a brand-spanking-new Phabricator instance running at [dev.serpentos.com](https://dev.serpentos.com).\nThis will be used for tracking issues, hosting repositories and will become a central pillar of our\ncommunity. All major development will happen on this new tracker, which is linked through the header\nof this page.\n\nYou can sign up with a username, or login via GitHub. All new members are required to verify their\nemail address, and some may have to pass a reCAPTCHA challenge. This is to help limit the amount of\nspam on the new site.\n\n![Our newly installed Phabricator](/img/blog/scaling-our-infrastructure/featured.webp)\n\nPlease note we're not accepting package requests at this moment in time, and generic 'Why??' issues not\nrelating to development will be closed as they only serve to derail work on the project, by taking away\nvaluable time.\n\n# Download server\n\nOur download server currently lives at [download.serpentos.com](https://download.serpentos.com) and co-exists\nwith the Tracker. We expect to serve downloads and repositories at this address for the `stage4` bootstrap.\n\n# Mirrors\n\n`fosshost` have also kindly integrated the Serpent OS servers into their mirror network. Currently you can find us\nat these locations:\n\n - https://uk.mirrors.fossho.st/serpentos/\n - https://us.mirrors.fossho.st/serpentos/\n\nRight now there is no content to download until such point as stage4 progresses.\n\n# Moving Our Repositories\n\nOur `bootstrap-scripts` repository has migrated from GitHub to our internal hosting. You can find it here:\n\n - https://dev.serpentos.com/source/bootstrap-scripts/\n\nIt is advisable to re-clone the project, as we've migrated from the `master` branch to the `main` branch.\nAll future Serpent OS repositories will now default to this branch, enabling more logical naming of\nbranches (`main/edge`).\n\n# Privacy Policy\n\nTo ensure that our users are comfortable, and ensure compliance with the GDPR, we've deployed an updated\nprivacy policy which can be found [here](/privacy/). This can be found in the top header of the website\nat any time.\n\n# A Final Word\n\nWe want to extend our warmest thanks to [fosshost.org](https://fosshost.org) for their early support of\nthe project, as it will ensure we have scale builtin from the outset to better serve our users.\n\nAdditionally, many thanks to the team and community for recent work and assistance in getting everything\nset up and running.\n\nIf you're interested in our Sponsorships, please visit the new [Sponsors](/sponsors) page."
    },
    {
      "id": "2020/08/20/stage3-progress",
      "metadata": {
        "permalink": "/blog/2020/08/20/stage3-progress",
        "source": "@site/blog/stage3-progress.md",
        "title": "Stage 3 Progress",
        "description": "Well, it's been a few days since we last spoke, so now it's time for a quick",
        "date": "2020-08-20T13:14:49.000Z",
        "formattedDate": "August 20, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 4.42,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Stage 3 Progress",
          "date": "2020-08-20T13:14:49.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/stage3-progress/screenshot.webp",
          "slug": "2020/08/20/stage3-progress"
        },
        "prevItem": {
          "title": "Scaling Our Infrastructure",
          "permalink": "/blog/2020/08/22/scaling-our-infrastructure"
        },
        "nextItem": {
          "title": "Stage2 Complete",
          "permalink": "/blog/2020/08/16/stage2-complete"
        }
      },
      "content": "Well, it's been a few days since we last spoke, so now it's time for a quick\nroundup. Long story short, we're approaching the end of the stage3 bootstrap.\n\n![Fully functional chroot](/img/blog/stage3-progress/screenshot.webp)\n\n<!--truncate-->\n\nIn an effort to simplify our bootstrap process, we dropped the newly-introduced `stage2.5` and\ncame up with a new strategy for `stage3`. In order to make it all work nicely, we bind-mount the\n`stage2` resulting runtime at `/serpent` within the `stage3` chroot environment, executing the\n`/serpent/usr/bin/bash` shell.\n\nIn order to make this work, we build an intermediate `musl` package for `libc.so` in the very\nstart of `stage3`, with all subsequent builds being performed in chroots. Part of the build\nis done on the host, i.e. extraction and patching, minimising the tool requirements for the chroot\nenvironment. The configuration, build and install is performed from within the initially empty\nchroot environment, replacing all the `/serpent/usr/bin` tools and `/serpent/usr/lib` libraries.\n\n# Mild Blockers\n\nAs we move further through stage3, towards a fully usable chroot environment, we've encountered\na small number of blockers. Now, we **could** solve them by using existing patchwork and workarounds,\nbut most have not and will not be accepted upstream. Additionally it is incredibly hard to track the\norigin and history of most of these, making security rather more painful.\n\n## libc-support\n\nWe're going to start working on a project to flesh out the `musl` runtime with some missing utilities,\nwritten with a clean-room approach. These will initially include the `getconf` and `getent` tools,\nwhich will be written only with Linux in mind, and no legacy/BSD support.\n\nThese will be maintained over at our [GitHub](https://github.com/serpent-linux/libc-support)\n\n## libwildebeest\n\nAs a project we strive for correctness in the most pragmatic way. Some software, such as systemd,\nis heavily reliant on GNU GCC/GLibc extensions. In some software there are feasible alternatives\nwhen using musl, however in a good number of cases, functionality required to build certain\nsoftware is missing and has no alternative.\n\nOver time we'll try to work with upstreams to resolve those issues, but we're working on an interim\nsolution called 'libwildebeest'. This will provide **source compatibility** for a limited number\nof software packages relying on so-called 'GNUisms'. Binary compatibility is not an aim whatsoever,\nand will not be implemented. This convenience library will centralise all patchwork on packages\nthat need more work to integrate with musl, until such time as upstreams have resolved the remaining\nissues.\n\nAdditionally it will help us track those packages needing attention in the distribution, as they\nwill have a build-time dependency on `libwildebeest`. We do not intend to use this project extensively.\n\nThis will be maintained over at our [GitHub](https://github.com/serpent-linux/libwildebeest)\n\n# A Word On Init Systems\n\nRecently we've had many queries regarding the init system, as there is an expectation that due to our\nuse of musl/llvm we also dislike systemd or wish to be a small OS, etc. There is a place in the world\nfor those projects already, and we wish them much success. However from our own stance and goals,\n`systemd` has already \"won the battle\" and actually fits in with our design.\n\n**If** it is possible in future with package manager considerations and packaging design, then we may\nmake it possible to swap `systemd` for a similar set of packages. However, we only intend at this time\nto support `systemd/udev/dbus` directly in Serpent OS and leave alternatives to the community.\n\n# Other News\n\nJust a quick heads up, we've been talking to the cool folks over at [fosshost.org](https://fosshost.org/)\nand they've agreed to kindly provide us with additional hosting and mirroring. This will allow us to\nbuild scale in from the very start, ensuring updates and images are always available. Once the new server\nis up and running we'll push another blogpost with the details and links.\n\nWhile initially we intended to avoid public bug trackers, the rate of growth within the project and community\nhave made it highly apparent that proper communication channels need establishing. Therefore we will be\nsetting up a public Phabricator instance for reporting issues, security flaws, and contributing packaging.\n\nMuch of our website is in much need of update, but our current priority is with building the OS. Please\nbe patient with us, we'll have it all sorted out in no time.\n\n## Where We At?\n\nWell, stage3 completes fully, builds the final compiler, which has also been verified. A usable chroot\nsystem is produced, built using `musl`, `libc++`, `libunwind`, `clang`, etc. Some might say that stage3\nis complete, however we wish to avoid circular dependency situations. We'll mark stage3 as complete once\nwe've integrated an initial slimmed down build of `systemd` and appropriately relinked the build.\n\nAs soon as this stage is done, we'll proceed with `stage4`. This is the final stage where we'll add\npackage management and define the OS itself, with global flags, policies, etc.\n\nWith the speed we're moving at, that really isn't too far away.\n\n## And finally..\n\nI personally wish to thank the Serpent OS team as a whole for the commitment and work undertaken of late.\nAdditionally I want to thank the growing community around Serpent OS, primarily residing in our IRC\nchannel (`#serpentOS` on freenode) and our Twitter account. Your input has been amazing, and it's\nso refreshing to have so many people on the same page. Stay awesome."
    },
    {
      "id": "2020/08/16/stage2-complete",
      "metadata": {
        "permalink": "/blog/2020/08/16/stage2-complete",
        "source": "@site/blog/stage2-complete.md",
        "title": "Stage2 Complete",
        "description": "Just in case you thought we were sleeping behind the wheel, we've got",
        "date": "2020-08-16T14:08:08.000Z",
        "formattedDate": "August 16, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 2.435,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Stage2 Complete",
          "date": "2020-08-16T14:08:08.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/stage2-complete/Featured.webp",
          "slug": "2020/08/16/stage2-complete"
        },
        "prevItem": {
          "title": "Stage 3 Progress",
          "permalink": "/blog/2020/08/20/stage3-progress"
        },
        "nextItem": {
          "title": "Stage1 Complete",
          "permalink": "/blog/stage1-complete"
        }
      },
      "content": "Just in case you thought we were sleeping behind the wheel, we've got\nanother blogpost for your viewing pleasure. In a nutshell, we completed\nstage2 bootstrap.\n\n![Complete build-target for ARMv8](/img/blog/stage2-complete/Featured.webp)\n\n<!--truncate-->\n\nIn order to simplify life, we greatly reduced the size of the `stage2` build component.\nThis decision was taken to better support cross-compilation in the face of software that\nis distinctly cross-compilation unfriendly.\n\nA support stage, `stage2.5` will be added which will `chroot` into a copy of `stage2`, and\nnatively compile a small handful of packages required to complete `stage3`, also within the\nchroot environment.\n\nFor cross-compilation, we'll be relying on `qemu-static` to complete `2.5` and `3`.\nHowever, at this point in time, we have the following:\n\n - Working cross-compilation of the entire bootstrap\n - Complete LLVM based toolchain: `clang`, `llvm`, `libc++`, `lib++abi`, `libunwind`\n - Entirety of stage2 built with `musl` libc.\n - Working, minimal, `chroot` environment as product of `stage2`, with working compiler (`C` & `C++`)\n\n![x86_64](/img/blog/stage2-complete/x86_64.webp \"Working x86_64 chroot\")\n\nThis is a major milestone for the project, as it is an early indication that we're self hosting.\n\n# Multiple Architecture Support\n\nAt this point in time, we now have build support for two targets: `x86_64` and `ARMV8a`.\nOur intent is to support `haswell` and above, or `zen` and above, for the x86_64 target.\n\nWith our `ARMv8` target, we're currently looking to support the Pine Book Pro, if we can\nmanage to get hold of some testing hardware. It will likely be some time after full\nx86_64 support that we'd officially support more hardware, however it is very important\nthat our bootstrap-scripts can trivially target multiple platforms.\n\n![ARMv8](/img/blog/stage2-complete/ARMv8.webp \"ARMv8 validation\")\n\nAn interesting change when cross-compiling for other architectures, is the chicken & egg\nsituation with `compiler-rt` and other LLVM libraries. When we detect cross-compilation,\nwe'll automatically bootstrap `compiler-rt` before building `musl`, and then cross-compile\n`libc++`, `libc++abi` and `libunwind` to ensure `stage1` can produce native binaries for\nthe target with correct linkage.\n\n# Next Steps\n\nAs we've mentioned, we'll push ahead with `2.5` and `3`, which will complete the initial\nSerpent OS bootstrap, producing a self-hosting, self-reliant rootfs. This is the point\nat which we can begin to bolt-on package management, boot management, stateless configuration\nutilities, etc.\n\nOur initial focus is `x86_64` hardware with UEFI, and as we gain access to more hardware we\ncan enable support for more targets, such as `ARMv8a`. Our `bootstrap-scripts` will always\nremain open source, as will all processes and tooling within Serpent OS, or anything used\nto build and deploy Serpent OS.\n\nThis will make it much easier in future to create custom spins of Serpent OS for different\nconfigurations or targets, without derailing the core project. It should therefore be the\nsimplest thing in the world to fork Serpent OS to one's liking or needs.\n\nIf you want to support our work, you can jump onto our IRC channel (`#serpentOS` on freenode)\nor support us via the [Team page](/team)."
    },
    {
      "id": "/stage1-complete",
      "metadata": {
        "permalink": "/blog/stage1-complete",
        "source": "@site/blog/stage1-complete.md",
        "title": "Stage1 Complete",
        "description": "Short and sweet, stage1 of the bootstrap is complete. As I indicated on the Lispy Snake blog,",
        "date": "2020-08-08T14:15:04.000Z",
        "formattedDate": "August 8, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.715,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Stage1 Complete",
          "date": "2020-08-08T14:15:04.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "image": "/img/blog/stage1-complete/progress.webp"
        },
        "prevItem": {
          "title": "Stage2 Complete",
          "permalink": "/blog/2020/08/16/stage2-complete"
        },
        "nextItem": {
          "title": "Server Fixes",
          "permalink": "/blog/2020/07/07/server-fixes"
        }
      },
      "content": "Short and sweet, stage1 of the bootstrap is complete. As I indicated on the [Lispy Snake](https://lispysnake.com/blog/2020/08/03/status-update/) blog,\nI'm still in the process of settling into new accommodation. This is going well, but still awaiting proper\nbroadband connectivity. Work has begun, however, and we're now moving onto stage2 of the bootstrap.\n\n![Validating stage1 cross-compiler](/img/blog/stage1-complete/progress.webp)\n\n<!--truncate-->\n\nThis is handled via our [bootstrap-scripts](https://github.com/serpent-linux/bootstrap-scripts) project\nand can be run by anyone on a relatively modern Linux distribution.\n\nNext on the list is completing stage2, which we've already started on. This is simply a cross-compiled\nchroot environment with all the basic bits in place to build stage3, sanitizing and cleansing the toolchain.\n\nEven though it is early days, we can already, automatically produce a working cross-compiler that targets\nLLVM's `libc++`, the musl `libc.so`, and `x86_64-serpent-linux-musl` host triplet.\n\nProbably a dull update for most, but an update it is."
    },
    {
      "id": "2020/07/07/server-fixes",
      "metadata": {
        "permalink": "/blog/2020/07/07/server-fixes",
        "source": "@site/blog/server-fixes.md",
        "title": "Server Fixes",
        "description": "Yet another 30-second update from the team. We've fixed some DNS issues so that we can drop the",
        "date": "2020-07-07T22:12:08.000Z",
        "formattedDate": "July 7, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.245,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Server Fixes",
          "date": "2020-07-07T22:12:08.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/07/07/server-fixes"
        },
        "prevItem": {
          "title": "Stage1 Complete",
          "permalink": "/blog/stage1-complete"
        },
        "nextItem": {
          "title": "Welcome: Project Manager",
          "permalink": "/blog/2020/07/07/welcome-project-manager"
        }
      },
      "content": "Yet another 30-second update from the team. We've fixed some DNS issues so that we can drop the\nugly `www.` prefix from URLs, assigned a static IP, fixing the A + CNAME records.\n\nAdditionally we updated the SSL configuration of `apache2`, earning an `A+` rating from SSL labs.\nSecurity++."
    },
    {
      "id": "2020/07/07/welcome-project-manager",
      "metadata": {
        "permalink": "/blog/2020/07/07/welcome-project-manager",
        "source": "@site/blog/welcome-project-manager.md",
        "title": "Welcome: Project Manager",
        "description": "The Serpent OS team wishes to formally welcome Aydemir Ulaş Şahin to the core team in the primary",
        "date": "2020-07-07T10:35:50.000Z",
        "formattedDate": "July 7, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.605,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Welcome: Project Manager",
          "date": "2020-07-07T10:35:50.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/07/07/welcome-project-manager"
        },
        "prevItem": {
          "title": "Server Fixes",
          "permalink": "/blog/2020/07/07/server-fixes"
        },
        "nextItem": {
          "title": "Website Refresh",
          "permalink": "/blog/2020/07/07/website-refresh"
        }
      },
      "content": "The Serpent OS team wishes to formally welcome Aydemir Ulaş Şahin to the core team in the primary\ncapacity of Project Manager. Aydemir will help us to coordinate and manage the project goals and\ninfrastructure, being responsible for certain \"keys to the kingdom\".\n\nAydemir has extensive experience with Linux, both as a user and developer, having worked in the\nmedia industry for many years. Additionally he will be contributing to the core system, code\nand packaging.\n\nOur initial trio has been announced, and you can always access information on the current team\nat the new [Team page](/team). Now that much of our groundwork has been laid, we can get on\nwith building the project. Check progress at the new [Roadmap](/roadmap) page."
    },
    {
      "id": "2020/07/07/website-refresh",
      "metadata": {
        "permalink": "/blog/2020/07/07/website-refresh",
        "source": "@site/blog/website-refresh.md",
        "title": "Website Refresh",
        "description": "A 30-second update here from ourselves: We've updated the style and layout of the website",
        "date": "2020-07-07T01:08:08.000Z",
        "formattedDate": "July 7, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.25,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Website Refresh",
          "date": "2020-07-07T01:08:08.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/07/07/website-refresh"
        },
        "prevItem": {
          "title": "Welcome: Project Manager",
          "permalink": "/blog/2020/07/07/welcome-project-manager"
        },
        "nextItem": {
          "title": "Welcome Peter",
          "permalink": "/blog/2020/07/07/welcome-peter"
        }
      },
      "content": "A 30-second update here from ourselves: We've updated the style and layout of the website\nwith a new landing page that is more mobile friendly. As indicated on the new landing page\nwe have a new [Roadmap](/roadmap) page.\n\nMore pages and updates will come soon, so F5 and stay tuned!"
    },
    {
      "id": "2020/07/07/welcome-peter",
      "metadata": {
        "permalink": "/blog/2020/07/07/welcome-peter",
        "source": "@site/blog/welcome-peter.md",
        "title": "Welcome Peter",
        "description": "Recently we did reveal on Twitter that long time friend and colleague-of-many-projects, Peter O'Connor, has formally joined the core team for",
        "date": "2020-07-06T15:12:07.000Z",
        "formattedDate": "July 6, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.37,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "Welcome Peter",
          "date": "2020-07-06T15:12:07.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/07/07/welcome-peter"
        },
        "prevItem": {
          "title": "Website Refresh",
          "permalink": "/blog/2020/07/07/website-refresh"
        },
        "nextItem": {
          "title": "The Great Experiment",
          "permalink": "/blog/2020/07/01/the-great-experiment"
        }
      },
      "content": "Recently we did reveal on Twitter that long time friend and colleague-of-many-projects, Peter O'Connor, has formally joined the core team for\nSerpent OS. Many of you know him as `sunnyflunk`. He will be working on core toolchain optimisations, benchmarking, and eventually Plasma inclusion.\n\n<!--truncate-->\n\nPeter has long wanted to get the most out of his computing experience, starting from Gentoo back in the 2000's to ensure there was no wasted resources on\nfeatures he didn't use. The desire to over-tinker and use unstable software often led to poor outcomes. While the itch laid dormant for over a decade,\nit has always been there.\n\n> \"There's not much about Linux that currently excites me, but I've always wanted to see what's possible with a full LLVM/Clang stack. There's never been a better time to find out, but it can be difficult to push the envelope when limited with backwards compatibility and supporting older CPU architectures. After experimenting with some tool-chain customisations the chance to test them out more broadly and with a new tool-chain was too hard to pass up. This is the research project I've always wanted to undertake and looking forward to using it as my daily driver\n\nWe'd like to officially extend our welcome to Peter, as the ramp up gets in place. It should be noted, he is partly to blame for Serpent OS\nbecoming manifest, as the core team got together after discussions on IRC. He has some fantastic ideas, and is a brilliant distribution\nengineer with proven success.\n\nAs an aside, we're working on improving the website to incorporate a team page, ready for the third team member reveal."
    },
    {
      "id": "2020/07/01/the-great-experiment",
      "metadata": {
        "permalink": "/blog/2020/07/01/the-great-experiment",
        "source": "@site/blog/the-great-experiment.md",
        "title": "The Great Experiment",
        "description": "So as many have come to realise, we had to rush out a website super quick yesterday as the cat",
        "date": "2020-07-01T18:39:56.000Z",
        "formattedDate": "July 1, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 1.26,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "The Great Experiment",
          "date": "2020-07-01T18:39:56.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/07/01/the-great-experiment"
        },
        "prevItem": {
          "title": "Welcome Peter",
          "permalink": "/blog/2020/07/07/welcome-peter"
        },
        "nextItem": {
          "title": "First Post",
          "permalink": "/blog/2020/06/30/first-post"
        }
      },
      "content": "So as many have come to realise, we had to rush out a website super quick yesterday as the cat\nwas already out of the bag. One thing that should also be clarified, is our approach to development.\n\n<!--truncate-->\n\n# We're A Lab\n\nPrimarily (but not exclusively) Serpent OS (future: Serpent Linux) is an endeavour worked on by the\n[Lispy Snake, Ltd](https://lispysnake.com) crew. So we're all about creating awesome technology and\ntrying to further the playing field.\n\nSerpent OS is not, however, **owned** by Lispy Snake. It is a contributor-based open source project,\nwith specific roles. Those will be clarified over the coming week, but there is a dedicated **Project Manager**.\nHint: It's not me.\n\n# Measuring Success\n\nSo, how does an experimental project.. measure success? Quite simply - if others adopt our methodologies or\ntechnology, we've succeeded. We're really **not** looking at \"number of installs\" or \"size of userbase\" as\na metric, as our chief aim is to _build technology_.\n\n# Options for Longevity\n\nIf the experiment is a success, which of course means having tight controls on scope and timescale, then\none would assume the primary way to **use** Serpent OS would be through some downstream repackaging or\nreconfiguration, i.e. basing upon the project, to meet specific user demand.\n\n# It Should Be Fun\n\nThis is the heart and soul of all Linux development. Developers should _enjoy_ the involvement and seeing\nthe end result. If we build something more in the end, then that's a journey we can create together."
    },
    {
      "id": "2020/06/30/first-post",
      "metadata": {
        "permalink": "/blog/2020/06/30/first-post",
        "source": "@site/blog/first-post.md",
        "title": "First Post",
        "description": "In a nutshell, this post wasn't meant to be coming out for a few weeks.",
        "date": "2020-06-30T15:24:32.000Z",
        "formattedDate": "June 30, 2020",
        "tags": [
          {
            "label": "news",
            "permalink": "/blog/tags/news"
          }
        ],
        "readingTime": 0.925,
        "hasTruncateMarker": false,
        "authors": [
          {
            "name": "Ikey Doherty",
            "title": "Architect",
            "email": "ikey@serpentos.com",
            "imageURL": "img/authors/ikey.jpeg",
            "key": "ikey"
          }
        ],
        "frontMatter": {
          "title": "First Post",
          "date": "2020-06-30T15:24:32.000Z",
          "authors": "ikey",
          "tags": [
            "news"
          ],
          "slug": "2020/06/30/first-post"
        },
        "prevItem": {
          "title": "The Great Experiment",
          "permalink": "/blog/2020/07/01/the-great-experiment"
        }
      },
      "content": "In a nutshell, this post wasn't meant to be coming out for a few weeks.\nAs it turns out, we had to get ahead of the cat coming out of the bag,\nand ensure clarity of goals and ambition.\n\nWe've got an awful lot of work ahead of us across the next few months and\nweeks, and we strongly suggest checking out the [About](/about) page which\nshould answer most questions.\n\nTL;DR This is not your typical user-facing Linux distribution.\n\nSo go ahead, check out the [About](/about) page.\n\nPosts will follow in the coming weeks, introducing the core team.\nIt should be pointed out, internally, we're in a stage1 bootstrap phase,\nwith stage2 ready. Tooling is next on the agenda.\n\nIf you're looking for a modern, lightweight, user-friendly, privacy-focused\nLinux desktop distribution, then you're in the wrong place.\n\nOtherwise, stay tuned for the updates. We know our website is insanely\nbasic, but it will be improved when it becomes a priority. You're more\nthan welcome to join us on `#serpentOS` on freenode to become part of the\ncommunity discussion in the mean time.\n\nTil next time."
    }
  ]
}